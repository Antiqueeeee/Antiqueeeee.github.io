<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>博客模板</title>
    <url>/0-%E5%8D%9A%E5%AE%A2%E6%A8%A1%E6%9D%BF/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>1.两数之和</title>
    <url>/1-%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/</url>
    <content><![CDATA[<p>给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target  的那 两个 整数，并返回它们的数组下标。你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。你可以按任意顺序返回答案。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/two-sum">https://leetcode.cn/problems/two-sum</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">twoSum</span>(<span class="params">self, nums, target</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :type target: int</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums) - <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>( (i+<span class="number">1</span>),<span class="built_in">len</span>(nums)):</span><br><span class="line">                num_a,num_b = nums[i],nums[j]</span><br><span class="line">                <span class="keyword">if</span> num_a + num_b  == target:</span><br><span class="line">                    <span class="keyword">return</span> i,j</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>1124. 表现良好的最长时间段</title>
    <url>/1124-%E8%A1%A8%E7%8E%B0%E8%89%AF%E5%A5%BD%E7%9A%84%E6%9C%80%E9%95%BF%E6%97%B6%E9%97%B4%E6%AE%B5/</url>
    <content><![CDATA[<p>给你一份工作时间表 hours，上面记录着某一位员工每天的工作小时数。<br>我们认为当员工一天中的工作小时数大于 8 小时的时候，那么这一天就是「劳累的一天」。<br>所谓「表现良好的时间段」，意味在这段时间内，「劳累的天数」是严格 大于「不劳累的天数」。<br>请你返回「表现良好时间段」的最大长度。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/longest-well-performing-interval">https://leetcode.cn/problems/longest-well-performing-interval</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。<br>对前缀和的理解还是很模糊。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestWPI</span>(<span class="params">self, hours</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type hours: List[int]</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        n = <span class="built_in">len</span>(hours)</span><br><span class="line">        score = [<span class="number">1</span> <span class="keyword">if</span> item &gt; <span class="number">8</span> <span class="keyword">else</span> -<span class="number">1</span> <span class="keyword">for</span> item <span class="keyword">in</span> hours ]</span><br><span class="line"></span><br><span class="line">        presum = [<span class="number">0</span>] * ( n + <span class="number">1</span> )</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line">            presum[i] = presum[i-<span class="number">1</span>] + score[i-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        index_record = <span class="built_in">list</span>()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(presum)):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(index_record) == <span class="number">0</span> <span class="keyword">or</span> presum[i] &lt; presum[index_record[-<span class="number">1</span>]]:</span><br><span class="line">                index_record.append(i)</span><br><span class="line">        result = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(presum) -<span class="number">1</span> ,-<span class="number">1</span>,-<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">while</span> <span class="built_in">len</span>(index_record) <span class="keyword">and</span> presum[i] - presum[index_record[-<span class="number">1</span>]] &gt; <span class="number">0</span> :</span><br><span class="line">                result = <span class="built_in">max</span>(i - index_record[-<span class="number">1</span>] , result)</span><br><span class="line">                index_record.pop()</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>1129. 颜色交替的最短路径</title>
    <url>/1129-%E9%A2%9C%E8%89%B2%E4%BA%A4%E6%9B%BF%E7%9A%84%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84/</url>
    <content><![CDATA[<p>在一个有向图中，节点分别标记为 0, 1, …, n-1。图中每条边为红色或者蓝色，且存在自环或平行边。<br>red_edges 中的每一个 [i, j] 对表示从节点 i 到节点 j 的红色有向边。类似地，blue_edges 中的每一个 [i, j] 对表示从节点 i 到节点 j 的蓝色有向边。<br>返回长度为 n 的数组 answer，其中 answer[X] 是从节点 0 到节点 X 的红色边和蓝色边交替出现的最短路径的长度。如果不存在这样的路径，那么 answer[x] &#x3D; -1。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/shortest-path-with-alternating-colors">https://leetcode.cn/problems/shortest-path-with-alternating-colors</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<p>算是接触的第一到图题，很多相关概念都忘记了，题干也没太看懂，重新整理了一下图的相关概念后，在评论区找了一段代码，题的思路和广度优先遍历弄明白了一些，但是也没完全弄懂。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">shortestAlternatingPaths</span>(<span class="params">self, n, redEdges, blueEdges</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type n: int</span></span><br><span class="line"><span class="string">        :type redEdges: List[List[int]]</span></span><br><span class="line"><span class="string">        :type blueEdges: List[List[int]]</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        g = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">        <span class="keyword">for</span> a, b <span class="keyword">in</span> redEdges:</span><br><span class="line">            g[a].append(-b)</span><br><span class="line">        <span class="keyword">for</span> a, b <span class="keyword">in</span> blueEdges:</span><br><span class="line">            g[-a].append(b)</span><br><span class="line">        <span class="built_in">print</span>(g)</span><br><span class="line">        ret = [-<span class="number">1</span>] * n</span><br><span class="line">        vis = <span class="built_in">set</span>()</span><br><span class="line">        q = deque([<span class="number">0</span>])</span><br><span class="line">        vis.add(<span class="number">0</span>)</span><br><span class="line">        s = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> q:</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(q)):</span><br><span class="line">                p = q.pop()</span><br><span class="line">                <span class="keyword">if</span> ret[<span class="built_in">abs</span>(p)] == -<span class="number">1</span>:</span><br><span class="line">                    ret[<span class="built_in">abs</span>(p)] = s</span><br><span class="line">                <span class="keyword">for</span> x <span class="keyword">in</span> g[p]:</span><br><span class="line">                    <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> vis:</span><br><span class="line">                        q.appendleft(x)</span><br><span class="line">                        vis.add(x)</span><br><span class="line">            s += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
      <tags>
        <tag>Leetcode贷</tag>
      </tags>
  </entry>
  <entry>
    <title>1138. 字母板上的路径</title>
    <url>/1138-%E5%AD%97%E6%AF%8D%E6%9D%BF%E4%B8%8A%E7%9A%84%E8%B7%AF%E5%BE%84/</url>
    <content><![CDATA[<p>我们从一块字母板上的位置 (0, 0) 出发，该坐标对应的字符为 board[0][0]。<br>在本题里，字母板为board &#x3D; [“abcde”, “fghij”, “klmno”, “pqrst”, “uvwxy”, “z”]，如下所示。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/alphabet-board-path">https://leetcode.cn/problems/alphabet-board-path</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">alphabetBoardPath</span>(<span class="params">self, target</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type target: str</span></span><br><span class="line"><span class="string">        :rtype: str</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        operation = <span class="built_in">list</span>()</span><br><span class="line">        current_location = [<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> tar <span class="keyword">in</span> target:</span><br><span class="line">            row = (<span class="built_in">ord</span>(tar) - <span class="number">97</span>) / <span class="number">5</span></span><br><span class="line">            col = (<span class="built_in">ord</span>(tar) - <span class="number">97</span>) % <span class="number">5</span></span><br><span class="line">            row_moving = row - current_location[<span class="number">0</span>]</span><br><span class="line">            col_moving = col - current_location[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> current_location[<span class="number">0</span>] == <span class="number">5</span> <span class="keyword">and</span> row_moving !=<span class="number">0</span> <span class="keyword">and</span> col_moving != <span class="number">0</span>:</span><br><span class="line">                operation += <span class="string">&quot;U&quot;</span></span><br><span class="line">                current_location[<span class="number">0</span>] -= <span class="number">1</span></span><br><span class="line">                row_moving += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> col_moving &gt; <span class="number">0</span>:</span><br><span class="line">                operation += [<span class="string">&quot;R&quot;</span>] * col_moving</span><br><span class="line">                current_location[<span class="number">1</span>] += col_moving</span><br><span class="line">            <span class="keyword">if</span> col_moving &lt; <span class="number">0</span>:</span><br><span class="line">                operation += [<span class="string">&quot;L&quot;</span>] * <span class="built_in">abs</span>(col_moving)</span><br><span class="line">                current_location[<span class="number">1</span>] += col_moving</span><br><span class="line">            <span class="keyword">if</span> row_moving &gt; <span class="number">0</span>:</span><br><span class="line">                operation += [<span class="string">&quot;D&quot;</span>] * row_moving</span><br><span class="line">                current_location[<span class="number">0</span>] += row_moving</span><br><span class="line">            <span class="keyword">if</span> row_moving &lt; <span class="number">0</span>:</span><br><span class="line">                operation += [<span class="string">&quot;U&quot;</span>] * <span class="built_in">abs</span>(row_moving)</span><br><span class="line">                current_location[<span class="number">0</span>] += row_moving</span><br><span class="line">            operation += [<span class="string">&quot;!&quot;</span>]</span><br><span class="line">            <span class="built_in">print</span>(operation,row_moving,col_moving)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join(operation)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>1145. 二叉树着色游戏</title>
    <url>/1145-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9D%80%E8%89%B2%E6%B8%B8%E6%88%8F/</url>
    <content><![CDATA[<p>有两位极客玩家参与了一场「二叉树着色」的游戏。游戏中，给出二叉树的根节点 root，树上总共有 n 个节点，且 n 为奇数，其中每个节点上的值从 1 到 n 各不相同。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/binary-tree-coloring-game">https://leetcode.cn/problems/binary-tree-coloring-game</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。<br>写的第一版，超时了，嘿嘿。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode(object):</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">btreeGameWinningMove</span>(<span class="params">self, root, n, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :type n: int</span></span><br><span class="line"><span class="string">        :type x: int</span></span><br><span class="line"><span class="string">        :rtype: bool</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">searching</span>(<span class="params">root</span>):</span><br><span class="line">            _foreach = deque([root])</span><br><span class="line">            node_nums = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> _foreach:</span><br><span class="line">                <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(_foreach)):</span><br><span class="line">                    _current_node = _foreach.pop()</span><br><span class="line">                    node_nums += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">for</span> node <span class="keyword">in</span> [_current_node.left,_current_node.right]:</span><br><span class="line">                        <span class="keyword">if</span> node:</span><br><span class="line">                            _foreach.appendleft(_current_node)</span><br><span class="line">            <span class="keyword">return</span> node_nums</span><br><span class="line"></span><br><span class="line">        foreach = deque([root])</span><br><span class="line">        <span class="keyword">while</span> foreach:</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(foreach)):</span><br><span class="line">                current_node = foreach.pop()</span><br><span class="line">                <span class="keyword">if</span> current_node.val == x:</span><br><span class="line">                    <span class="comment"># if current_node.left:</span></span><br><span class="line">                    left_num = searching(current_node.left) </span><br><span class="line">                    <span class="keyword">if</span> current_node.right:</span><br><span class="line">                        right_num = searching(current_node.left) </span><br><span class="line">                    <span class="keyword">if</span> left_num <span class="keyword">and</span> right_num:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">                <span class="keyword">if</span> current_node.left.val == x:</span><br><span class="line">                    left_num = searching(current_node.left)</span><br><span class="line">                    right_num = searching(current_node.right)</span><br><span class="line">                    <span class="keyword">if</span> right_num &gt; left_num:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">                <span class="keyword">if</span> current_node.right.val == x:</span><br><span class="line">                    left_num = searching(current_node.left)</span><br><span class="line">                    right_num = searching(current_node.right)</span><br><span class="line">                    <span class="keyword">if</span> left_num &gt; right_num:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>后来发现searching写的有问题，不应该写循环，可是还是没通过，嘿嘿。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode(object):</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">btreeGameWinningMove</span>(<span class="params">self, root, n, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :type n: int</span></span><br><span class="line"><span class="string">        :type x: int</span></span><br><span class="line"><span class="string">        :rtype: bool</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">searching</span>(<span class="params">root</span>):</span><br><span class="line">            node_nums = <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> root:</span><br><span class="line">                node_nums += <span class="number">1</span></span><br><span class="line">                left_num = searching(root.left)</span><br><span class="line">                right_num = searching(root.right)</span><br><span class="line">                node_nums += left_num + right_num</span><br><span class="line">            <span class="keyword">return</span> node_nums</span><br><span class="line"></span><br><span class="line">        foreach = deque([root])</span><br><span class="line">        <span class="keyword">while</span> foreach:</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(foreach)):</span><br><span class="line">                current_node = foreach.pop()</span><br><span class="line">                <span class="keyword">if</span> current_node.val == x:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;current:&quot;</span>)</span><br><span class="line">                    <span class="comment"># if current_node.left:</span></span><br><span class="line">                    left_num = searching(current_node.left) </span><br><span class="line">                    right_num = searching(current_node.right)</span><br><span class="line">                    <span class="built_in">print</span>(left_num,right_num)</span><br><span class="line">                    <span class="comment"># if left_num &gt; (n - 1)  or right_num &gt; (n - 1) :</span></span><br><span class="line">                    <span class="keyword">if</span> left_num &gt; (right_num + <span class="number">1</span>) <span class="keyword">or</span> right_num &gt; (left_num + <span class="number">1</span>) :</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">                <span class="keyword">if</span> current_node.left <span class="keyword">and</span> current_node.left.val == x:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;left:&quot;</span>)</span><br><span class="line">                    left_num = searching(current_node.left)</span><br><span class="line">                    right_num = searching(current_node.right)</span><br><span class="line">                    <span class="built_in">print</span>(left_num,right_num,left_num&gt;right_num)</span><br><span class="line">                    <span class="keyword">if</span> (right_num + <span class="number">1</span>) &gt; left_num:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">                <span class="keyword">if</span> current_node.right <span class="keyword">and</span> current_node.right.val == x:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;right:&quot;</span>)</span><br><span class="line">                    left_num = searching(current_node.left)</span><br><span class="line">                    right_num = searching(current_node.right)</span><br><span class="line">                    <span class="built_in">print</span>(left_num,right_num)</span><br><span class="line">                    <span class="keyword">if</span> (left_num + <span class="number">1</span>) &gt; right_num:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">                <span class="keyword">if</span> current_node.left:</span><br><span class="line">                    foreach.appendleft(current_node.left)</span><br><span class="line">                <span class="keyword">if</span> current_node.right:</span><br><span class="line">                    foreach.appendleft(current_node.right)</span><br></pre></td></tr></table></figure>
<p>贴一个评论区的解决方案，没太看懂，嘿嘿。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="type">int</span> Lcnt;</span><br><span class="line"><span class="type">int</span> Rcnt;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">count</span><span class="params">(TreeNode* root)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(root)</span><br><span class="line">    &#123;</span><br><span class="line">        cnt++;</span><br><span class="line">        <span class="type">int</span> Lc = <span class="built_in">count</span>(root-&gt;left);</span><br><span class="line">        <span class="type">int</span> Rc = <span class="built_in">count</span>(root-&gt;right);</span><br><span class="line">        cnt += Lc + Rc;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> cnt;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(TreeNode* root,<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(root)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(root-&gt;val == x)</span><br><span class="line">        &#123;</span><br><span class="line">            Lcnt = <span class="built_in">count</span>(root-&gt;left);</span><br><span class="line">            Rcnt = <span class="built_in">count</span>(root-&gt;right);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">dfs</span>(root-&gt;left,x);</span><br><span class="line">            <span class="built_in">dfs</span>(root-&gt;right,x);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">btreeGameWinningMove</span><span class="params">(TreeNode* root, <span class="type">int</span> n, <span class="type">int</span> x)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">dfs</span>(root,x);</span><br><span class="line">    <span class="keyword">if</span>(n &gt; <span class="number">2</span> * (Lcnt + Rcnt + <span class="number">1</span>))<span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">if</span>(<span class="number">2</span> * Lcnt &gt; n)<span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">if</span>(<span class="number">2</span> * Rcnt &gt; n)<span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
      <tags>
        <tag>Leetcode贷</tag>
      </tags>
  </entry>
  <entry>
    <title>121. 买卖股票的最佳时机</title>
    <url>/121-%E4%B9%B0%E5%8D%96%E8%82%A1%E7%A5%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E6%97%B6%E6%9C%BA/</url>
    <content><![CDATA[<p>给定一个数组 prices ，它的第 i 个元素 prices[i] 表示一支给定股票第 i 天的价格。</p>
<p>你只能选择 某一天 买入这只股票，并选择在 未来的某一个不同的日子 卖出该股票。设计一个算法来计算你所能获取的最大利润。</p>
<p>返回你可以从这笔交易中获取的最大利润。如果你不能获取任何利润，返回 0 。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock">https://leetcode.cn/problems/best-time-to-buy-and-sell-stock</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<p>你说这是动态规划吧…没啥问题…你说不是吧…好像也没啥问题…毕竟连个dp的定义都没有…可能我用力过猛了…</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxProfit</span>(<span class="params">self, prices: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        min_price,max_profit = <span class="built_in">int</span>(<span class="number">1e9</span>), <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> prices:</span><br><span class="line">            min_price = <span class="built_in">min</span>(min_price,i)</span><br><span class="line">            max_profit = <span class="built_in">max</span>(max_profit,i - min_price)</span><br><span class="line">        <span class="keyword">return</span> max_profit</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>1233. 删除子文件夹</title>
    <url>/1233-%E5%88%A0%E9%99%A4%E5%AD%90%E6%96%87%E4%BB%B6%E5%A4%B9/</url>
    <content><![CDATA[<p>你是一位系统管理员，手里有一份文件夹列表 folder，你的任务是要删除该列表中的所有 子文件夹，并以 任意顺序 返回剩下的文件夹。<br>如果文件夹 folder[i] 位于另一个文件夹 folder[j] 下，那么 folder[i] 就是 folder[j] 的 子文件夹 。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/remove-sub-folders-from-the-filesystem">https://leetcode.cn/problems/remove-sub-folders-from-the-filesystem</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<p>在官解中这道题有两种解法，一种是基于排序的方法，一种是基于字典树的方法。基于排序的方法就是将所有输入的path进行排序然后再进行相应处理，稍微研究了一下没通过，也觉得不是“正经”解法，这道题应该是可以应用数据结构的，然后看到了方法二，基于字典树的方法。</p>
<img src="/1233-%E5%88%A0%E9%99%A4%E5%AD%90%E6%96%87%E4%BB%B6%E5%A4%B9/%E5%AD%97%E5%85%B8%E6%A0%91.webp" class="" title="字典树">

<p>输入的每条路径中，可以将每一层文件夹当做一个节点，添加一个向量ref，如果节点是输入中的一个路径，则ref为该路径在输入中的索引。将所有路径都保存在字典树中后，对第一层的各节点进行一次深度遍历，只要遇到ref不为-1的节点则将路径添加至答案中，因为后续的都是该节点的子文件夹。<br>这里仍然还有一个疑点，在dfs函数中的return的写法，没太看懂。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Trie</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.children = <span class="built_in">dict</span>()</span><br><span class="line">        self.ref = -<span class="number">1</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeSubfolders</span>(<span class="params">self, folder</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type folder: List[str]</span></span><br><span class="line"><span class="string">        :rtype: List[str]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> fold <span class="keyword">in</span> folder:</span><br><span class="line">            <span class="built_in">print</span>(fold.split(<span class="string">&quot;/&quot;</span>)[<span class="number">1</span>:])</span><br><span class="line">        root = Trie()</span><br><span class="line">        <span class="keyword">for</span> i,fold <span class="keyword">in</span> <span class="built_in">enumerate</span>(folder):</span><br><span class="line">            fold = fold.split(<span class="string">&quot;/&quot;</span>)</span><br><span class="line">            cur = root</span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> fold:</span><br><span class="line">                <span class="keyword">if</span> name <span class="keyword">not</span> <span class="keyword">in</span> cur.children:</span><br><span class="line">                    cur.children[name] = Trie()</span><br><span class="line">                cur = cur.children[name]</span><br><span class="line">            cur.ref = i</span><br><span class="line">        ans = <span class="built_in">list</span>()</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">cur</span>):</span><br><span class="line">            <span class="keyword">if</span> cur.ref != -<span class="number">1</span>:</span><br><span class="line">                ans.append(folder[cur.ref])</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            <span class="keyword">for</span> child <span class="keyword">in</span> cur.children.values():</span><br><span class="line">                    dfs(child)</span><br><span class="line">        dfs(root)</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
      <tags>
        <tag>Leetcode贷</tag>
      </tags>
  </entry>
  <entry>
    <title>1250. 检查「好数组」</title>
    <url>/1250-%E6%A3%80%E6%9F%A5%E3%80%8C%E5%A5%BD%E6%95%B0%E7%BB%84%E3%80%8D/</url>
    <content><![CDATA[<p>给你一个正整数数组 nums，你需要从中任选一些子集，然后将子集中每一个数乘以一个 任意整数，并求出他们的和。<br>假如该和结果为 1，那么原数组就是一个「好数组」，则返回 True；否则请返回 False。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/check-if-it-is-a-good-array">https://leetcode.cn/problems/check-if-it-is-a-good-array</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<p>大乌龙。这做了二十多道题了才发现自己每天提交的不是python3。因为最大公约数的gcd方法死活找不到，math.gcd也找不到，from math import gcd也找不到，后来仔细一看才发现选择的环境不是Python3。。。。<br>通过这道题长见识了，知道了<a href="/%E8%A3%B4%E8%9C%80%E5%AE%9A%E7%90%86/" title="裴蜀定理">裴蜀定理</a>，内容会记在题解笔记里面。还可以研究一下python中gcd的实现，学习一下源码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isGoodArray</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">return</span> gcd(*nums) == <span class="number">1</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>1557. 可以到达所有点的最少点数目</title>
    <url>/1557-%E5%8F%AF%E4%BB%A5%E5%88%B0%E8%BE%BE%E6%89%80%E6%9C%89%E7%82%B9%E7%9A%84%E6%9C%80%E5%B0%91%E7%82%B9%E6%95%B0%E7%9B%AE/</url>
    <content><![CDATA[<p>给你一个 有向无环图 ， n 个节点编号为 0 到 n-1 ，以及一个边数组 edges ，其中 edges[i] &#x3D; [fromi, toi] 表示一条从点  fromi 到点 toi 的有向边。</p>
<p>找到最小的点集使得从这些点出发能到达图中所有点。题目保证解存在且唯一。</p>
<p>你可以以任意顺序返回这些节点编号。</p>
<p>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/minimum-number-of-vertices-to-reach-all-nodes">https://leetcode.cn/problems/minimum-number-of-vertices-to-reach-all-nodes</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<p>最开始的想法中，vertext并未转换为set类型，判断edge[1]是否在vertex中，如果在则删除，但是超时了。后来换成把入度非0的节点和vertex都换成集合就过了。。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findSmallestSetOfVertices</span>(<span class="params">self, n, edges</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type n: int</span></span><br><span class="line"><span class="string">        :type edges: List[List[int]]</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        vertex = <span class="built_in">set</span>(<span class="built_in">list</span>(<span class="built_in">range</span>(n)))</span><br><span class="line">        indegree = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">for</span> edge <span class="keyword">in</span> edges:</span><br><span class="line">            indegree.add(edge[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">list</span>(vertex - indegree)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>1604. 警告一小时内使用相同员工卡大于等于三次的人</title>
    <url>/1604-%E8%AD%A6%E5%91%8A%E4%B8%80%E5%B0%8F%E6%97%B6%E5%86%85%E4%BD%BF%E7%94%A8%E7%9B%B8%E5%90%8C%E5%91%98%E5%B7%A5%E5%8D%A1%E5%A4%A7%E4%BA%8E%E7%AD%89%E4%BA%8E%E4%B8%89%E6%AC%A1%E7%9A%84%E4%BA%BA/</url>
    <content><![CDATA[<p>力扣公司的员工都使用员工卡来开办公室的门。每当一个员工使用一次他的员工卡，安保系统会记录下员工的名字和使用时间。如果一个员工在一小时时间内使用员工卡的次数大于等于三次，这个系统会自动发布一个 警告 。<br>给你字符串数组 keyName 和 keyTime ，其中 [keyName[i], keyTime[i]] 对应一个人的名字和他在 某一天 内使用员工卡的时间。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/alert-using-same-key-card-three-or-more-times-in-a-one-hour-period">https://leetcode.cn/problems/alert-using-same-key-card-three-or-more-times-in-a-one-hour-period</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<p>先把keytime处理成int，然后用np.argsort返回keytime的升序索引，下面遍历的时候直接用这个升序索引访问name和time。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> argsort</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">alertNames</span>(<span class="params">self, keyName, keyTime</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type keyName: List[str]</span></span><br><span class="line"><span class="string">        :type keyTime: List[str]</span></span><br><span class="line"><span class="string">        :rtype: List[str]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        keyTime = [<span class="built_in">int</span>(k.replace(<span class="string">&quot;:&quot;</span>,<span class="string">&quot;&quot;</span>)) <span class="keyword">for</span> k <span class="keyword">in</span> keyTime]</span><br><span class="line">        sequence = argsort(keyTime)</span><br><span class="line">        result = <span class="built_in">set</span>()</span><br><span class="line">        order = defaultdict(<span class="built_in">list</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> sequence:</span><br><span class="line">            name = keyName[index]</span><br><span class="line">            order[name].append(keyTime[index])</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(order[name]) &gt;= <span class="number">3</span>:</span><br><span class="line">                <span class="keyword">if</span> (<span class="built_in">int</span>(order[name][-<span class="number">1</span>]) - <span class="built_in">int</span>(order[name][-<span class="number">3</span>])) &lt;= <span class="number">100</span> :</span><br><span class="line">                    result.add(name)</span><br><span class="line">        result = <span class="built_in">list</span>(result)</span><br><span class="line">        result.sort()</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>1669. 合并两个链表</title>
    <url>/1669-%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E9%93%BE%E8%A1%A8/</url>
    <content><![CDATA[<p>给你两个链表 list1 和 list2 ，它们包含的元素分别为 n 个和 m 个。请你将 list1 中下标从 a 到 b 的全部节点都删除，并将list2 接在被删除节点的位置，请你返回结果链表的头指针。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/merge-in-between-linked-lists">https://leetcode.cn/problems/merge-in-between-linked-lists</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode(object):</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, next=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.next = next</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mergeInBetween</span>(<span class="params">self, list1, a, b, list2</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type list1: ListNode</span></span><br><span class="line"><span class="string">        :type a: int</span></span><br><span class="line"><span class="string">        :type b: int</span></span><br><span class="line"><span class="string">        :type list2: ListNode</span></span><br><span class="line"><span class="string">        :rtype: ListNode</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">#保存链表起始位置</span></span><br><span class="line">        head_node = list1</span><br><span class="line">        temp_node = list1</span><br><span class="line">        pre_node,post_node = <span class="literal">None</span>,<span class="literal">None</span></span><br><span class="line">        <span class="comment">#根据索引保存欲删除节点的前一个和后一个节点</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(b):</span><br><span class="line">            <span class="keyword">if</span> i == (a-<span class="number">1</span>) :</span><br><span class="line">                pre_node = list1</span><br><span class="line">                list1 = list1.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                list1 = list1.<span class="built_in">next</span></span><br><span class="line">        post_node = list1.<span class="built_in">next</span></span><br><span class="line">        <span class="comment">#将list2接入list1</span></span><br><span class="line">        pre_node.<span class="built_in">next</span> = list2</span><br><span class="line">        <span class="comment">#从头遍历list1，找到list2的最后一个节点</span></span><br><span class="line">        <span class="comment">#这个循环最开始写的是 while temp_node，可是一直无法将后续节点补回list1</span></span><br><span class="line">        <span class="comment">#因为需要将最后一个节点的next连接到补回节点中，直接遍历到空节点即使赋值，在原链表中也无法找到后续节点</span></span><br><span class="line">        <span class="keyword">while</span> temp_node.<span class="built_in">next</span>:</span><br><span class="line">            temp_node = temp_node.<span class="built_in">next</span></span><br><span class="line">        <span class="comment">#将后续节点补入list1</span></span><br><span class="line">        temp_node.<span class="built_in">next</span> = post_node</span><br><span class="line">        <span class="keyword">return</span> head_node</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>1791. 找出星型图的中心节点</title>
    <url>/1791-%E6%89%BE%E5%87%BA%E6%98%9F%E5%9E%8B%E5%9B%BE%E7%9A%84%E4%B8%AD%E5%BF%83%E8%8A%82%E7%82%B9/</url>
    <content><![CDATA[<p>有一个无向的 星型 图，由 n 个编号从 1 到 n 的节点组成。星型图有一个 中心 节点，并且恰有 n - 1 条边将中心节点与其他每个节点连接起来。<br>给你一个二维整数数组 edges ，其中 edges[i] &#x3D; [ui, vi] 表示在节点 ui 和 vi 之间存在一条边。请你找出并返回 edges 所表示星型图的中心节点。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/find-center-of-star-graph">https://leetcode.cn/problems/find-center-of-star-graph</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findCenter</span>(<span class="params">self, edges</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type edges: List[List[int]]</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        degree_status = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        <span class="keyword">for</span> edge <span class="keyword">in</span> edges:</span><br><span class="line">            degree_status[edge[<span class="number">1</span>]] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(degree_status,key=degree_status.get)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>1797. 设计一个验证系统</title>
    <url>/1797-%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E9%AA%8C%E8%AF%81%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<p>你需要设计一个包含验证码的验证系统。每一次验证中，用户会收到一个新的验证码，这个验证码在 currentTime 时刻之后 timeToLive 秒过期。如果验证码被更新了，那么它会在 currentTime （可能与之前的 currentTime 不同）时刻延长 timeToLive 秒。<br>请你实现 AuthenticationManager 类：<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/design-authentication-manager">https://leetcode.cn/problems/design-authentication-manager</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<p>莫名其妙的一道题，题干描述莫名其妙，通过的也莫名其妙，没啥营养，也不知道有啥优化方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AuthenticationManager</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, timeToLive</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type timeToLive: int</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.timeToLive = timeToLive</span><br><span class="line">        self.token_limit = <span class="built_in">dict</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, tokenId, currentTime</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type tokenId: str</span></span><br><span class="line"><span class="string">        :type currentTime: int</span></span><br><span class="line"><span class="string">        :rtype: None</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.token_limit[tokenId] = currentTime + self.timeToLive</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">renew</span>(<span class="params">self, tokenId, currentTime</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type tokenId: str</span></span><br><span class="line"><span class="string">        :type currentTime: int</span></span><br><span class="line"><span class="string">        :rtype: None</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> tokenId <span class="keyword">in</span> self.token_limit:</span><br><span class="line">            <span class="keyword">if</span> currentTime &lt; self.token_limit[tokenId]:</span><br><span class="line">                self.token_limit[tokenId] = currentTime + self.timeToLive</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">countUnexpiredTokens</span>(<span class="params">self, currentTime</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type currentTime: int</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> k,v <span class="keyword">in</span> self.token_limit.items():</span><br><span class="line">            <span class="keyword">if</span> currentTime &lt; v:</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> count </span><br><span class="line"><span class="comment"># Your AuthenticationManager object will be instantiated and called as such:</span></span><br><span class="line"><span class="comment"># obj = AuthenticationManager(timeToLive)</span></span><br><span class="line"><span class="comment"># obj.generate(tokenId,currentTime)</span></span><br><span class="line"><span class="comment"># obj.renew(tokenId,currentTime)</span></span><br><span class="line"><span class="comment"># param_3 = obj.countUnexpiredTokens(currentTime)</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>2.两数相加</title>
    <url>/2-%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/</url>
    <content><![CDATA[<p>给你两个 非空 的链表，表示两个非负的整数。它们每位数字都是按照 逆序 的方式存储的，并且每个节点只能存储 一位 数字。请你将两个数相加，并以相同形式返回一个表示和的链表。你可以假设除了数字 0 之外，这两个数都不会以 0 开头。<br>示例 1：</p>
<img src="/2-%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/%E5%9B%BE%E4%BE%8B.png" class="" title="图例">
<p>输入：l1 &#x3D; [2,4,3], l2 &#x3D; [5,6,4]<br>输出：[7,0,8]<br>解释：342 + 465 &#x3D; 807.<br>示例 2：<br>输入：l1 &#x3D; [0], l2 &#x3D; [0]<br>输出：[0]<br>示例 3：<br>输入：l1 &#x3D; [9,9,9,9,9,9,9], l2 &#x3D; [9,9,9,9]<br>输出：[8,9,9,9,0,0,0,1]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode(object):</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, next=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.next = next</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addTwoNumbers</span>(<span class="params">self, l1, l2</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type l1: ListNode</span></span><br><span class="line"><span class="string">        :type l2: ListNode</span></span><br><span class="line"><span class="string">        :rtype: ListNode</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        carry = <span class="number">0</span></span><br><span class="line">        res = ListNode()</span><br><span class="line">        record = res <span class="comment">#记录起点</span></span><br><span class="line">        <span class="keyword">while</span> l1 <span class="keyword">or</span> l2  :</span><br><span class="line">            x = l1.val <span class="keyword">if</span> l1 <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            y = l2.val <span class="keyword">if</span> l2 <span class="keyword">else</span> <span class="number">0</span>          </span><br><span class="line">            carry,sums = (x + y + carry) // <span class="number">10</span>,(x + y + carry) % <span class="number">10</span></span><br><span class="line">            res.<span class="built_in">next</span> = ListNode(sums)</span><br><span class="line">            res = res.<span class="built_in">next</span></span><br><span class="line">            l1 = l1.<span class="built_in">next</span> <span class="keyword">if</span> l1 <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">            l2 = l2.<span class="built_in">next</span> <span class="keyword">if</span> l2 <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> carry != <span class="number">0</span>:</span><br><span class="line">            res.<span class="built_in">next</span> = ListNode(carry)</span><br><span class="line">        <span class="keyword">return</span> record.<span class="built_in">next</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>2319. 判断矩阵是否是一个 X 矩阵</title>
    <url>/2319-%E5%88%A4%E6%96%AD%E7%9F%A9%E9%98%B5%E6%98%AF%E5%90%A6%E6%98%AF%E4%B8%80%E4%B8%AA-X-%E7%9F%A9%E9%98%B5/</url>
    <content><![CDATA[<p>如果一个正方形矩阵满足下述 全部 条件，则称之为一个 X 矩阵 ：矩阵对角线上的所有元素都 不是 0，矩阵中所有其他元素都是 0。<br>给你一个大小为 n x n 的二维整数数组 grid ，表示一个正方形矩阵。如果 grid 是一个 X 矩阵 ，返回 true ；否则，返回 false 。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/check-if-matrix-is-x-matrix">https://leetcode.cn/problems/check-if-matrix-is-x-matrix</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<p>题干中的grid是嵌套的列表，第一时间就想到了把这个列表转成np.array()然后再通过矩阵相关的方法去处理，用时150ms，然后在评论区发现有人用Java答题的思路跟我一样，但是他只需要1ms甚至0ms。<br>找耗时长的过程中发现，虽然我把列表转换成了np.array，但是这种转换并没有带来任何方便，然后我就直接在原始列表上操作数据，时间从150ms降到了70-80ms，但这与1ms仍差距甚远，<br>然后发现虽然我去掉了np.array的转换操作。。。但是import np的语句我没删掉。。去掉引包语句之后时间降到了30ms。。。引包的耗时还是挺长的。。。<br>可是可是可是，为什么相同的解题思路python要30ms，Java只要1ms甚至0ms呢？<br>还有更奇怪的，下面最后一个，也就是第四个代码块的实现方式理论上遍历的次数要比代码块3要少。但是代码块3只要30ms，而代码块4要将近70？？因为测试用例中True的情况比较多？False的情况比较少?<br>为啥Java比python快那么多？为啥硬遍历比计算diff还要快？？我不理解。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">checkXMatrix</span>(<span class="params">self, grid</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type grid: List[List[int]]</span></span><br><span class="line"><span class="string">        :rtype: bool</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">		matrix = np.array(grid)        </span><br><span class="line">        shape = matrix.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(shape):</span><br><span class="line">            diff = shape - i - <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> matrix[i,i] == <span class="number">0</span> <span class="keyword">or</span> matrix[i,diff] == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(shape):</span><br><span class="line">                <span class="keyword">if</span> j != i <span class="keyword">and</span> j != diff:</span><br><span class="line">                    <span class="keyword">if</span> matrix[i,j] != <span class="number">0</span>:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span>    </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">checkXMatrix</span>(<span class="params">self, grid</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type grid: List[List[int]]</span></span><br><span class="line"><span class="string">        :rtype: bool</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        matrix = np.array(grid)        </span><br><span class="line">        shape = matrix.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(shape):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(shape):</span><br><span class="line">                <span class="keyword">if</span> i == j <span class="keyword">or</span> ((i + j) == (shape - <span class="number">1</span>) ):</span><br><span class="line">                    <span class="keyword">if</span> matrix[i,j] == <span class="number">0</span>:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> matrix[i,j] != <span class="number">0</span>:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="built_in">print</span>(matrix)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">checkXMatrix</span>(<span class="params">self, grid</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type grid: List[List[int]]</span></span><br><span class="line"><span class="string">        :rtype: bool</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span>  </span><br><span class="line">        shape = <span class="built_in">len</span>(grid)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(shape):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(shape):</span><br><span class="line">                <span class="keyword">if</span> i == j <span class="keyword">or</span> ((i + j) == (shape - <span class="number">1</span>) ):</span><br><span class="line">                    <span class="keyword">if</span> grid[i][j] == <span class="number">0</span>:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> grid[i][j] != <span class="number">0</span>:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">checkXMatrix</span>(<span class="params">self, grid</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type grid: List[List[int]]</span></span><br><span class="line"><span class="string">        :rtype: bool</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span>  </span><br><span class="line">        shape = <span class="built_in">len</span>(grid)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(shape):</span><br><span class="line">            diff = shape - i - <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> grid[i][i] == <span class="number">0</span> <span class="keyword">or</span> grid[i][diff] == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(shape):</span><br><span class="line">                <span class="keyword">if</span> j != i <span class="keyword">and</span> j != diff:</span><br><span class="line">                    <span class="keyword">if</span> grid[i][j] != <span class="number">0</span>:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>2325.解密消息</title>
    <url>/2325-%E8%A7%A3%E5%AF%86%E6%B6%88%E6%81%AF/</url>
    <content><![CDATA[<p>给你字符串 key 和 message ，分别表示一个加密密钥和一段加密消息。解密 message 的步骤如下：<br>使用 key 中 26 个英文小写字母第一次出现的顺序作为替换表中的字母 顺序 。<br>将替换表与普通英文字母表对齐，形成对照表。<br>按照对照表 替换 message 中的每个字母。<br>空格 ‘ ‘ 保持不变。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/decode-the-message">https://leetcode.cn/problems/decode-the-message</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decodeMessage</span>(<span class="params">self, key, message</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type key: str</span></span><br><span class="line"><span class="string">        :type message: str</span></span><br><span class="line"><span class="string">        :rtype: str</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">#统一存ASCII码，用chr()将ASCII转换为字母，空格符ASCII码是32</span></span><br><span class="line">        mapping_dict = <span class="built_in">dict</span>(&#123;<span class="string">&quot; &quot;</span>:<span class="number">32</span>&#125;)</span><br><span class="line">        start_index = <span class="number">97</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> key:</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> mapping_dict:</span><br><span class="line">                mapping_dict[i] = start_index</span><br><span class="line">                start_index += <span class="number">1</span> </span><br><span class="line">        decode = <span class="built_in">str</span>()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> message:</span><br><span class="line">            decode += <span class="built_in">chr</span>(mapping_dict[i])</span><br><span class="line">        <span class="keyword">return</span> decode</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>2331. 计算布尔二叉树的值</title>
    <url>/2331-%E8%AE%A1%E7%AE%97%E5%B8%83%E5%B0%94%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E5%80%BC/</url>
    <content><![CDATA[<p>给你一棵 完整二叉树 的根，这棵树有以下特征：</p>
<p>叶子节点 要么值为 0 要么值为 1 ，其中 0 表示 False ，1 表示 True 。<br>非叶子节点 要么值为 2 要么值为 3 ，其中 2 表示逻辑或 OR ，3 表示逻辑与 AND 。<br>计算 一个节点的值方式如下：<br>如果节点是个叶子节点，那么节点的 值 为它本身，即 True 或者 False 。<br>否则，计算 两个孩子的节点值，然后将该节点的运算符对两个孩子值进行 运算 。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/evaluate-boolean-binary-tree">https://leetcode.cn/problems/evaluate-boolean-binary-tree</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode(object):</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evaluateTree</span>(<span class="params">self, root</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type root: Optional[TreeNode]</span></span><br><span class="line"><span class="string">        :rtype: bool</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">node</span>):</span><br><span class="line">            <span class="keyword">if</span> node.val == <span class="number">2</span> :  </span><br><span class="line">                <span class="keyword">return</span> dfs(node.left) <span class="keyword">or</span> dfs(node.right)</span><br><span class="line">            <span class="keyword">elif</span> node.val == <span class="number">3</span> :</span><br><span class="line">                <span class="keyword">return</span> dfs(node.left) <span class="keyword">and</span> dfs(node.right)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> node.val == <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> dfs(root)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>2335. 装满杯子需要的最短总时长</title>
    <url>/2335-%E8%A3%85%E6%BB%A1%E6%9D%AF%E5%AD%90%E9%9C%80%E8%A6%81%E7%9A%84%E6%9C%80%E7%9F%AD%E6%80%BB%E6%97%B6%E9%95%BF/</url>
    <content><![CDATA[<p>现有一台饮水机，可以制备冷水、温水和热水。每秒钟，可以装满 2 杯 不同 类型的水或者 1 杯任意类型的水。<br>给你一个下标从 0 开始、长度为 3 的整数数组 amount ，其中 amount[0]、amount[1] 和 amount[2] 分别表示需要装满冷水、温水和热水的杯子数量。返回装满所有杯子所需的 最少 秒数。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/minimum-amount-of-time-to-fill-cups">https://leetcode.cn/problems/minimum-amount-of-time-to-fill-cups</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<p>评论区都是数学解，我就想用程序解，然后被一道简单题干服了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fillCups</span>(<span class="params">self, amount</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type amount: List[int]</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        sequence = deque()</span><br><span class="line">        mapping = &#123;<span class="number">0</span>:<span class="string">&quot;Cold&quot;</span>,<span class="number">1</span>:<span class="string">&quot;Normal&quot;</span>,<span class="number">2</span>:<span class="string">&quot;Hot&quot;</span>&#125;</span><br><span class="line">        <span class="comment">#每次将amount中的最大值和最小值对应的饮品加入队列</span></span><br><span class="line">        <span class="keyword">while</span> amount[<span class="number">0</span>] <span class="keyword">or</span> amount[<span class="number">1</span>] <span class="keyword">or</span> amount[<span class="number">2</span>]:</span><br><span class="line">            index_max = amount.index(<span class="built_in">max</span>(amount))</span><br><span class="line">            sequence.appendleft(mapping[index_max])</span><br><span class="line">            amount[index_max] -= <span class="number">1</span></span><br><span class="line">            <span class="comment">#杯数相同时直接使用index会报错，所以需要带着索引排序，就可以直接使用最小值的索引</span></span><br><span class="line">            temp = [(amount[i],i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(amount)) <span class="keyword">if</span> amount[i] &gt; <span class="number">0</span> <span class="keyword">and</span> i != index_max]</span><br><span class="line">            temp.sort()</span><br><span class="line">            <span class="keyword">if</span> temp:</span><br><span class="line">                index_min = temp[-<span class="number">1</span>][-<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">if</span> index_max != index_min:</span><br><span class="line">                    sequence.appendleft(mapping[index_min])</span><br><span class="line">                    amount[index_min] -= <span class="number">1</span></span><br><span class="line">        cost = <span class="number">0</span></span><br><span class="line">        <span class="comment">#每次从队列中pop两个值出来，不同cost + 1，相同cost + 2，有空值或均为空时退出循环</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                type1 = sequence.pop()</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                type1 = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                type2 = sequence.pop()</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                type2 = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">if</span> type1 <span class="keyword">and</span> type2:</span><br><span class="line">                <span class="keyword">if</span> type1 != type2:</span><br><span class="line">                    cost += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    cost += <span class="number">2</span></span><br><span class="line">            <span class="keyword">elif</span> type1 <span class="keyword">or</span> type2:</span><br><span class="line">                cost += <span class="number">1</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>2341. 数组能形成多少数对</title>
    <url>/2341-%E6%95%B0%E7%BB%84%E8%83%BD%E5%BD%A2%E6%88%90%E5%A4%9A%E5%B0%91%E6%95%B0%E5%AF%B9/</url>
    <content><![CDATA[<p>给你一个下标从 0 开始的整数数组 nums 。在一步操作中，你可以执行以下步骤：</p>
<p>从 nums 选出 两个 相等的 整数<br>从 nums 中移除这两个整数，形成一个 数对<br>请你在 nums 上多次执行此操作直到无法继续执行。</p>
<p>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/maximum-number-of-pairs-in-array">https://leetcode.cn/problems/maximum-number-of-pairs-in-array</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">numberOfPairs</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        pair_num = <span class="number">0</span></span><br><span class="line">        remaining_num = <span class="built_in">list</span>()</span><br><span class="line">        <span class="keyword">while</span> nums:</span><br><span class="line">            item = nums.pop()</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                index = nums.index(item)</span><br><span class="line">                nums.pop(index)</span><br><span class="line">                pair_num += <span class="number">1</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                remaining_num.append(item)</span><br><span class="line">        <span class="keyword">return</span> [pair_num,<span class="built_in">len</span>(remaining_num)]</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>2347. 最好的扑克手牌</title>
    <url>/2347-%E6%9C%80%E5%A5%BD%E7%9A%84%E6%89%91%E5%85%8B%E6%89%8B%E7%89%8C/</url>
    <content><![CDATA[<p>给你一个整数数组 ranks 和一个字符数组 suit 。你有 5 张扑克牌，第 i 张牌大小为 ranks[i] ，花色为 suits[i] 。</p>
<p>下述是从好到坏你可能持有的 手牌类型 ：</p>
<p>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/best-poker-hand">https://leetcode.cn/problems/best-poker-hand</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">bestHand</span>(<span class="params">self, ranks: <span class="type">List</span>[<span class="built_in">int</span>], suits: <span class="type">List</span>[<span class="built_in">str</span>]</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        dic = <span class="built_in">dict</span>(Counter(ranks))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(<span class="built_in">set</span>(suits)) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;Flush&quot;</span></span><br><span class="line">        <span class="keyword">elif</span> dic[<span class="built_in">max</span>(dic,key=dic.get)] &gt;= <span class="number">3</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;Three of a Kind&quot;</span></span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">len</span>(<span class="built_in">set</span>(ranks)) &lt;= <span class="number">4</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;Pair&quot;</span></span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">len</span>(<span class="built_in">set</span>(ranks)) == <span class="number">5</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&quot;High Card&quot;</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>2389. 和有限的最长子序列</title>
    <url>/2389-%E5%92%8C%E6%9C%89%E9%99%90%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E5%BA%8F%E5%88%97/</url>
    <content><![CDATA[<p>给你一个长度为 n 的整数数组 nums ，和一个长度为 m 的整数数组 queries 。<br>返回一个长度为 m 的数组 answer ，其中 answer[i] 是 nums 中 元素之和小于等于 queries[i] 的 子序列 的 最大 长度  。<br>子序列 是由一个数组删除某些元素（也可以不删除）但不改变剩余元素顺序得到的一个数组。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/longest-subsequence-with-limited-sum">https://leetcode.cn/problems/longest-subsequence-with-limited-sum</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。<br>这道题拿过来第一眼就觉得nums可以排序，然后又仔细看了看题目说不能影响顺序，就放弃了排序的想法，经过评论区提醒才发现，加法条件，排序不排序对元素顺序哪有什么影响。。又是被忽悠的一天。。。<br>可以排序就省事很多了，首先想到的就是循环解决，然后觉得递归也能解决，然后还觉得quries那里应该也可以做一些手脚。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">answerQueries</span>(<span class="params">self, nums, queries</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :type queries: List[int]</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        nums.sort()</span><br><span class="line">        answer = <span class="built_in">list</span>()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(queries)):</span><br><span class="line">            res = <span class="number">0</span></span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> item <span class="keyword">in</span> nums:</span><br><span class="line">                res += item</span><br><span class="line">                <span class="keyword">if</span> res &lt;= queries[i]:</span><br><span class="line">                    count += <span class="number">1</span></span><br><span class="line">                <span class="comment"># else:</span></span><br><span class="line">            answer.append(count) </span><br><span class="line">        <span class="keyword">return</span> answer        </span><br></pre></td></tr></table></figure>
<p>然后想办法写成递归，好像写不成递归啊，递归的出口是已知的，这里是未知的，所以应该写不成递归？？quries想了想好像也没什么动手脚的余地，折腾来折腾去每次遍历的次数也不会改变。<br>然后在官解中看到了“<a href="/%E5%89%8D%E7%BC%80%E5%92%8C/" title="前缀和">前缀和</a>”的概念。</p>
]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>2443. 反转之后的数字和</title>
    <url>/2443-%E5%8F%8D%E8%BD%AC%E4%B9%8B%E5%90%8E%E7%9A%84%E6%95%B0%E5%AD%97%E5%92%8C/</url>
    <content><![CDATA[<p>给你一个 非负 整数 num 。如果存在某个 非负 整数 k 满足 k + reverse(k) &#x3D; num  ，则返回 true ；否则，返回 false 。<br>reverse(k) 表示 k 反转每个数位后得到的数字。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/sum-of-number-and-its-reverse">https://leetcode.cn/problems/sum-of-number-and-its-reverse</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sumOfNumberAndReverse</span>(<span class="params">self, num</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type num: int</span></span><br><span class="line"><span class="string">        :rtype: bool</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> num == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        flag = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,num,-<span class="number">1</span>):</span><br><span class="line">            rev = <span class="built_in">int</span>(<span class="string">&quot;&quot;</span>.join(<span class="built_in">reversed</span>(<span class="built_in">str</span>(i))))</span><br><span class="line">            <span class="keyword">if</span> i + rev == num:</span><br><span class="line">                flag = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">return</span> flag</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>27-移除元素</title>
    <url>/26-%E5%88%A0%E9%99%A4%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E9%A1%B9/</url>
    <content><![CDATA[<p>给你一个 升序排列 的数组 nums ，请你 原地 删除重复出现的元素，使每个元素 只出现一次 ，返回删除后数组的新长度。元素的 相对顺序 应该保持 一致 。然后返回 nums 中唯一元素的个数。</p>
<p>考虑 nums 的唯一元素的数量为 k ，你需要做以下事情确保你的题解可以被通过：</p>
<p>更改数组 nums ，使 nums 的前 k 个元素包含唯一元素，并按照它们最初在 nums 中出现的顺序排列。nums 的其余元素与 nums 的大小不重要。<br>返回 k 。</p>
<p>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/remove-duplicates-from-sorted-array">https://leetcode.cn/problems/remove-duplicates-from-sorted-array</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<p>记录一个val，转换为与<a href="/27-%E7%A7%BB%E9%99%A4%E5%85%83%E7%B4%A0/" title="27-移除元素">27-移除元素</a>相同的解决方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeDuplicates</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        slow,quick = <span class="number">0</span>,<span class="number">0</span></span><br><span class="line">        val = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">while</span> quick &lt; <span class="built_in">len</span>(nums):</span><br><span class="line">            <span class="keyword">if</span> val <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                val = nums[slow]</span><br><span class="line">                slow += <span class="number">1</span></span><br><span class="line">                quick += <span class="number">1</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> nums[quick] != val:</span><br><span class="line">                nums[slow] = nums[quick]</span><br><span class="line">                val = nums[slow]</span><br><span class="line">                slow += <span class="number">1</span></span><br><span class="line">            quick += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> slow</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>移除元素</tag>
      </tags>
  </entry>
  <entry>
    <title>27-移除元素</title>
    <url>/27-%E7%A7%BB%E9%99%A4%E5%85%83%E7%B4%A0/</url>
    <content><![CDATA[<p>给你一个数组 nums 和一个值 val，你需要 原地 移除所有数值等于 val 的元素，并返回移除后数组的新长度。</p>
<p>不要使用额外的数组空间，你必须仅使用 O(1) 额外空间并 原地 修改输入数组。</p>
<p>元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。</p>
<p>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/remove-element">https://leetcode.cn/problems/remove-element</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<p>一个一个的找，fast用来找与val不同的值，slow用于保存当前找到的值。找到了fast就会与slow不同，不同则将用fast将slow覆盖。<br>在本题中，实际上每次都会把slow这个位置的数值返回到结果集合中，所以leetcode识别的结果是每一次slow对应的数值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeElement</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], val: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        slow,fast = <span class="number">0</span>,<span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> fast &lt; <span class="built_in">len</span>(nums):</span><br><span class="line">            <span class="keyword">if</span> nums[fast] != val:</span><br><span class="line">                nums[slow] = nums[fast]</span><br><span class="line">                slow += <span class="number">1</span></span><br><span class="line">            fast += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> slow</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>移除元素</tag>
      </tags>
  </entry>
  <entry>
    <title>34. 在排序数组中查找元素的第一个和最后一个位置</title>
    <url>/34-%E5%9C%A8%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E6%9F%A5%E6%89%BE%E5%85%83%E7%B4%A0%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%92%8C%E6%9C%80%E5%90%8E%E4%B8%80%E4%B8%AA%E4%BD%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>给你一个按照非递减顺序排列的整数数组 nums，和一个目标值 target。请你找出给定目标值在数组中的开始位置和结束位置。<br>如果数组中不存在目标值 target，返回 [-1, -1]。<br>你必须设计并实现时间复杂度为 O(log n) 的算法解决此问题。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/find-first-and-last-position-of-element-in-sorted-array">https://leetcode.cn/problems/find-first-and-last-position-of-element-in-sorted-array</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">searchRange</span>(<span class="params">self, nums, target</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :type target: int</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        result = [-<span class="number">1</span>,-<span class="number">1</span>]</span><br><span class="line">        left_flag,right_flag = <span class="number">0</span>,<span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">len</span>(nums):</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            left,right = <span class="number">0</span>,<span class="built_in">len</span>(nums) - <span class="number">1</span> </span><br><span class="line">            <span class="keyword">while</span>(left &lt;= right):</span><br><span class="line">                <span class="keyword">if</span> nums[left] == target:</span><br><span class="line">                    result[<span class="number">0</span>] = left</span><br><span class="line">                    left_flag = <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    left += <span class="number">1</span> </span><br><span class="line">                <span class="keyword">if</span> nums[right] == target:</span><br><span class="line">                    result[<span class="number">1</span>] = right</span><br><span class="line">                    right_flag =<span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    right -= <span class="number">1</span> </span><br><span class="line">                <span class="keyword">if</span> right_flag <span class="keyword">and</span> left_flag:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
      <tags>
        <tag>二分查找</tag>
      </tags>
  </entry>
  <entry>
    <title>343. 整数拆分</title>
    <url>/343-%E6%95%B4%E6%95%B0%E6%8B%86%E5%88%86/</url>
    <content><![CDATA[<p>给定一个正整数 n ，将其拆分为 k 个 正整数 的和（ k &gt;&#x3D; 2 ），并使这些整数的乘积最大化。返回 你可以获得的最大乘积 。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/integer-break">https://leetcode.cn/problems/integer-break</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">integerBreak</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        dp = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(dp) &gt;= <span class="number">3</span>:</span><br><span class="line">            dp[<span class="number">1</span>] = <span class="number">0</span> </span><br><span class="line">            dp[<span class="number">2</span>] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>,n+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,index):</span><br><span class="line">                j = index - i</span><br><span class="line">                dp[index] = <span class="built_in">max</span>(i * j, i* dp[j],dp[index])</span><br><span class="line">        <span class="keyword">return</span> dp[n]</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>509. 斐波那契数</title>
    <url>/509-%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0/</url>
    <content><![CDATA[<p>斐波那契数 （通常用 F(n) 表示）形成的序列称为 斐波那契数列 。该数列由 0 和 1 开始，后面的每一项数字都是前面两项数字的和。也就是：<br>F(0) &#x3D; 0，F(1) &#x3D; 1<br>F(n) &#x3D; F(n - 1) + F(n - 2)，其中 n &gt; 1<br>给定 n ，请计算 F(n) 。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/fibonacci-number">https://leetcode.cn/problems/fibonacci-number</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fib</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> n &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> n</span><br><span class="line">        dp = [-<span class="number">1</span>] * (n + <span class="number">1</span>) </span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        dp[<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">        bala = [d <span class="keyword">for</span> d <span class="keyword">in</span> dp <span class="keyword">if</span> d != -<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span> (<span class="built_in">len</span>(bala),n + <span class="number">1</span>):</span><br><span class="line">            dp[i] = dp[i-<span class="number">1</span>] + dp[i-<span class="number">2</span>]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;i:<span class="subst">&#123;i&#125;</span>\ndp[i]:<span class="subst">&#123;dp[i]&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> dp[n]</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>62. 不同路径</title>
    <url>/62-%E4%B8%8D%E5%90%8C%E8%B7%AF%E5%BE%84/</url>
    <content><![CDATA[<p>一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为 “Start” ）。<br>机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为 “Finish” ）。<br>问总共有多少条不同的路径？<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/unique-paths">https://leetcode.cn/problems/unique-paths</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">uniquePaths</span>(<span class="params">self, m: <span class="built_in">int</span>, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment">#dp为，到当前位置有多少种路径</span></span><br><span class="line">        <span class="comment">#dp[i][j] = dp[i-1][j] + dp[i][j-1]</span></span><br><span class="line">        <span class="comment">#比示例图中多增加一行增加一列，全部为0，就可以按照行遍历的方式初始化dp</span></span><br><span class="line">        dp = [[<span class="number">0</span> <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>)] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m + <span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,m + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,n + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">and</span> j == <span class="number">1</span>:</span><br><span class="line">                    dp[<span class="number">1</span>][<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                dp[i][j] = dp[i-<span class="number">1</span>][j] + dp[i][j-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># #打印dp</span></span><br><span class="line">        <span class="comment"># for d in dp:</span></span><br><span class="line">        <span class="comment">#     print(d)</span></span><br><span class="line">        <span class="keyword">return</span> dp[m][n]</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>63. 不同路径 II</title>
    <url>/63-%E4%B8%8D%E5%90%8C%E8%B7%AF%E5%BE%84-II/</url>
    <content><![CDATA[<p>一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为 “Start” ）。<br>机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为 “Finish”）。<br>现在考虑网格中有障碍物。那么从左上角到右下角将会有多少条不同的路径？<br>网格中的障碍物和空位置分别用 1 和 0 来表示。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/unique-paths-ii">https://leetcode.cn/problems/unique-paths-ii</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">uniquePathsWithObstacles</span>(<span class="params">self, obstacleGrid: <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        row = <span class="built_in">len</span>(obstacleGrid) + <span class="number">1</span></span><br><span class="line">        col = <span class="built_in">len</span>(obstacleGrid[<span class="number">0</span>]) + <span class="number">1</span></span><br><span class="line">        dp = [[<span class="number">0</span> <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(col)]<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(row)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,row):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,col):</span><br><span class="line">                <span class="comment">#初始化</span></span><br><span class="line">                <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">and</span> j == <span class="number">1</span> <span class="keyword">and</span> obstacleGrid[i-<span class="number">1</span>][j-<span class="number">1</span>] == <span class="number">0</span>:</span><br><span class="line">                    dp[i][j] = <span class="number">1</span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                    <span class="comment"># print(f&quot;i:&#123;i&#125;,j:&#123;j&#125;&quot;)</span></span><br><span class="line">                <span class="keyword">if</span> obstacleGrid[i-<span class="number">1</span>][j-<span class="number">1</span>] != <span class="number">1</span>:</span><br><span class="line">                    dp[i][j] = dp[i][j-<span class="number">1</span>] + dp[i-<span class="number">1</span>][j]</span><br><span class="line">                    <span class="comment">#打印dp</span></span><br><span class="line">                    <span class="comment"># for d in dp:</span></span><br><span class="line">                    <span class="comment">#     print(d)</span></span><br><span class="line">        <span class="keyword">return</span> dp[row-<span class="number">1</span>][col-<span class="number">1</span>]</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>69. x 的平方根</title>
    <url>/69-x-%E7%9A%84%E5%B9%B3%E6%96%B9%E6%A0%B9/</url>
    <content><![CDATA[<p>给你一个非负整数 x ，计算并返回 x 的 算术平方根 。<br>由于返回类型是整数，结果只保留 整数部分 ，小数部分将被 舍去 。<br>注意：不允许使用任何内置指数函数和算符，例如 pow(x, 0.5) 或者 x ** 0.5 。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/sqrtx">https://leetcode.cn/problems/sqrtx</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mySqrt</span>(<span class="params">self, x: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        left,right,answer = <span class="number">0</span>,x,-<span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">            middle = (left + right) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> middle * middle &lt;= x:</span><br><span class="line">                answer = middle</span><br><span class="line">                left = middle + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                right = middle - <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> answer</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mySqrt</span>(<span class="params">self, x: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    	<span class="keyword">return</span> <span class="built_in">int</span>(sqrt(x))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
      <tags>
        <tag>二分查找</tag>
      </tags>
  </entry>
  <entry>
    <title>704. 二分查找</title>
    <url>/704-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/</url>
    <content><![CDATA[<p>给定一个 n 个元素有序的（升序）整型数组 nums 和一个目标值 target  ，写一个函数搜索 nums 中的 target，如果目标值存在返回下标，否则返回 -1。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/binary-search">https://leetcode.cn/problems/binary-search</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, nums, target</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :type target: int</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">while</span> left &lt;= right :</span><br><span class="line">            middle = (right + left) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> middle &gt; <span class="built_in">len</span>(nums) - <span class="number">1</span> :</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> nums[middle] &lt; target:</span><br><span class="line">                left = middle + <span class="number">1</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> nums[middle] &gt; target:</span><br><span class="line">                right = middle - <span class="number">1</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> nums[middle] == target:</span><br><span class="line">                <span class="keyword">return</span> middle</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
      <tags>
        <tag>二分查找</tag>
      </tags>
  </entry>
  <entry>
    <title>746. 使用最小花费爬楼梯</title>
    <url>/746-%E4%BD%BF%E7%94%A8%E6%9C%80%E5%B0%8F%E8%8A%B1%E8%B4%B9%E7%88%AC%E6%A5%BC%E6%A2%AF/</url>
    <content><![CDATA[<p>给你一个整数数组 cost ，其中 cost[i] 是从楼梯第 i 个台阶向上爬需要支付的费用。一旦你支付此费用，即可选择向上爬一个或者两个台阶。<br>你可以选择从下标为 0 或下标为 1 的台阶开始爬楼梯。<br>请你计算并返回达到楼梯顶部的最低花费。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/min-cost-climbing-stairs">https://leetcode.cn/problems/min-cost-climbing-stairs</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minCostClimbingStairs</span>(<span class="params">self, cost: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        dp = [<span class="number">0</span>] *(<span class="built_in">len</span>(cost) + <span class="number">1</span>)</span><br><span class="line">        dp[<span class="number">1</span>],dp[<span class="number">2</span>] = cost[<span class="number">0</span>],cost[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>([d <span class="keyword">for</span> d <span class="keyword">in</span> dp <span class="keyword">if</span> d != <span class="number">0</span>]),<span class="built_in">len</span>(cost) + <span class="number">1</span>):</span><br><span class="line">            dp[i] = <span class="built_in">min</span>(dp[i-<span class="number">1</span>],dp[i-<span class="number">2</span>]) + cost[i-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">min</span>(dp[<span class="built_in">len</span>(cost)],dp[<span class="built_in">len</span>(cost)-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>83. 删除排序链表中的重复元素</title>
    <url>/83-%E5%88%A0%E9%99%A4%E6%8E%92%E5%BA%8F%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E9%87%8D%E5%A4%8D%E5%85%83%E7%B4%A0/</url>
    <content><![CDATA[<p>给定一个已排序的链表的头 head ， 删除所有重复的元素，使每个元素只出现一次 。返回 已排序的链表 。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/remove-duplicates-from-sorted-list">https://leetcode.cn/problems/remove-duplicates-from-sorted-list</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode(object):</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, next=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.next = next</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">deleteDuplicates</span>(<span class="params">self, head</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type head: ListNode</span></span><br><span class="line"><span class="string">        :rtype: ListNode</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">#未出现的节点存入res，result用于记录头节点</span></span><br><span class="line">        res = ListNode()</span><br><span class="line">        result = res</span><br><span class="line">        <span class="comment">#遍历链表中所有元素，并存入集合，如果集合长度未变则出现过，反之则未出现过</span></span><br><span class="line">        item = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">while</span> head:</span><br><span class="line">            lens = <span class="built_in">len</span>(item)</span><br><span class="line">            item.add(head.val)</span><br><span class="line">            new_lens = <span class="built_in">len</span>(item)</span><br><span class="line">            <span class="keyword">if</span> lens != new_lens:</span><br><span class="line">                res.<span class="built_in">next</span> = ListNode(head.val)</span><br><span class="line">                res = res.<span class="built_in">next</span></span><br><span class="line">            head = head.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> result.<span class="built_in">next</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>96. 不同的二叉搜索树</title>
    <url>/96-%E4%B8%8D%E5%90%8C%E7%9A%84%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/</url>
    <content><![CDATA[<p>给你一个整数 n ，求恰由 n 个节点组成且节点值从 1 到 n 互不相同的 二叉搜索树 有多少种？返回满足题意的二叉搜索树的种数。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/unique-binary-search-trees">https://leetcode.cn/problems/unique-binary-search-trees</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">numTrees</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="comment">#dp是什么？</span></span><br><span class="line">        <span class="comment">#dp为 节点数为n时有多少种方案</span></span><br><span class="line">        dp = [<span class="number">0</span>] * (n+<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#dp如何初始化？</span></span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        dp[<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">        <span class="comment"># dp[2] = 2</span></span><br><span class="line">        <span class="comment">#如何推导？</span></span><br><span class="line">        <span class="comment">#[1]</span></span><br><span class="line">        <span class="comment">#       左子树为0，右子树为0</span></span><br><span class="line">        <span class="comment">#[1,2]</span></span><br><span class="line">        <span class="comment">#       1为头节点时， 左子树节点数为0，右子树节点数为1</span></span><br><span class="line">        <span class="comment">#       2为头节点时，左子树节点数为1，右子树节点数为0</span></span><br><span class="line">        <span class="comment">#[1,2,3]</span></span><br><span class="line">        <span class="comment">#        1为头节点时，左子树节点数为0，右子树节点数为2</span></span><br><span class="line">        <span class="comment">#        2为头节点时，左子树节点数为1，右子树节点数为1</span></span><br><span class="line">        <span class="comment">#        3为头节点时，左子树节点数为2，右子树节点数为0</span></span><br><span class="line">        <span class="comment">#[1,2,3,4]</span></span><br><span class="line">        <span class="comment">#        1为头节点时，左子树节点数为0，右子树节点数为3</span></span><br><span class="line">        <span class="comment">#        2为头节点时，左子树节点数为1，右子树节点数为2</span></span><br><span class="line">        <span class="comment">#        3为头节点时，左子树节点数为2，右子树节点数为1</span></span><br><span class="line">        <span class="comment">#        4为头节点时，左子树节点数为3，右子树节点数为0</span></span><br><span class="line">        <span class="comment">#dp的推导顺序？</span></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>,n+<span class="number">1</span>):</span><br><span class="line">            nums = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,index+<span class="number">1</span>):</span><br><span class="line">                left = i - <span class="number">1</span></span><br><span class="line">                right = index - i</span><br><span class="line">                nums += <span class="built_in">max</span>(dp[left],<span class="number">1</span>) * <span class="built_in">max</span>(dp[right],<span class="number">1</span>) </span><br><span class="line">            dp[index] = nums </span><br><span class="line"></span><br><span class="line">        <span class="comment">#举例推导dp</span></span><br><span class="line">        <span class="keyword">return</span> dp[n]</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>967. 连续差相同的数字</title>
    <url>/967-%E8%BF%9E%E7%BB%AD%E5%B7%AE%E7%9B%B8%E5%90%8C%E7%9A%84%E6%95%B0%E5%AD%97/</url>
    <content><![CDATA[<p>返回所有长度为 n 且满足其每两个连续位上的数字之间的差的绝对值为 k 的 非负整数 。<br>请注意，除了 数字 0 本身之外，答案中的每个数字都 不能 有前导零。例如，01 有一个前导零，所以是无效的；但 0 是有效的。</p>
<p>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/numbers-with-same-consecutive-differences">https://leetcode.cn/problems/numbers-with-same-consecutive-differences</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<p>时常有这种想法，就是为什么在学编程的时候要学“循环”这种东西，超时了。。。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">numsSameConsecDiff</span>(<span class="params">self, n, k</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type n: int</span></span><br><span class="line"><span class="string">        :type k: int</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        mini_num,max_num = <span class="number">10</span>**(n-<span class="number">1</span>),<span class="number">10</span>**n</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">seperate</span>(<span class="params">num,k</span>):</span><br><span class="line">            num = [item <span class="keyword">for</span> item <span class="keyword">in</span> <span class="built_in">str</span>(num)]</span><br><span class="line">            flag = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(num)):</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">abs</span>(<span class="built_in">int</span>(num[i]) - <span class="built_in">int</span>(num[i-<span class="number">1</span>])) != k:</span><br><span class="line">                    flag = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> flag:</span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">int</span>(<span class="string">&quot;&quot;</span>.join(num))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        result = <span class="built_in">list</span>()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(mini_num,max_num):</span><br><span class="line">            res = seperate(i,k)</span><br><span class="line">            <span class="keyword">if</span> res:</span><br><span class="line">                result.append(res)</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<p>然后看到了这个题解:</p>
<img src="/967-%E8%BF%9E%E7%BB%AD%E5%B7%AE%E7%9B%B8%E5%90%8C%E7%9A%84%E6%95%B0%E5%AD%97/%E9%A2%98%E8%A7%A3.png" class="" title="题解">
<p>找到了类似思路的代码，但是没看懂</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">numsSameConsecDiff</span>(<span class="params">self, N, K</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type N: int</span></span><br><span class="line"><span class="string">        :type K: int</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> N == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">10</span>))</span><br><span class="line">        </span><br><span class="line">        res = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10</span>))</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N):</span><br><span class="line">            tmp_res = <span class="built_in">list</span>()</span><br><span class="line">            <span class="keyword">for</span> num <span class="keyword">in</span> res:</span><br><span class="line">                num_s = num % <span class="number">10</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> &#123;num_s - K, num_s + K&#125;:</span><br><span class="line">                    <span class="keyword">if</span> <span class="number">0</span> &lt;= i &lt;= <span class="number">9</span>:</span><br><span class="line">                        tmp_res.append(num * <span class="number">10</span> + i)</span><br><span class="line">            res = tmp_res</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
      <tags>
        <tag>Leetcode贷</tag>
      </tags>
  </entry>
  <entry>
    <title>997. 找到小镇的法官</title>
    <url>/997-%E6%89%BE%E5%88%B0%E5%B0%8F%E9%95%87%E7%9A%84%E6%B3%95%E5%AE%98/</url>
    <content><![CDATA[<p>小镇里有 n 个人，按从 1 到 n 的顺序编号。传言称，这些人中有一个暗地里是小镇法官。<br>如果小镇法官真的存在，那么：<br>小镇法官不会信任任何人。<br>每个人（除了小镇法官）都信任这位小镇法官。<br>只有一个人同时满足属性 1 和属性 2 。<br>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/find-the-town-judge">https://leetcode.cn/problems/find-the-town-judge</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<p>这个代码没有通过测试，因为碰到两个测试用例我觉得与题意相悖。题目输入包括n和trust，其数据格式如代码块中注释所示。有一个测试用例n&#x3D;1，trust&#x3D;[]。按照题干来说，法官要有人信他他才是法官，只有一个人时我个人理解他并不能算是法官。<br>第二个案例就是n&#x3D;4，trust&#x3D;[[1,3],[1,4],[2,3]]，答案为3。但4并未相信3，为什么3能为法官？这个案例哪怕脱离题干中“法官”的情景应该也过不去吧?</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findJudge</span>(<span class="params">self, n, trust</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type n: int</span></span><br><span class="line"><span class="string">        :type trust: List[List[int]]</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        trust_dict = &#123;i:[] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,n+<span class="number">1</span>)&#125;</span><br><span class="line">        trust_valid = <span class="built_in">list</span>()</span><br><span class="line">        <span class="keyword">for</span> lst <span class="keyword">in</span> trust:</span><br><span class="line">            person_id,trusted_id = lst[<span class="number">0</span>],lst[<span class="number">1</span>]</span><br><span class="line">            trust_dict[person_id].append(trusted_id)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> trust_dict.values():</span><br><span class="line">            trust_valid.extend(i)</span><br><span class="line">        judger_id_maybe = <span class="built_in">min</span>(trust_dict,key = trust_dict.get)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(trust_dict[judger_id_maybe]) == <span class="number">0</span> <span class="keyword">and</span> <span class="built_in">len</span>(trust_valid):</span><br><span class="line">            <span class="keyword">for</span> <span class="built_in">id</span>,trust <span class="keyword">in</span> trust_dict.items():</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">id</span> == judger_id_maybe:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> judger_id_maybe <span class="keyword">not</span> <span class="keyword">in</span> trust:</span><br><span class="line">                        <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">return</span> judger_id_maybe</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>官方提供的代码如下，可是能过，呵呵。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findJudge</span>(<span class="params">self, n, trust</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type n: int</span></span><br><span class="line"><span class="string">        :type trust: List[List[int]]</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        n = <span class="number">4</span></span><br><span class="line">        trust = [[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">4</span>],[<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">3</span>]]</span><br><span class="line">        inDegrees = Counter(y <span class="keyword">for</span> _, y <span class="keyword">in</span> trust)</span><br><span class="line">        outDegrees = Counter(x <span class="keyword">for</span> x, _ <span class="keyword">in</span> trust)</span><br><span class="line">        <span class="built_in">print</span>(inDegrees)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;==============&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(outDegrees)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">next</span>((i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>) <span class="keyword">if</span> inDegrees[i] == n - <span class="number">1</span> <span class="keyword">and</span> outDegrees[i] == <span class="number">0</span>), -<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
  </entry>
  <entry>
    <title>A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild</title>
    <url>/A%20Lip%20Sync%20Expert%20Is%20All%20You%20Need%20for%20Speech%20to%20Lip%20Generation%20In%20The%20Wild/</url>
    <content><![CDATA[<p>随着视听内容消费的指数级增长，快速的视频内容创作已经成为一种典型的需求。与此同时，将这些视频翻译成不同的语言也是一个关键的挑战。</p>
<p>有两种方式可以实现视频内容的翻译：</p>
<p>（1） 使用某个人几个小时的说话素材，根据演讲内容直接生成图像。</p>
<p>（2） 精准改变口型</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>造成口型不同步的关键原因在于L1 重建的损失函数和LipGAN中的鉴别器的损失函数不足以惩罚不准确的口型生成。决定使用与训练的专家对口型鉴别器来准确检测实时视频中的同步，SyncNet就是被用来纠正创建大型口型同步数据集错误的模型。</p>
<h2 id="SyncNet概述"><a href="#SyncNet概述" class="headerlink" title="SyncNet概述"></a>SyncNet概述</h2><p>SyncNet中输入的是时间窗口$V$中连续的一系列脸部下方图像$T_v$，和语音片段S，其中为一系列的$T_a \times D$的音频。</p>
<ul>
<li>通过随机采样音频窗口来区分音频和视频之间的同步，该窗口要么与视频对齐，要么来自不同的时间步。</li>
<li>包含一个人脸编码器和一个音频编码器，两者都由2d卷积堆栈组成，从这些编码器生成的嵌入之间计算L2距离，并使用最大边际损失（Hinge Loss）来训练模型，以最小化或者最大化同步或不同步之间的距离。</li>
</ul>
<h3 id="口型纠错器"><a href="#口型纠错器" class="headerlink" title="口型纠错器"></a>口型纠错器</h3><p>基于改进过的SyncNet实现口型纠错，变更如下：</p>
<ol>
<li><p>输入模型的不是灰度图像，而是彩色图像；</p>
</li>
<li><p>使用残差连接加深模型深度；</p>
</li>
<li><p>使用具有二元交叉熵损失的余弦相似度；</p>
</li>
<li><p>计算relu激活的视频和语音嵌入v, s之间的点积，为每个样本产生一个在[0,1]之间的单个值，表示输入音频-视频对同步的概率：</p>
<p>$$<br>P_{sync} &#x3D; \frac{v\cdot s}{max(|v|_2\cdot|s|_2,\epsilon)}<br>$$</p>
<p>使用Adam优化器，初始学习率1e-3，$T_v&#x3D;5帧$，在LRS2数据集上训练（大约29小时），batch为64。</p>
</li>
</ol>
<h2 id="嘴唇同步纠错专家指导同步口型生成"><a href="#嘴唇同步纠错专家指导同步口型生成" class="headerlink" title="嘴唇同步纠错专家指导同步口型生成"></a>嘴唇同步纠错专家指导同步口型生成</h2><h3 id="生成器架构"><a href="#生成器架构" class="headerlink" title="生成器架构"></a>生成器架构</h3><p>结构和LipGAN类似，包括三个模块：</p>
<ol>
<li>Identity Encoder : 编码一个随机参考帧$R$，沿着通道的axis拼接一个pose-prior $P$（目标脸，下半部分被遮蔽）</li>
<li>Speech Encoder ：2维卷积的堆叠，用于编码语音段S，将其与面部表示连接</li>
<li>Face Decoder ：同样是卷积层的堆叠，用于上采样的转置卷积</li>
</ol>
<p>生成器的训练目标是最小化生成的frames$L_g$和ground-truth$L_G$之间的L1重建损失：</p>
<p>$$<br>L_{recon} &#x3D; \frac{1}{N}\sum^N_{i&#x3D;1}|L_g-L_G|_1<br>$$</p>
<p>训练阶段，口型纠错器一次处理$T_v&#x3D;5$个连续帧，所以生成器$G$来生成所有的$T_v&#x3D;5$帧，对参考帧的随机连续窗口进行采样，确保帧数与训练时保持一致。</p>
<p>由于生成器独立处理每一帧，我们在提供参考帧的同时，沿着批次唯独叠加时间步，得到$(N\cdot T_v,H,W,3)$形状的数据，N、H、W代表Batch size、高和宽。在将生成的帧馈送给专家鉴别器的同时，时间步长沿着信道维度进行串联，这也是鉴别器训练过程中所做的。专家鉴别器的最终输入形状为$(N,\frac{H}{2},W,3\cdot T_v)$，仅使用生成的人脸的下半部分进行鉴别。生成器还经过训练，以最小化来自专家鉴别器的“专家同步损失”$E_{sync}$。</p>
<p>$$<br>E_{sync} &#x3D; \frac{1}{N}\sum^N_{i&#x3D;1}-log(P^i_{sync})<br>$$</p>
<p>其中，$P^i_{sync}$由第一个公式计算得到，在生成器的训练过程中，专家鉴别器的权重保持不变。这种纯粹基于从真实视频中学习到的对口型概念的强烈辨别，迫使生成器也实现真实的对口型，以尽量减少对口型损失。</p>
<h3 id="生成逼真的人脸"><a href="#生成逼真的人脸" class="headerlink" title="生成逼真的人脸"></a>生成逼真的人脸</h3><p>在我们的实验中，我们观察到使用一个强大的唇同步鉴别器迫使生成器产生准确的唇形。然而，它有时会导致变形区域稍微模糊或包含轻微的伪影。为了减轻这种轻微的质量损失，我们在GAN设置中与生成器一起训练了一个简单的视觉质量鉴别器。因此，我们有两个鉴别器，一个用于同步精度，另一个用于更好的视觉质量。由于3.2中解释的原因，在GAN设置中没有训练口型同步鉴别器。另一方面，由于视觉质量鉴别器不执行对口型的任何检查，只惩罚不真实的面部生成，因此它是在生成的面部上进行训练的。</p>
<p>鉴别器D由一堆卷积块组成。每个块由一个卷积层和一个Leaky ReLU激活组成[20]。训练鉴别器使目标函数$L_{disc}$最大化：</p>
<p>$$<br>L_{gen} &#x3D; E_{x~L_g}[log(1-D(x)]<br>$$</p>
<p>$$<br>L_{disc} &#x3D; E_{x~L_G}[log(D(x))] + L_{gen}<br>$$</p>
<p>对应生成器$G$的图像，$L_G$对应于真实图像。生成器的最终优化目标为：</p>
<p>$$<br>L_{total} &#x3D; (1-s_w-s_g)\cdot L_{recon}+ s_w\cdot E_{sync} + s_g \cdot L_{gen}<br>$$</p>
<p>其中$s_w$是同步惩罚权重，$s_g$是对抗损失，在我们所有的实验中，依照经验设置为0.03和0.07。因此，我们的完整网络使用两个不相交的鉴别器进行了优化，以获得更高的同步精度和质量。</p>
<p>我们只在LRS2训练集上训练模型，批量大小为80。使用Adam优化器，初始学习率为1e−4，β1 &#x3D; 0.5， β2 &#x3D; 0.999，用于生成器和视觉质量鉴别器$D$。注意，口型同步鉴别器没有进一步微调，因此其权重被冻结。我们通过解释它在真实视频推理过程中的工作原理来总结我们提出的架构的描述。与LipGAN类似，该模型逐帧生成会说话的人脸视频。每个时间步的视觉输入是当前人脸裁剪(来自源帧)，与相同的当前人脸裁剪(下半部分被遮罩用作姿态先验)连接在一起。因此，在推理过程中，模型不需要改变姿态，大大减少了伪影。将相应的音频片段作为输入输入到语音子网络中，该网络生成输入的人脸裁剪，但嘴巴区域发生了变形。<br>$$</p>
]]></content>
      <categories>
        <category>论文浅读</category>
      </categories>
      <tags>
        <tag>数字人</tag>
      </tags>
  </entry>
  <entry>
    <title>A Method for Parsing and Vectorization of Semi-structured Data used in Retrieval Augmented Generation</title>
    <url>/A%20Method%20for%20Parsing%20and%20Vectorization%20of%20Semi-structured%20Data%20used%20in%20Retrieval%20Augmented%20Generation/</url>
    <content><![CDATA[<p><a href="https://arxiv.org/abs/2405.03989">文章</a>读起来感觉比较混乱，感觉像是还没定稿，<a href="https://github.com/linancn/TianGong-AI-Unstructure">仓库地址</a>。<br>大概意思是，将html、pdf、xml、xlsx等类型数据都转换成docx，docx中包括标题、文本元素、和表格。<br>使用detectorn2将docx中的内容分为：标题、文本、图像、表格、页眉和页脚等多个元素，再将这些元素细化为标题、文本元素、和表格。<br>表格怎么怎么存储下来；<br>图像用gpt4描述成文字；<br>然后就开始切块开始比较了。<br>但是怎么把HTML\PDF转换成docx的，不知道。但是在原文中提到了一个<a href="https://github.com/Unstructured-IO/unstructured?tab=readme-ov-file">处理数据的工具</a>，可能就是用这个工具把所有类型数据处理成docx的？</p>
]]></content>
      <categories>
        <category>论文浅读</category>
      </categories>
  </entry>
  <entry>
    <title>论文浅读-A Simulacrum of Hospital with Evolvable Medical Agents</title>
    <url>/A%20Simulacrum%20of%20Hospital%20with%20Evolvable%20Medical%20Agents/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>本文介绍了一种模拟医院诊疗全过程的Agent医院仿真系统。所有的病人、护士和医生都是由大型语言模型(llm)驱动的自主代理。我们的中心目标是使医生代理能够学习如何在模拟中治疗疾病。为此，我们提出了一种称为MedAgent-Zero的方法。由于simulacrum可以基于知识库和llm来模拟疾病的发生和发展，医生代理可以不断地从成功和不成功的案例中积累经验。仿真实验表明，医生代理在各种任务上的处理性能不断提高。更有趣的是，医生代理人在代理医院获得的知识适用于现实世界的医疗保险基准。在治疗了大约1万名患者(现实世界的医生可能需要两年多的时间)之后，进化的医生代理在MedQA数据集的一个子集上达到了93.06%的最先进的准确率，该数据集涵盖了主要的呼吸系统疾病。这项工作为推进llm驱动的代理技术在医疗场景中的应用铺平了道路。</p>
<p>代理医院中，目标是训练熟练的“医生”来处理医疗任务，如诊断和治疗建议，传统的研究通常将医学知识整合到llm &#x2F;agent中，通过预训练、监督微调或检索增强生成策略来构建强大的医学模型。然而，我们提出了一种新的策略，通过在模拟环境中模拟医患互动来训练医生代理。由于没有使用手动标记的数据，我们将提出的策略命名为MedAgent-Zero。在agent Hospital中，医生agent与各种患者agent相互作用，从成功案例中积累记录，从失败案例中汲取经验，成为更加优秀的agent。由于医生代理培训的低成本和高效率，我们可以让代理在短短几天内轻松处理数万个病例，实现这一目标需要现实世界的医生几年的时间才能做到。</p>
]]></content>
      <categories>
        <category>论文浅读</category>
      </categories>
      <tags>
        <tag>大语言模型</tag>
      </tags>
  </entry>
  <entry>
    <title>CNN在NLP中的使用</title>
    <url>/CNN%E5%9C%A8NLP%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="滤波器与卷积核"><a href="#滤波器与卷积核" class="headerlink" title="滤波器与卷积核"></a>滤波器与卷积核</h2><p>一般在检索卷积神经网络相关资料时都会看到类似的样例图。</p>
<img src="/CNN%E5%9C%A8NLP%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8/%E5%9B%BE%E4%BE%8B.png" class="" title="图例">
<p>这里需要强调的是，滤波器（Filter）和卷积核（Kernel）并非指的是同一样东西，<strong>每个通道的数据都需要对应一个卷积核，若干个卷积核总称为滤波器。</strong>在图示中，输入图像数据包含3通道，所以对应的卷积核就要包含3个，这3个卷积核组合成为一个滤波器。<br><strong>滤波器的个数与输出通道数相对应</strong>，若假设输入数据通道数为$\bf{C_{in}}$，out_channels为$\bf{N}$，，每个通道需要对应一个卷积核，那么实际上就有$3\bf{N}$个卷积核。以上图为例，模型输入为三通道，对应三个卷积核对每个通道中的数据进行卷积操作，将最终得到的矩阵相加得到最终结果。</p>
<h2 id="Pytorch中的卷积层定义"><a href="#Pytorch中的卷积层定义" class="headerlink" title="Pytorch中的卷积层定义"></a>Pytorch中的卷积层定义</h2><h3 id="引包"><a href="#引包" class="headerlink" title="引包"></a>引包</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<h3 id="Conv1d"><a href="#Conv1d" class="headerlink" title="Conv1d"></a>Conv1d</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.nn.Conv1d(</span><br><span class="line">    in_channels</span><br><span class="line">    , out_channels</span><br><span class="line">    , kernel_size</span><br><span class="line">    , stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">True</span>, padding_mode=<span class="string">&#x27;zeros&#x27;</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="Conv2d"><a href="#Conv2d" class="headerlink" title="Conv2d"></a>Conv2d</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.nn.Conv2d(</span><br><span class="line">    in_channels</span><br><span class="line">    , out_channels</span><br><span class="line">    , kernel_size</span><br><span class="line">    , stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">True</span>, padding_mode=<span class="string">&#x27;zeros&#x27;</span>, device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Con1d和Conv2d的函数定义如上所示，in_channels代表着输入数据的通道数，以图像数据为例，如果输入的是3通道，那么in_channels就要为3，输入数据需要与卷积层通道数对齐。out_channels代表滤波器的个数，kernel_size为卷积核的尺寸。kernel_size、dilation、padding可以设定为整型也可以设定为元组。kernel_size输入为整型$\bf{X}$时，卷积核尺寸为$\bf{X}\times\bf{X}$；当输入为（x，y）时，卷积核尺寸即为$\bf{X}\times\bf{Y}$。padding和dilation输入为整型时，意为对行、对列的padding或dilation均相同；当输入为元组时，代表当前卷积层进行padding和dilation时对行、对列设定不同数值。</p>
<h3 id="卷积层的输入"><a href="#卷积层的输入" class="headerlink" title="卷积层的输入"></a>卷积层的输入</h3><p>Conv1D与Conv2D的输入略有不同，输入Conv1d的往往为三维数据，即$(\bf{N},\bf{C_{in}},\bf{L})$，输入Conv2d的数据往往为四维，即$(\bf{N},\bf{C_{in}},\bf{H},\bf{W})$。以$(\bf{N},\bf{C_{in}},\bf{L})$为例，$\bf{N}$代表一个batch中有$\bf{N}$条数据，每条数据的长度为$\bf{L}$，数据的通道数为$\bf{C_{in}}$；$(\bf{N},\bf{C_{in}},\bf{H},\bf{W})$中$\bf{N}$和$\bf{C_{in}}$的含义不变，$\bf{H}$和$\bf{W}$分别代表数据的高度和宽度。在自然语言处理相关任务中，输入数据的维度多为$(\bf{N},\bf{L},Embedding_dim)$，并没有“通道”的概念。</p>
<h2 id="CNN在NLP中的应用"><a href="#CNN在NLP中的应用" class="headerlink" title="CNN在NLP中的应用"></a>CNN在NLP中的应用</h2><p>这里我们先假设一组embedding之后的数据，batch大小为3，批中语句长度为12，每个字的embedding_dim为7，共3个滤波器，最终得到的数据如代码块所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">batch = <span class="number">2</span></span><br><span class="line">seq_len = <span class="number">12</span></span><br><span class="line">bert_hid_size = <span class="number">7</span></span><br><span class="line">output_channel = <span class="number">3</span></span><br><span class="line">sequence = torch.tensor(np.random.rand(batch,seq_len,bert_hid_size),dtype=torch.<span class="built_in">float</span>)</span><br></pre></td></tr></table></figure>
<p>前面介绍了再Pytorch中Conv2d层的输入是四个维度，所以需要将embedding之后的数据升1维，变为（batch，1，len，embedding_dim）。这时可以有两种做法，一种是直接将“1”看作数据的通道数，另一种是将原始数据转变维度，使用seq_len或embedding_dim作为通道数。直接将“1”看作通道数的实现如以下代码块所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line">conv2d_layer = torch.nn.Conv2d(</span><br><span class="line">    in_channels = <span class="number">1</span></span><br><span class="line">    ,out_channels = <span class="number">3</span></span><br><span class="line">    ,kernel_size = <span class="number">3</span></span><br><span class="line">)</span><br><span class="line">inputs = deepcopy(sequence).unsqueeze(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;inputs.shape:<span class="subst">&#123;inputs.shape&#125;</span>&quot;</span>)</span><br><span class="line">outputs = conv2d_layer(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;outputs.shape:<span class="subst">&#123;outputs.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># inputs.shape:torch.Size([2, 1, 12, 7])</span></span><br><span class="line"><span class="comment"># outputs.shape:torch.Size([2, 3, 10, 5])</span></span><br></pre></td></tr></table></figure>
<p>除此之外，还可以将embedding_dim作为通道数进行卷积操作，实现如以下代码块所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line">conv2d_layer = torch.nn.Conv2d(</span><br><span class="line">    in_channels = bert_hid_size</span><br><span class="line">    ,out_channels = <span class="number">3</span></span><br><span class="line">    ,kernel_size = <span class="number">3</span></span><br><span class="line"><span class="comment">#     ,padding = 1</span></span><br><span class="line">)</span><br><span class="line">inputs = deepcopy(sequence).unsqueeze(<span class="number">1</span>).permute(<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;inputs.shape:<span class="subst">&#123;inputs.shape&#125;</span>&quot;</span>)</span><br><span class="line">outputs = conv2d_layer(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;outputs.shape:<span class="subst">&#123;outputs.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># RuntimeError：Calculated padded input size per channel: (1 x 12). Kernel size: (3 x 3). </span></span><br><span class="line"><span class="comment"># Kernel size can&#x27;t be greater than actual input size</span></span><br></pre></td></tr></table></figure>
<p>这里如果没有进行padding操作会报错“其中提到计算出的每个channel输入大小：(1x 12)，kernel大小：(3 x3)，内核大小不能大于实际输入大小”。这是不是可以说明，将embedding_dim作为通道后，在每个channel中的数据实际上是（batch, 1, 1, seq_len），也就是channel把原来的矩阵“掰碎”了？在原始数据中使用维度为768的向量表示一个token，现在在每个通道中，使用一个维度为1的向量代表一个token，换句话说，卷积核的尺寸设置成3，就已经是3-gram模型了？</p>
<p>在这种情况下，kernel_size设计成（1，seq_len）还是（1，int）？如果设计成（1，3）是对应着3-gram模型吗？<br>这里如果增加了padding，也是可以运行的，结果为torch.Size([3, 3, 1, 12])，这里每个维度代表的含义？第一个3是batch，第二个3是几个滤波器，12代表seq_len，那最终的1可以理解成将每个token的向量转变成了1个数字吗？</p>
<p>对数据进行padding后，上述报错的代码即可执行，实现效果如下代码块所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line">conv2d_layer = torch.nn.Conv2d(</span><br><span class="line">    in_channels = bert_hid_size</span><br><span class="line">    ,out_channels = <span class="number">3</span></span><br><span class="line">    ,kernel_size = <span class="number">3</span></span><br><span class="line">    ,padding = <span class="number">1</span></span><br><span class="line">)</span><br><span class="line">inputs = deepcopy(sequence).unsqueeze(<span class="number">1</span>).permute(<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;inputs.shape:<span class="subst">&#123;inputs.shape&#125;</span>&quot;</span>)</span><br><span class="line">outputs = conv2d_layer(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;outputs.shape:<span class="subst">&#123;outputs.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># inputs.shape:torch.Size([2, 7, 1, 12])</span></span><br><span class="line"><span class="comment"># outputs.shape:torch.Size([2, 3, 1, 12])</span></span><br></pre></td></tr></table></figure>
<p>这里观察卷积后得到的数据维度可以发现，batch、seq_len、以及升维的channel都没有变，原本7维的词向量缩短为3维。<br>也可以将seq_len作为channel，程序依旧可以运行，实现如以下代码块所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line">conv2d_layer = torch.nn.Conv2d(</span><br><span class="line">    in_channels = seq_len</span><br><span class="line">    ,out_channels = <span class="number">3</span></span><br><span class="line">    ,kernel_size = <span class="number">3</span></span><br><span class="line">    ,padding = <span class="number">1</span></span><br><span class="line">)</span><br><span class="line">inputs = deepcopy(sequence).unsqueeze(<span class="number">1</span>).permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;inputs.shape:<span class="subst">&#123;inputs.shape&#125;</span>&quot;</span>)</span><br><span class="line">outputs = conv2d_layer(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;outputs.shape:<span class="subst">&#123;outputs.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># inputs.shape:torch.Size([2, 12, 1, 7])</span></span><br><span class="line"><span class="comment"># outputs.shape:torch.Size([2, 3, 1, 7])</span></span><br></pre></td></tr></table></figure>
<p>这里观察输出结果维度发现，batch、channel、embedding维度没有变，但是原本对应句子长度的数值缩减到3。<br>torch.nn.Conv1d实现起来与Conv2d差不多，下面两个代码块依次以embedding_dim和seq_len作为通道。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line">conv1d_layer = torch.nn.Conv1d(</span><br><span class="line">    in_channels = bert_hid_size</span><br><span class="line">    ,out_channels = <span class="number">3</span></span><br><span class="line">    ,kernel_size = <span class="number">3</span></span><br><span class="line">    ,padding = <span class="number">1</span></span><br><span class="line">)</span><br><span class="line">inputs = deepcopy(sequence).permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;inputs.shape:<span class="subst">&#123;inputs.shape&#125;</span>&quot;</span>)</span><br><span class="line">outputs = conv1d_layer(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;outputs.shape:<span class="subst">&#123;outputs.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># inputs.shape:torch.Size([2, 7, 12])</span></span><br><span class="line"><span class="comment"># outputs.shape:torch.Size([2, 3, 12])</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line">conv1d_layer = torch.nn.Conv1d(</span><br><span class="line">    in_channels = seq_len</span><br><span class="line">    ,out_channels = <span class="number">3</span></span><br><span class="line">    ,kernel_size = <span class="number">3</span></span><br><span class="line">    ,padding = <span class="number">1</span></span><br><span class="line">)</span><br><span class="line">inputs = deepcopy(sequence)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;inputs.shape:<span class="subst">&#123;inputs.shape&#125;</span>&quot;</span>)</span><br><span class="line">outputs = conv1d_layer(inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;outputs.shape:<span class="subst">&#123;outputs.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># inputs.shape:torch.Size([2, 12, 7])</span></span><br><span class="line"><span class="comment"># outputs.shape:torch.Size([2, 3, 7])</span></span><br></pre></td></tr></table></figure>
<p>所以说使用CNN进行NLP相关任务的时候最重要的一个原则，需要沿着embedding_dim那个维度进行卷积，例如输入为（batch，len，dim）时（4维时同理），将dim作为通道数，即（batch，dim，len）每个通道内形状为（1，len）相当于原来的embedding被“降维”成了一个数，此时kernel_size设为（1，3）则是一个3-gram模型。<br>也有许多博客说Conv1d和Conv2d的区别在于卷积的方向，Conv1D的卷积方向是一维的，只能一个方向，Conv2D的卷积方向是二维的，横着卷完竖着卷。但是在处理文本数据时，理论上应该把每个词的词向量完整的包含在卷积核中，对于一个（batch, len, embedding_dim）的矩阵，卷积核尺寸应该类似于（height, embedding_dim），这时卷积的方向不也是只能向下了嘛？<br>上述的方法的确可以使用CNN对文本embedding后的矩阵进行运算，但是存在一个比较严重的问题就是模型<strong>很难做深</strong>。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>ChatDoctor A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge</title>
    <url>/ChatDoctor%20A%20Medical%20Chat%20Model%20Fine-tuned%20on%20LLaMA%20Model%20using%20Medical%20Domain/</url>
    <content><![CDATA[<h1 id="ChatDoctor-A-Medical-Chat-Model-Fine-tuned-on-LLaMA-Model-using-Medical-Domain-Knowledge"><a href="#ChatDoctor-A-Medical-Chat-Model-Fine-tuned-on-LLaMA-Model-using-Medical-Domain-Knowledge" class="headerlink" title="ChatDoctor A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge"></a>ChatDoctor A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>ChatGPT获得巨大成功，可是在医疗领域其回答准确性不足，医疗领域中没有类似能力的llm。因此，采集10万医患对话来微调语言模型，并添加了自主知识检索的功能。提出模型在理解患者需求和提供建议方面的能力有显著提升。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>现有llm训练过程中未包含医疗领域的数据，导致模型回复医学问题时准确性较差。通过实用医患对话数据微调大预言模型提升模型理解患者需求的能力。还设计了一种基于Wiki的医学领域数据库知识大脑，实时访问权威的医学信息，模型可以根据这些信息回答患者问题。实验证明，微调过的对话模型P\R\F1都要优于CHatGPT。采用LLaMA作为基座，实用Alpaca的52K指令跟踪数据训练通用会话模型。本文贡献有三个：</p>
<ol>
<li>设计了一个微调医学llm的框架</li>
<li>收集、开放了对话数据集</li>
<li>提出一种能实时访问wiki，能在线分析的ChatDoctor模型</li>
</ol>
<h2 id="Mehtod"><a href="#Mehtod" class="headerlink" title="Mehtod"></a>Mehtod</h2><h3 id="医患对话数据集"><a href="#医患对话数据集" class="headerlink" title="医患对话数据集"></a>医患对话数据集</h3><p>收集数据 + 人工过滤 + 语法纠错</p>
<h3 id="外部知识大脑"><a href="#外部知识大脑" class="headerlink" title="外部知识大脑"></a>外部知识大脑</h3><p>llm推理过程中很可能给出错误答案，生成的答案往往是不可能控的、随机的，这在医学领域是不可接受的，于是构建一个数据库，其中包括约700种疾病及其相关症状，进一步的医学检测或措施，以及推荐的药物。数据库可以随时更新，无需对模型进行训练，理论上可以根据部门或特定目标建立特定疾病数据库。</p>
<h3 id="基于知识大脑的ChatDoctor"><a href="#基于知识大脑的ChatDoctor" class="headerlink" title="基于知识大脑的ChatDoctor"></a>基于知识大脑的ChatDoctor</h3><p>在构建了外部知识大脑之后，我们需要让ChatDoctor自主检索他需要的知识，这通常可以通过构建适当的提示在一个大的语言模型中实现。为了使这一过程自动化，我们设计了关键词挖掘提示符，用于ChatDoctor提取关键字进行相关知识搜索。</p>
<blockquote>
<p>A question is provided below. Given the question, extract keywords from the text.<br>Focus on extracting the keywords that we can use to best lookup answers to the<br>question.</p>
<hr>
<p>{Question of patient}</p>
<hr>
<p>Provide keywords in the following comma-separated format.<br>Keywords:</p>
</blockquote>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>使用LLaMA作为基座，以Alpaca的训练方式，使用HealthCareMagic-100k的对话对LLaMA模型进行微调，以获得一些基本的对话能力。使用6 * A100 进行3个小时的微调，训练超参数设置为：</p>
<table>
<thead>
<tr>
<th align="left">Batch Size</th>
<th align="right">learning rate</th>
<th align="center">epoch</th>
<th align="center">max length</th>
<th align="center">warm up</th>
<th align="center">weight decay</th>
</tr>
</thead>
<tbody><tr>
<td align="left">192</td>
<td align="right">2e-5</td>
<td align="center">3</td>
<td align="center">512</td>
<td align="center">0.03</td>
<td align="center">0</td>
</tr>
</tbody></table>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>为了测试基于知识大脑的自主ChatDoctor模型的能力，向模型提出了一些最新的医学问题，如Mpox(猴痘)，它于2022年11月28日被世界卫生组织(WHO)新命名。由于这是一个新术语，ChatGPT完全无法回答它，而ChatDoctor可以自主检索Mpox的Wikipedia内容，并给出准确的答案。对于一些一般性的医学问题，如中耳炎，ChatDoctor通过知识检索后提供了非常可靠的答案。Daybue在2023年3月被FDA批准为药物，我们的模型在自主信息检索后也提供了准确的答案。<br>为了定量评估ChatDoctor的表现，使用iCliniq的问题作为输入，然后使用iCliniq对应的真实医生的回答作为ground truth，也将相同的输入给ChatGPT并记录其回答。我们使用BERT分数分别计算ChatDoctor和ChatGPT的Precision, Recall和F1分数。我们发现经过微调的ChatDoctor模型在Precision, Recall和F1方面都优于ChatGPT。</p>
<h2 id="讨论与结论"><a href="#讨论与结论" class="headerlink" title="讨论与结论"></a>讨论与结论</h2><p>ChatDoctor首先使用真实的医患对话数据进行微调，从而让模型更好地理解患者的问题，从而做出更明智的回答，而且还能够自主检索大脑的知识，然后提供答案，进一步提高了模型响应的可信度。在实际应用中，ChatDoctor可以提高医疗诊断的准确性和效率，减少医疗专业人员的工作量，同时增加医疗咨询的机会，特别是对大多数服务不足的医院和第三世界国家的患者。我们相信，我们的聊天医生可以是一个宝贵的援助，改善病人的结果和推进医学研究。<br>但你不能用，因为这个模型在诊断和医嘱中潜在的语言错误可能会造成严重的后果。而大型语言模型往往会对他们不知道的知识产生许多不正确和有害的陈述(幻觉)，这可能会导致滥用。ChatDoctor仅用于学术研究，任何商业用途和临床用途都是严格禁止的。首先，我们没有设计足够的安全措施，现有的模型不能保证医学诊断和建议的完全正确性。其次，我们的模型没有获得医疗保健相关目的[9]许可。第三，ChatDoctor基于LLaMA，拥有非商业许可，所以我们有必要继承这些规则。</p>
]]></content>
      <categories>
        <category>论文浅读</category>
      </categories>
  </entry>
  <entry>
    <title>论文浅读-任务型对话系统中的自然语言生成研究进展综述</title>
    <url>/Clinical%20Note%20Generation%20from%20Doctor-Patient%20Conversations%20using%20Large%20Language%20Models/</url>
    <content><![CDATA[<h1 id="WangLab-at-MEDIQA-Chat-2023-Clinical-Note-Generation-from-Doctor-Patient-Conversations-using-Large-Language-Models"><a href="#WangLab-at-MEDIQA-Chat-2023-Clinical-Note-Generation-from-Doctor-Patient-Conversations-using-Large-Language-Models" class="headerlink" title="WangLab at MEDIQA-Chat 2023: Clinical Note Generation from Doctor-Patient Conversations using Large Language Models"></a>WangLab at MEDIQA-Chat 2023: Clinical Note Generation from Doctor-Patient Conversations using Large Language Models</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>本文描述了我们对Mediqa-Chat 2023共享任务的提交，该任务用于从医患对话中自动生成临床记录。 我们报告了两种方法的结果：第一种方法在共享任务数据上微调预先训练的语言模型(PLM)；第二种方法在大语言模型(LLM)中使用少量上下文学习(ICL)。 两者都实现了通过自动度量（例如Rouge,BertScore）来衡量的高性能，并分别在共享任务的所有提交中排名第二和第一。 专家的人类审查表明，通过基于ICL的方法和GPT-4生成的笔记与人类书写的笔记一样经常被首选，这使得它成为从医患对话中自动生成笔记的一条有希望的途径。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>日益增长的临床文档负担已经成为医疗保健中的一个关键问题，增加了临床医生的工作不满和倦怠率，并对患者体验产生了负面影响。<br>通过医患对话生成电话记录，MEDIQA-Chat dialgue2note共享任务被提出。我们探索了两种方法； 第一种，<strong>微调一个预先训练的语言模型(PLM，§3.1)<strong>；第二种，</strong>使用少量的上下文学习(ICL，§3.2)<strong>。 这两个都实现了很高的性能，由自动</strong>自然语言生成度量(§4)来衡量</strong>，并分别在共享任务的所有提交中排名第二和第一。 在一项由三位专家医生进行的人类评估中，通过基于ICL的方法和GPT-4生成的笔记被首选，其频率与人类书写的笔记一样(§4.3)。</p>
<h2 id="2-Shared-Task-and-Dataset"><a href="#2-Shared-Task-and-Dataset" class="headerlink" title="2 Shared Task and Dataset"></a>2 Shared Task and Dataset</h2><p>MEDIQA-Chat 2023中包含<strong>对话总结</strong>和<strong>根据医生和病人之间的对话生成病历</strong></p>
<h3 id="2-1-Task-definition"><a href="#2-1-Task-definition" class="headerlink" title="2.1 Task definition"></a>2.1 Task definition</h3><p>病历中包含一个或多个部分，比如如“主诉”和“家族史”，评估过程中，各部分被分为四个类别之一：<strong>Subjective</strong>、 <strong>Objective Exam</strong>、 <strong>Objective Results</strong>和<strong>Assessment and Plan</strong>，下图为医患对话和病历的的示例。</p>
<img src="/Clinical%20Note%20Generation%20from%20Doctor-Patient%20Conversations%20using%20Large%20Language%20Models/figure2.png" class="" title="医患对话示例">

<h3 id="2-2-Dataset"><a href="#2-2-Dataset" class="headerlink" title="2.2 Dataset"></a>2.2 Dataset</h3><p>该数据集包括67个训练和20个验证示例，以医生和病人相遇的转录对话和由此产生的临床医生写的笔记为特色。 每个示例都标有“数据集源”，表明用于生成笔记的对话转录系统</p>
<h1 id="3-Approach"><a href="#3-Approach" class="headerlink" title="3 Approach"></a>3 Approach</h1><p>我们对共享任务采用两种高性能方法。首先，我们在所提供的训练集(§3.1)上微调一个预先训练的语言模型(PLM)。 在第二个方面，我们将上下文学习(ICL)与一个大型语言模型(LLM，§3.2)结合起来。</p>
<h2 id="3-1-Fine-tuning-pre-trained-language-models"><a href="#3-1-Fine-tuning-pre-trained-language-models" class="headerlink" title="3.1 Fine-tuning pre-trained language models"></a>3.1 Fine-tuning pre-trained language models</h2><p>作为第一种方法，我们在训练集上微调PLM遵循规范的、序列到序列的训练过程（图1a；有关详细信息，请参见附录C）。 给定输入对话的长度（图3），我们选择使用LongFormerEncoder-Decoder(LED，Beltagy et al.2020)，其最大输入大小为16,384个令牌。 我们从PubMed总结数据集（Cohan et al.，2018）上调优的LEDLARGE检查点开始微调，该检查点在初步实验中表现最好。3该模型是在一个NVIDIA A100-40GB的GPU上使用HuggingFace Transformers（Wolf et al.，2020）微调的。 对验证集中的超参数进行了轻微调优。</p>
<h2 id="3-2-In-context-learning-with-LLMs"><a href="#3-2-In-context-learning-with-LLMs" class="headerlink" title="3.2  In-context learning with LLMs"></a>3.2  In-context learning with LLMs</h2><p>作为第二种方法，我们使用ICL尝试子任务B。我们选择GPT-4(OpenAI，2023)作为LLM，并设计了一个简单的提示符，其中包括自然语言指令和上下文示例（图4）。 我们将提示符大小限制在6192个令牌–允许2000个输出令牌，因为模型的最大令牌大小为8192–并使用了尽可能多的上下文示例，最多为3个。 我们将温度参数设置为0.2，并将OpenAI API的所有其他超参数设置为默认值。<br><strong>自然语言指令</strong><br>Prompt：Write a clinical note reflecting this doctor-patient dialogue. Use the example notes below to decide the structure of the clinical note. Do not make up information.<br><strong>In-context example selection</strong><br>每个上下文中的示例都是来自训练集的注释。 为了选择注释，我们首先嵌入每个训练示例的对话和输入对话。 然后根据与输入对话的余弦相似度对训练对话进行排序； 结果的top-k训练示例的注释被选择为上下文中的示例（参见图1，b）。 对话是使用支持自然语言教学的文本编码器讲师（Su et al.，2022a）嵌入的。6最后，我们限制上下文中的示例与输入对话属于相同的“数据集源”（参见§2.2），假设这可能会提高性能。</p>
<h2 id="3-2-Evaluation"><a href="#3-2-Evaluation" class="headerlink" title="3.2 Evaluation"></a>3.2 Evaluation</h2><p>模型使用验证集中的官方评估脚本8进行评估(因为没有提供测试注释)。通过ROUGE (Lin, 2004)、BERTScore (Zhang et al.， 2020)和BLEURT (Sellam et al.， 2020)对生成的笔记与提供的地面真实笔记进行评估。我们报告的性能作为算术平均值的ROUGE-1 F1, BERTScore F1和BLEURT-20</p>
]]></content>
      <categories>
        <category>论文浅读</category>
      </categories>
  </entry>
  <entry>
    <title>Coding随手记</title>
    <url>/Coding%E9%9A%8F%E6%89%8B%E8%AE%B0/</url>
    <content><![CDATA[<ol>
<li>序列标注任务，或者说是基于bert的任务，尽量手动convert_tokens_to_id，如果使用encode_plus可能在处理数据时产生错误。比如序列标注任务时，在语句中添加的[CLS]和[SEP]后，标签可能会和原句不对齐</li>
<li>tokenizer.tokenize(some_word)返回的结果是列表，可以使用list.extend将返回结果写入tokens : list   中</li>
</ol>
<hr>
<ol>
<li>变长padding，感觉可以在get_examples阶段进行。读取所有数据后先根据长度排序后再返回。</li>
<li>lens&#x3D;[所有数据长度]，len_index &#x3D; np.argsort(lens)，然后data_list &#x3D; data_list[index]即可，让-1*lens见鬼去吧。</li>
<li>在collate_fn之前要先处理好数据长度问题，如果数据长度超过max_length这里也没办法处理。因为到collate_fn的时候已经加好了[CLS]和[SEP]，不能使用截断操作了。</li>
<li>TorchCRF的crf的是有mask参数的，mask&#x3D;attention_mask.byte()就有作用了。</li>
</ol>
<hr>
<ol>
<li>list.extend(tokenizer.unk_token) 会添加’[‘, ‘U’, ‘N’, ‘K’, ‘]’，但是extend([tokenizer.unk_token])则添加的是”[UNK]”</li>
</ol>
<hr>
<ol>
<li>torch.nn.utils.RNN.pack_padded_sequence<br>原文链接：<a href="https://www.cxyzjd.com/article/kejizuiqianfang/100835528">https://www.cxyzjd.com/article/kejizuiqianfang/100835528</a><br>假设存在两条数据：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure>
输入RNN的其实是[1，2]，[2，3]，[3，4]，[4，5]，[5，6]，[6，7]，[7，0]，最终输入模型的0并非真实数据，若参与运算则会影响模型效果且浪费算力，于是使用torch.utils.nn.RNN.pack_padded_sequence方法（后称“pack”）去除掉输入进模型的padding标记，上述两条数据pack之后得到的结果为：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">PackedSequence(data=tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">7</span>]), batch_sizes=tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>]), sorted_indices=<span class="literal">None</span>, unsorted_indices=<span class="literal">None</span>)</span><br><span class="line">PackedSequence(data=tensor([<span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">7</span>]), batch_sizes=tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>]), sorted_indices=<span class="literal">None</span>, unsorted_indices=<span class="literal">None</span>)</span><br><span class="line">PackedSequence(data=tensor([<span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">7</span>]), batch_sizes=tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>]), sorted_indices=<span class="literal">None</span>, unsorted_indices=<span class="literal">None</span>)</span><br><span class="line">PackedSequence(data=tensor([<span class="number">7</span>]), batch_sizes=tensor([<span class="number">1</span>]), sorted_indices=<span class="literal">None</span>, unsorted_indices=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
暂且只看第一条数据，pack后的数据为：[1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7]，实际上就是去除掉两条数据中padding标记的结果，RNN通过batch_sizes中的数字决定每次输入模型“几个数”，这里的batch_sizes为tensor([2, 2, 2, 2, 2, 2, 1])，意为连续取6次两位数，在第7次时只取一个数。</li>
</ol>
<hr>
<ol>
<li>Python中的矩阵乘法<br>Python中，星乘（*）指两个矩阵对应位置相乘；点乘（.dot）指数学上的矩阵乘法。<br>在进行星乘时，Python会对“低维度的矩阵”进行广播操作，使之能与另一矩阵维度匹配，而后进行对应位置相乘。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#假设现有一文本序列</span></span><br><span class="line">seq = torch.tensor(np.random.randint(<span class="number">0</span>,<span class="number">2</span>,(batch,seq_len)))</span><br><span class="line"><span class="comment">#以及该序列所对应的词向量矩阵</span></span><br><span class="line">vector_mat = torch.tensor(np.random.randn(batch,seq_len,dim))</span><br><span class="line"><span class="comment">#现要对padding的部分做mask</span></span><br><span class="line">mask = seq.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">mask = torch.tensor(mask&gt;torch.tensor(np.array([<span class="number">0</span>])),dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"><span class="built_in">print</span>(seq)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;**********&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(mask)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;**********&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(vector_mat * mask)</span><br></pre></td></tr></table></figure>
<img src="/Coding%E9%9A%8F%E6%89%8B%E8%AE%B0/1-%E8%BE%93%E5%87%BA%E7%BB%93%E6%9E%9C.png" class="" title="输出结果"></li>
</ol>
<hr>
<ol>
<li>plt.subplots<br>如果想要创建多个子图，可以使用plt.subplots方法创建“画布”。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#设置一块 2*2 的画布，figsize设置子图？整个画布？的尺寸</span></span><br><span class="line">f,ax = plt.subplots(nrows=<span class="number">2</span>,ncols=<span class="number">2</span>,figsize=(<span class="number">18</span>,<span class="number">12</span>))</span><br><span class="line"><span class="comment">#此时ax是一个多维的变量，可以通过flatten变成普通list索引访问</span></span><br><span class="line">ax = ax.flatten()</span><br><span class="line"><span class="comment">#假设此时要画3个hist，可是定义的画布是2*2的，也就是可以画4个图，最后一个会显示空白</span></span><br><span class="line">ax[<span class="number">0</span>].hist(train_text_lens)</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">&quot;train_text_lens&quot;</span>)</span><br><span class="line">ax[<span class="number">1</span>].hist(dev_text_lens)</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">&quot;dev_text_lens&quot;</span>)</span><br><span class="line">ax[<span class="number">2</span>].hist(test_text_lens)</span><br><span class="line">ax[<span class="number">2</span>].set_title(<span class="string">&quot;test_text_lens&quot;</span>)</span><br><span class="line"><span class="comment">#想要删除某个子图可以使用plt.delaxes方法</span></span><br><span class="line">plt.delaxes(ax[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<ol>
<li>AutoTokenizer<br>项目中AutoTokenizer较为常见，与特定的Tokenizer（类似于BertTokenizer）的区别在于，直接使用特定Tokenizer时，cache路径下不需要包括tokenizer.json文件。但是如果使用AutoTokenizer，cache中需要包含的就不仅仅是vocab、model.bin、config.json三个文件了，还需要一个类似于tokenizer.json的文件。</li>
</ol>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">re.finditer()<span class="comment">#找到字符串中的所有目标字符</span></span><br></pre></td></tr></table></figure>
<ol>
<li>json.load()和json.loads()的区别<br>json.loads()将<strong>字符串</strong>读取成<strong>Python数据结构</strong>，json.load()将<strong>文件</strong>读取成<strong>Python数据结构</strong><br>另外，如果键值对中包含整型数据，将其保存为json后，会将所有的数据转化为字符串。可以使用object_hook参数来解决这个问题。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line">path = <span class="string">&quot;something/something.json&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">jsonKey2int</span>(<span class="params">x</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(x,<span class="built_in">dict</span>):</span><br><span class="line">                <span class="keyword">return</span> &#123;<span class="built_in">int</span>(k) : v <span class="keyword">for</span> k,v <span class="keyword">in</span> x.items()&#125;</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(path,encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        json.load(f,obgject_hook=jsonKey2int)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>pytorch中nn.CrossEntropyLoss()的计算过程中是包含Softmax<br>如果在模型forward过程中自己计算softmax然后再使用交叉熵很可能会导致loss不下降<br>@生命奇点_ZRY 用交叉熵完成分类任务，线性层的输出可别softmax了喔</li>
</ol>
<hr>
<ol>
<li>引包<br>小项目的工作路径中直接包含若干python文件或包，编写代码时可以直接调用。如果项目规模较大，在工作路径中包含若干子项目时，可能会涉及到不同包下模块的引用，经常会出现无法引用自己写的块的问题。<blockquote>
<p>–project<br>—-main.py<br>—-folder1<br>–  –  test1.py<br>–  –  test3.py</p>
<hr>
</blockquote>
</li>
</ol>
<blockquote>
<p>—-folder2<br>–  –  test2.py<br>–  –  test4.py<br>最外层文件夹名称project，包含main.py和文件夹folder1、folder2。folder1中包含test1.py和test3.py，folder2中包含test2.py和test4.py。<br>想要正常import，模块必须在sys.path中能被找到，import的查找顺序为：</p>
</blockquote>
<ul>
<li>内置模块</li>
<li>.py文件所在目录（当前工作路径）</li>
<li>环境变量中列出的目录（虚拟环境）</li>
<li>pip 或 easy_install安装的包<br>可以通过打印sys.path的方式查看当前.py文件中sys.path包含了哪些内容<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> sys.path:</span><br><span class="line">        <span class="built_in">print</span>(i)</span><br></pre></td></tr></table></figure>
假设当前执行test1.py，打印其sys.path，可以发现folder1是在其中的，此时若需要引用folder2中的模块，那必然是会报错的，但是由于Pycharm等IDE可将floder2设置为source root则可以避免报错，使用cmd执行时就会报错。</li>
</ul>
<p><strong>如何解决？</strong></p>
<ul>
<li>sys.path.append(“..”)<br>想要调用父级目录中的模块时，可以将父级目录添加到sys.path中。<blockquote>
<p>from 父级目录.folder_name import module<br>这样就可以通过上述方式引用调用模块。</p>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>常用工具</category>
      </categories>
  </entry>
  <entry>
    <title>Feynmind的数学课堂-第一课</title>
    <url>/Feynmind%E7%9A%84%E6%95%B0%E5%AD%A6%E8%AF%BE%E5%A0%82-%E7%AC%AC%E4%B8%80%E8%AF%BE/</url>
    <content><![CDATA[<p>Markdown公式手册：<br><a href="https://freeopen.github.io/mathjax/">https://freeopen.github.io/mathjax/</a></p>
<p>$\lnot$  <br></p>
<p>$\land$ <br></p>
<p>$\rightarrow$  <br></p>
<p>$\lor $ <br></p>
<p>$\equiv$ $\iff$ $\leftrightarrow$  <br></p>
<h1 id="逻辑符号"><a href="#逻辑符号" class="headerlink" title="逻辑符号"></a>逻辑符号</h1><h2 id="关系与括号"><a href="#关系与括号" class="headerlink" title="关系与括号"></a>关系与括号</h2><p>优先级由高到低，我们会用到的逻辑符号有：<br></p>
<table>
<thead>
<tr>
<th>逻辑符号</th>
<th>中文读法</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>$\lnot$</td>
<td>非</td>
<td>原值取反</td>
</tr>
<tr>
<td>$\land$</td>
<td>与</td>
<td>两命题均为真，则结果为真</td>
</tr>
<tr>
<td>$\lor$</td>
<td>或</td>
<td>两命题中任意一命题为真，则结果为真</td>
</tr>
<tr>
<td>$\rightarrow$</td>
<td>蕴含</td>
<td>若….则….</td>
</tr>
<tr>
<td>$\iff$</td>
<td>等价</td>
<td>两命题等价</td>
</tr>
</tbody></table>
<p>真值表</p>
<table>
<thead>
<tr>
<th>A</th>
<th>B</th>
<th>$\lnot$ A</th>
<th>A $\land$ B</th>
<th>A $\lor$ B</th>
<th>A $\rightarrow$ B</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody></table>
<p>Attention：<br><br>对于命题A $\rightarrow$ B,当A为假时，B可以为真也可以为假</p>
<h2 id="集合及其初等运算："><a href="#集合及其初等运算：" class="headerlink" title="集合及其初等运算："></a>集合及其初等运算：</h2><ol>
<li>康托尔朴素集合定义：<br><br>(1) 集合可由任意不同的事务组成；<br><br>(2) 集合由构成它的事务集聚而唯一确定；<br><br>(3) 任何性质都定义一个具有该性质的事务的集合。<br><br>若$x$是一事务，$P$是一个性质，我们使用$P(x)$表示$x$有性质$P$;用{ $x$ | $P(x)$ }表示具有性质$P$的一切事物的集合，组成集合的事物，叫做集合的的元素。 <br></li>
<li>包含关系 <br><br>若$x$是集合$X$的元素，我们使用简单的符号<br><br>$x \in X$ 或 $ X\ni x $ <br><br>表示，而它的否命题，$x$不是$X$的元素，我们使用记号<br>$x \notin X$ 或者$ X \notni x$ <br><br>表示。<br></li>
</ol>
<p><strong>定义符号</strong>  :&#x3D;  ：表示左边的东西由右边定义 <br><br><strong>集合相等</strong> : $A&#x3D;B$ , 否命题为 $ A \neq B$ <br><br><strong>集合包含</strong> : $A \subset B :&#x3D; (x \in A) \rightarrow (x \in B)$<br><br><strong>真子集</strong> ：A $ \subseteq B $ ，表示A包含于B，且$A \neq B$  <br><br><strong>空集</strong> : $\emptyset$ 啥也没有的集合,<strong>空集是任意集合的子集</strong>。</p>
<h1 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h1><ol>
<li>验证逻辑基本的表达式以及相关的逻辑运算是否和你通常的观念相符，需要特别注意，如果$A$为假，那么$A\rightarrow B$总是真的。 <br><br>略 <br></li>
<li>画出$A \iff B$的真值表。<br></li>
</ol>
<table>
<thead>
<tr>
<th>A</th>
<th>B</th>
<th>$\lnot$ A</th>
<th>A $\land$ B</th>
<th>A $\lor$ B</th>
<th>A $\rightarrow$ B</th>
<th>$A \iff B$</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody></table>
<ol start="3">
<li>利用基本逻辑的真值表，验证如下逻辑表达式：<br><br>(1) $\lnot (A \land B) \iff \lnot A  \lor \lnot B$ <br></li>
</ol>
<table>
<thead>
<tr>
<th>A</th>
<th>B</th>
<th>$\lnot (A \land B)$</th>
<th>$ \lnot A  \lor \lnot B$</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody></table>
<p>(2) $\lnot (A \lor B) \iff \lnot A  \land \lnot B$ <br></p>
<table>
<thead>
<tr>
<th>A</th>
<th>B</th>
<th>$\lnot (A \lor B) $</th>
<th>$ \lnot A  \land \lnot B$</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody></table>
<p>(3) $(A \rightarrow B) \iff (\lnot B \rightarrow \lnot A)$ <br></p>
<table>
<thead>
<tr>
<th>A</th>
<th>B</th>
<th>$(A \rightarrow B) $</th>
<th>$ (\lnot B \rightarrow \lnot A)$</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody></table>
<p>(4) $(A \rightarrow B) \iff \lnot A \lor B$ <br></p>
<table>
<thead>
<tr>
<th>A</th>
<th>B</th>
<th>$(A \rightarrow B)$</th>
<th>$ \lnot A \lor B $</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody></table>
<p>(5) $\lnot (A\rightarrow B) \iff A \land \lnot B$</p>
<table>
<thead>
<tr>
<th>A</th>
<th>B</th>
<th>$\lnot (A\rightarrow B)$</th>
<th>$ A \land \lnot B $</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody></table>
<p>下课！</p>
]]></content>
      <categories>
        <category>Feynmind的数学课堂</category>
      </categories>
      <tags>
        <tag>数学分析</tag>
      </tags>
  </entry>
  <entry>
    <title>Feynmind的数学课堂-第三课</title>
    <url>/Feynmind%E7%9A%84%E6%95%B0%E5%AD%A6%E8%AF%BE%E5%A0%82-%E7%AC%AC%E4%B8%89%E8%AF%BE/</url>
    <content><![CDATA[<h1 id="数学归纳法"><a href="#数学归纳法" class="headerlink" title="数学归纳法"></a>数学归纳法</h1><p>数学归纳法是一种证明数学命题的方法，它用来证明一个关于所有自然数的命题。这种方法通常分为两步：</p>
<ol>
<li>基础步骤（Base Case）：首先需要证明命题对于最小的自然数（通常是1）成立。这是归纳过程的起点。</li>
<li>归纳步骤（Inductive Step）：然后假设命题对某个自然数n成立（称为归纳假设），并证明这个命题也必然对n+1成立。这一步骤表明如果命题对某个数n成立，那么它也对n+1成立。<br><br>通过这两个步骤，我们可以推断出命题对所有的自然数都成立，因为：<br><br>既然基础步骤证明了命题对1成立，归纳步骤保证了如果它对某个数n成立，就对n+1成立，<br>那么通过反复应用归纳步骤，命题就可以依次证明对2, 3, 4, …等所有自然数成立。<br><br>数学归纳法是数学中非常强大且常用的证明技术，尤其在处理数列、序列、组合数学、图论等领域的问题时。</li>
</ol>
<h1 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h1><ol>
<li>证明$1+2+3+\cdots +n &#x3D; \frac{n(n+1)}{2}$对$\forall n \in N^*$<br><br>当$n&#x3D;1$时：$1&#x3D;\frac{1(1+1)}{2}$ 成立<br><br>当$n&#x3D;2$时：$1+2&#x3D;\frac{2(1+2)}{2}$ 成立<br><br>假设$1+2+3+\cdots +n &#x3D; \frac{n(n+1)}{2}$成立,当$n&#x3D;n+1$时：<br><br>$\begin{array}{ll}<br>1+2+3+\cdots +n+(n+1) &amp;&#x3D; \frac{n(n+1)}{2}+(n+1) \<br>&amp;&#x3D;\frac{n(n+1)}{2} + \frac{2(n+1)}{2} \<br>&amp;&#x3D;\frac{(n+1)(n+2)}{2}<br>\end{array} $<br><br>成立，所以原命题$1+2+3+\cdots +n &#x3D; \frac{n(n+1)}{2}$成立。</li>
<li>证明：$1+3+5+\dots+(2n-1)&#x3D;n^2$<br><br>当$n&#x3D;1$时，$1&#x3D;1^2$ <br><br>当$n&#x3D;2$时，$1+3&#x3D;2^2$ <br><br>假设$1+3+5+\dots+(2n-1)&#x3D;n^2$成立，当n&#x3D;n+1时：<br><br>$\begin{array}{ll}<br>1+3+5+\dots+(2n-1)+(2(n+1)-1) &amp;&#x3D; n^2 +(2(n+1)-1) \<br>&amp;&#x3D; n^2 + (2n+1) \<br>&amp;&#x3D; (n+1)^2 \<br>成立<br>\end{array}$</li>
<li>伯努利不等式：$(1+x_1)(1+x_2)\cdots (1+x_n) \geq 1+x_1+x_2+\cdots +x_n$，其中$x_1,x_2,\cdots x_n$符号相同且$x_i&gt;-1$ <br><br>当$n&#x3D;1$时: $1+x_1 \geq 1+x_1$ 成立 <br><br>当$n&#x3D;2$时: <br><br>$\begin{array}{ll}<br>(1+x_1)(1+x_2) &amp;\geq 1+x_1+x_2 \<br>1+x_2+x_1+x_1x_2 &amp;\geq 1+x_1+x_2 \<br>x_1x_2 &amp;\geq 0<br>\end{array}$ <br><br>成立<br><br>假设$(1+x_1)(1+x_2)\cdots (1+x_n) \geq 1+x_1+x_2+\cdots +x_n$成立，当$n&#x3D;n+1$时：<br><br>$\begin{array}{ll}<br>(1+x_1)(1+x_2)\cdots (1+x_n)(1+x_{n+1}) &amp;\geq (1+x_1+x_2+\cdots +x_n)(1+x_{n+1}) \<br>&amp;\geq 1+x_1+x_2+\cdots +x_n + x_{n+1} + (x_1 + \cdots + x_n)x_{n+1} \<br>\because x_1,\cdots ,x_n 符号均相同 \<br>\therefore (x_1 + \cdots + x_n)x_{n+1} &gt; 0 \<br>\therefore 1+x_1+x_2+\cdots +x_n + x_{n+1}+ (x_1 + \cdots + x_n)x_{n+1} &amp;\geq 1+x_1+x_2+\cdots +x_n + x_{n+1} \<br>\therefore (1+x_1)(1+x_2)\cdots (1+x_n)(1+x_{n+1}) &amp;\geq 1+x_1+x_2+\cdots +x_n + x_{n+1} (加上一堆大于0的东西都比人家小，去掉之后更是要比人家要小)<br>\end{array}$ <br></li>
<li>证明：$1+\frac{1}{\sqrt{2}}+\frac{1}{\sqrt{3}}+\cdots+\frac{1}{\sqrt{n}}&gt;\sqrt{n}(n\geq2)$<br><br>当$n&#x3D;2$时:  <br><br>$\begin{array}{ll}<br>1+\frac{1}{\sqrt{2}} &amp;\geq \sqrt{2} \<br>\sqrt{2}(1+\frac{1}{\sqrt{2}}) &amp;\geq 2 \<br>\sqrt{2} + 1  &amp;\geq 2<br>\end{array}$<br><br>成立<br><br>假设$1+\frac{1}{\sqrt{2}}+\frac{1}{\sqrt{3}}+\cdots+\frac{1}{\sqrt{n}}&gt;\sqrt{n}(n\geq2)$成立，当n&#x3D;n+1时：<br></li>
</ol>
<p>$\begin{array}{ll}<br>1+\frac{1}{\sqrt{2}}+\frac{1}{\sqrt{3}}+\cdots+\frac{1}{\sqrt{n}}+\frac{1}{\sqrt{n+1}} &amp;&gt; \sqrt{n} + \frac{1}{\sqrt{n+1}} \<br>&amp; &gt;\frac{\sqrt{n}\cdot \sqrt{n+1} + 1}{\sqrt{n+1}} \<br>\end{array}$<br><br>$<br>\because \sqrt{n+1} &gt; \sqrt{n} \<br>\therefore 1+\frac{1}{\sqrt{2}}+\frac{1}{\sqrt{3}}+\cdots+\frac{1}{\sqrt{n}}+\frac{1}{\sqrt{n+1}} &gt; \frac{\sqrt{n}\cdot \sqrt{n} + 1}{\sqrt{n+1}}  \<br>\therefore 1+\frac{1}{\sqrt{2}}+\frac{1}{\sqrt{3}}+\cdots+\frac{1}{\sqrt{n}}+\frac{1}{\sqrt{n+1}} &gt; \sqrt{n+1}<br>$</p>
<h1 id="猜想"><a href="#猜想" class="headerlink" title="猜想"></a>猜想</h1><p>数学归纳法推理的时候：</p>
<ol>
<li>首先找若干个n，看命题是否成立；</li>
<li>若成立，按照命题的形式继续向下多写一个$n+1$项，并保持式子两侧成立，<br>$1+2+3+\dots+n$时就是多加一个$n+1$，$1+3+5+\dots+(2n-1)$时就是再加一个$2(n+1)-1$，并保持式子仍成立；</li>
<li>预想出式子右侧的最终形式，再用恒等变形或放缩得到结论。</li>
</ol>
]]></content>
      <categories>
        <category>Feynmind的数学课堂</category>
      </categories>
      <tags>
        <tag>数学分析</tag>
      </tags>
  </entry>
  <entry>
    <title>Feynmind的数学课堂-第一课</title>
    <url>/Feynmind%E7%9A%84%E6%95%B0%E5%AD%A6%E8%AF%BE%E5%A0%82-%E7%AC%AC%E4%BA%8C%E8%AF%BE/</url>
    <content><![CDATA[<p>(练习2全做，练习3除最后三个)</p>
<h1 id="集合之间的简单运算"><a href="#集合之间的简单运算" class="headerlink" title="集合之间的简单运算"></a>集合之间的简单运算</h1><p>首先我们假设集合$A$和$B$都是某个集合$M$的子集。</p>
<h2 id="集合的并"><a href="#集合的并" class="headerlink" title="集合的并"></a>集合的并</h2><p>$A\cup B :&#x3D; {x\in M|(x\in A)\lor (x\in B)}$</p>
<h2 id="集合的交"><a href="#集合的交" class="headerlink" title="集合的交"></a>集合的交</h2><p>$A\cap B:&#x3D;{x\in M|(x\in A)\land (x\in B)}$</p>
<h2 id="集合的差"><a href="#集合的差" class="headerlink" title="集合的差"></a>集合的差</h2><p>$A$ \ $B$ :&#x3D; ${x\in M |(x\in A)\land (x\notin B)}$ <br><br>$A$ - $B$ :&#x3D; ${x\in M |(x\in A)\land (x\notin B)}$</p>
<h2 id="补集"><a href="#补集" class="headerlink" title="补集"></a>补集</h2><p>集合$M$与它的子集$A$的差集通常叫做A在$M$中的补集，记作$C_MA$，也可以简单记作$\overline A$</p>
<h1 id="德摩根定律"><a href="#德摩根定律" class="headerlink" title="德摩根定律"></a>德摩根定律</h1><ul>
<li>$\overline{A\cup B} &#x3D; \overline A \cap \overline B $ <br><br>证明：<br><br>$\begin{array}{ll}<br>x\in \overline{(A\cup B)} &amp; \rightarrow x\notin A\cup B \<br>&amp; \rightarrow ((x\notin A)\land (x\notin B)) \<br>&amp; \rightarrow ((x\in \overline A) \land (x\in \overline B)) \<br>&amp; \rightarrow (x \in \overline A \cap \overline B)<br>\end{array} $<br><br>此时得到结论:$x\in \overline{(A\cup B)} \rightarrow (x \in \overline A \cap \overline B)$ <br><br>刚好符合$A\subset B$的定义 :&#x3D; $(x \in A) \rightarrow (x \in B)$<br><br>所以：$\overline{A\cup B} \subset (\overline A \cap \overline B)$<br>我们同样可以这样推理：<br><br>$\begin{array}{ll}<br>(x\in (\overline A \cap \overline B)) &amp; \rightarrow ((x\in \overline A) \land (x\in \overline B)) \<br>&amp; \rightarrow ((x\notin A) \land (x\notin B)) \<br>&amp; \rightarrow (x\notin A \cup B) \<br>&amp; \rightarrow (x\in \overline{A\cup B})<br>\end{array}$<br><br>即，$x\in (\overline A \cap \overline B) \rightarrow (x\in \overline{A\cup B})$，根据集合定义可得出$\overline A \cap \overline B \subset x\in \overline{A\cup B}$ <br><br>所以：$\overline{A\cup B} &#x3D; \overline A \cap \overline B $<br><br><br></li>
<li>$\overline{A\cap B} &#x3D; \overline A \cup \overline B$<br><br>证明:<br><br>$\begin{array}{ll}<br>x\in \overline{A\cap B} &amp; \rightarrow x \notin (A \cap B) \<br>&amp; \rightarrow (x \notin A) \lor (x\notin B) \<br>&amp; \rightarrow (x \in \overline A) \lor (x \in \overline B) \<br>&amp; \rightarrow x \in \overline A\cap \overline B \<br>x\in \overline{A\cap B} &amp; \subset x \in \overline A\cap \overline B \<br>\text{同时：} \<br>x \in \overline A\cap \overline B &amp; \rightarrow ((x \in \overline A) \lor x\in \overline B) \<br>&amp; \rightarrow ((x \notin A) \lor (x \notin B)) \<br>&amp; \rightarrow  x \notin A \cap B \<br>&amp; \rightarrow x \in \overline{A\cap B}\<br>\overline A\cap \overline B \subset \overline{A\cap B}\<br>\end{array}$<br><br>所以：$\overline{A\cap B} &#x3D; \overline A \cup \overline B$</li>
</ul>
<h1 id="课后作业"><a href="#课后作业" class="headerlink" title="课后作业"></a>课后作业</h1><h2 id="练习二"><a href="#练习二" class="headerlink" title="练习二"></a>练习二</h2><p>假设$A\subset M,B\subset M,C\subset M$</p>
<ol>
<li>验证下面的关系式：<br><br>（1） $(A\subset C) \land (B\subset C) \iff ((A\cup B)\subset C)$ <br><br>$A\subset C,B\subset C \rightarrow (A\cup B) \subset C$ <br></li>
</ol>
<p>$\begin{array}{ll}<br>((A\cup B)\subset C) &amp;\rightarrow (x\in (A\cup B)  \land  x\in C) \<br>&amp; \rightarrow ((x\in A \lor x\in B) \land x\in C ) \<br>&amp; \rightarrow (x\in A \land x\in C) \lor (x\in B \land x\in C) \<br>&amp; \rightarrow (x\in A\cap C) \lor x\in(B\cap C) \<br>&amp; \rightarrow (x\in (A\cap C)\cup(B\cap C)) \<br>&amp; \rightarrow ((A\cap C)\cup(B\cap C))\<br>&amp; \rightarrow (A\cup B)\cap C<br>\end{array}$ </p>
<p>（2） $(C\subset A) \land (C\subset B) \iff (C\subset (A\cap B))$ <br><br>（3） <br><br>（4）$(A\subset \overline B) \iff (B\subset \overline A)$<br><br>（5）$(A\subset B) \iff (\overline B \subset \overline A )$<br><br>2. 试证明：<br><br>（1）$A\cup (B\cup C) &#x3D; (A \cup B)\cup C$<br><br>（2）$A\cap (B\cap C) &#x3D; (A\cap B)\cap C$<br><br>（3）$A\cap (B\cup C) &#x3D; (A\cap B) \cup (A\cap C)$<br><br>（4）$A\cup(B\cap C) &#x3D; (A \cup B) \cap (A\cup C)$<br></p>
<ol start="3">
<li>验证并与交的对偶关系<br><br>（1）$\overline{A\cup B} &#x3D; \overline A \cap \overline B$<br><br>证明：<br><br>$\begin{array}{ll}<br>x\in \overline{(A\cup B)} &amp; \rightarrow x\notin A\cup B \<br>&amp; \rightarrow ((x\notin A)\land (x\notin B)) \<br>&amp; \rightarrow ((x\in \overline A) \land (x\in \overline B)) \<br>&amp; \rightarrow (x \in \overline A \cap \overline B)<br>\end{array} $<br><br>此时得到结论:$x\in \overline{(A\cup B)} \rightarrow (x \in \overline A \cap \overline B)$ <br><br>刚好符合$A\subset B$的定义 :&#x3D; $(x \in A) \rightarrow (x \in B)$<br><br>所以：$\overline{A\cup B} \subset (\overline A \cap \overline B)$<br>我们同样可以这样推理：<br><br>$\begin{array}{ll}<br>(x\in (\overline A \cap \overline B)) &amp; \rightarrow ((x\in \overline A) \land (x\in \overline B)) \<br>&amp; \rightarrow ((x\notin A) \land (x\notin B)) \<br>&amp; \rightarrow (x\notin A \cup B) \<br>&amp; \rightarrow (x\in \overline{A\cup B})<br>\end{array}$<br><br>即，$x\in (\overline A \cap \overline B) \rightarrow (x\in \overline{A\cup B})$，根据集合定义可得出$\overline A \cap \overline B \subset x\in \overline{A\cup B}$ <br><br>所以：$\overline{A\cup B} &#x3D; \overline A \cap \overline B $<br></li>
</ol>
<p>（2）$\overline{A\cap B} &#x3D; \overline A \cup \overline B$<br><br>证明:<br><br>$\begin{array}{ll}<br>x\in \overline{A\cap B} &amp; \rightarrow x \notin (A \cap B) \<br>&amp; \rightarrow (x \notin A) \lor (x\notin B) \<br>&amp; \rightarrow (x \in \overline A) \lor (x \in \overline B) \<br>&amp; \rightarrow x \in \overline A\cap \overline B \<br>x\in \overline{A\cap B} &amp; \subset x \in \overline A\cap \overline B \<br>\text{同时：} \<br>x \in \overline A\cap \overline B &amp; \rightarrow ((x \in \overline A) \lor x\in \overline B) \<br>&amp; \rightarrow ((x \notin A) \lor (x \notin B)) \<br>&amp; \rightarrow  x \notin A \cap B \<br>&amp; \rightarrow x \in \overline{A\cap B}\<br>\overline A\cap \overline B \subset \overline{A\cap B}\<br>\end{array}$<br><br>所以：$\overline{A\cap B} &#x3D; \overline A \cup \overline B$</p>
<ol start="4">
<li>试证明：<br><br>（1）$X\times Y&#x3D;\emptyset \iff (X&#x3D;\emptyset) \land (Y &#x3D; \emptyset)$<br><br>（2）$X\times Y \ne \emptyset ,(A\times B\subset X\times Y \iff (A \subset X)\land (B\subset Y))$<br><br>（3）$X\times Y \ne \emptyset ,(X\times Y)\cup (Z\times Y) &#x3D; (X\cup Z)\times Y$<br><br>（4）$X\times Y \ne \emptyset ,(X\times Y)\cap (X’\times Y’)\times(Y\times Y’)$<br></li>
</ol>
]]></content>
      <categories>
        <category>Feynmind的数学课堂</category>
      </categories>
      <tags>
        <tag>数学分析</tag>
      </tags>
  </entry>
  <entry>
    <title>GBDT</title>
    <url>/GBDT/</url>
    <content><![CDATA[<p>XGBoost的基础是梯度提升算法，因此我们必须先从了解梯度提升算法开始。梯度提升（Gradient boosting）是构建预测模型的最强大技术之一，它是集成算法中提升法（Boosting）的代表算法。集成算法通过在数据上构建多个弱评估器，汇总所有弱评估器的建模结果，以获取比单个模型更好的回归或分类表现。<br>集成不同弱评估器的方法有很多种。有像我们曾经在随机森林的课中介绍的，一次性建立多个平行独立的弱评估器的装袋法。也有像我们今天要介绍的提升法这样，逐一构建弱评估器，经过多次迭代逐渐累积多个弱评估器的方法。提升法的中最著名的算法包括Adaboost和梯度提升树，XGBoost就是由梯度提升树发展而来的。梯度提升树中可以有回归树也可以有分类树，两者都以CART树算法作为主流，XGBoost背后也是CART树，这意味着XGBoost中所有的树都是二叉的。<br>对于梯度提升回归树来说，每个样本的预测结果可以表示为所有树上的结果的加权求和：<br>\haty_i^(k) &#x3D; \sum^K_k \gama_k h_k(x_i)<br>其中，K是树的总数量， k代表第k棵树， \gama_k是这棵树的权重，h_k表示这棵树上的预测结果。</p>
]]></content>
  </entry>
  <entry>
    <title>Git使用</title>
    <url>/Git%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h2 id="什么是Git？"><a href="#什么是Git？" class="headerlink" title="什么是Git？"></a>什么是Git？</h2><p>分布式版本控制工具</p>
<h2 id="为什么要用Git？"><a href="#为什么要用Git？" class="headerlink" title="为什么要用Git？"></a>为什么要用Git？</h2><pre><code>因为需要版本控制，尤其多人协作开发的时候，用于版本回滚。
主流的版本控制器有：
SVN（Subversion）
CVS（Concurrent Versions System）
VSS（Micorosoft Visual SourceSafe）
TFS（Team Foundation Server）
Visual Studio Online
版本控制产品非常的多，影响力最大且使用最广泛的就是Git。
</code></pre>
<h3 id="与SVN的区别？"><a href="#与SVN的区别？" class="headerlink" title="与SVN的区别？"></a>与SVN的区别？</h3><p>SVN时集中版本控制工具，所有人在进行开发时都要在联网状态下从远程仓库中拉取最新代码才能进行开发，开发后还要上传到远程服务器。如果服务器损坏将无法继续工作。<br>Git是分布式版本控制工具，所有参与开发的人员手中都有完整的项目代码，内容变更时只需要将自己更新的内容推送给其他人即可。此外，开发过程中不依赖网络，服务器损毁对工作人员也无影响。</p>
<h2 id="安装Git及环境配置"><a href="#安装Git及环境配置" class="headerlink" title="安装Git及环境配置"></a>安装Git及环境配置</h2><p>访问<a href="https://git-scm.com/downloads">git官方网站</a>下载适合自己硬件环境的安装包，无脑下一步即可。</p>
<h2 id="Git的必要配置"><a href="#Git的必要配置" class="headerlink" title="Git的必要配置"></a>Git的必要配置</h2><p>1.配置用户名</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git config --global user.name <span class="string">&quot;你的名字&quot;</span></span><br></pre></td></tr></table></figure>
<p>2.配置邮箱</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git config --global user.email <span class="string">&quot;你的@邮箱.com&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><p>git中包含工作目录、暂存区、本地仓库和远程仓库4个工作区域。<br>工作区就是平时存放项目代码的路径；暂存区用于临时存放变更的内容，其本质是一个文件，保存即将提交到文件列表的信息；本地仓库是安全存放数据的位置，这里面有你提交的所有版本的数据。其中HEAD只想最新放入仓库的版本；远程仓库就是托管代码的服务器，简单理解就是github中后缀是“.git”的仓库。与使用者之间直接相关的只有工作目录和远程仓库，暂存区和本地仓库只涉及到一些命令操作。<br>在工作目录使用git add指令将变更内容推送至暂存区，暂存区的内容使用git commit推送至本地仓库，使用git push将本地仓库中的内容推送至远程仓库;反向操作时，使用git pull将远程仓库的内容回滚到本地仓库中，使用git reset将本地仓库的内容回滚到暂存区中，使用git checkout将暂存区中的内容回滚到工作目录中，上述命令的使用如下图所示。</p>
<img src="/Git%E4%BD%BF%E7%94%A8/%E5%85%A8%E9%83%A8%E5%91%BD%E4%BB%A4.png" class="" title="工作区域涉及到的指令">
<h2 id="Git项目创建及克隆"><a href="#Git项目创建及克隆" class="headerlink" title="Git项目创建及克隆"></a>Git项目创建及克隆</h2><p>在工作路径中右键空白处，唤出git bash界面。<br>1.本地仓库搭建<br>在git bash界面中输入以下指令，初始化git项目。 </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure>
<p>init结束后会在工作路径中生成一个隐藏的.git文件夹，表示当前工作路径已然是一个Git项目。git init后可以在将本地变动添加至暂存区和本地仓库。<br>2.配置远程仓库<br>init后如果需要将本地内容推送至服务器，需要使用git remote指令对远程服务器地址进行配置。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git remote add origin url</span><br><span class="line"><span class="comment"># 如git remote add origin ssh:/xxx.git</span></span><br></pre></td></tr></table></figure>
<p>需要修改url时可以使用git remote set-url指令。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git remote set-url origin url</span><br><span class="line"><span class="comment"># 如git remote set-url origin ssh:/xxx.git</span></span><br></pre></td></tr></table></figure>
<p>另外，remote指令还可以用来验证是否成功添加配置。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git remote -v</span><br><span class="line"><span class="comment"># 如git remote set-url origin ssh:/xxx.git</span></span><br></pre></td></tr></table></figure>

<p><strong>推荐姿势</strong><br>使用init的方式创建项目后，还是需要在github中创建仓库之后再使用remote进行配置。不如先在github中创建库存然后使用gitclone的方式将库存下载到本地，省去一捏捏配置的步骤。<br>创建库存后制git链接，在工作路径中唤出git bash界面，使用git clone “.git链接”下载该项目的代码，复制git链接的方法如图所示。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> xxx.git</span><br></pre></td></tr></table></figure>
<img src="/Git%E4%BD%BF%E7%94%A8/%E5%8D%9A%E5%AE%A2-%E5%A4%8D%E5%88%B6git%E9%93%BE%E6%8E%A5.png" class="" title="复制git链接">

<h2 id="gitignore文件的配置使用"><a href="#gitignore文件的配置使用" class="headerlink" title=".gitignore文件的配置使用"></a>.gitignore文件的配置使用</h2><p>使用git过程中，有些文件比如日志、临时文件、数据等通常不会被提交到代码仓库当中，这时就需要设置相应的忽略规则，避免不必要的文件上传到远程仓库当中，.gitignore文件就是用于设定工作路径中哪些文件不会被上传。在Github中创建库存时，勾选生成.gitignore文件选项即可生成该文件。clone到本地后可以对其进行修改配置，.gitignore文件的内容书写方式如下。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#不需要提交的目录</span></span><br><span class="line">/folder name</span><br><span class="line"><span class="comment">#不需要提交的文件</span></span><br><span class="line">_config.yml</span><br><span class="line">.gitignore</span><br><span class="line"><span class="comment">#log 不需要提交任意后缀名为log的文件</span></span><br><span class="line">*.<span class="built_in">log</span></span><br><span class="line"><span class="comment">#package Files 不需要提交的任意包含后缀名为jar的文件</span></span><br><span class="line">*.jar</span><br></pre></td></tr></table></figure>
<p>这里只对gitignore文件的基本用法进行介绍，详见<a href="https://zhuanlan.zhihu.com/p/52885189">[Git].gitignore文件的配置使用</a>。</p>
<h2 id="Git的基本操作命令"><a href="#Git的基本操作命令" class="headerlink" title="Git的基本操作命令"></a>Git的基本操作命令</h2><h3 id="？？？Git中文件的4种状态"><a href="#？？？Git中文件的4种状态" class="headerlink" title="？？？Git中文件的4种状态"></a>？？？Git中文件的4种状态</h3><p>Git中，文件包含<strong>未跟踪（Untracked）</strong>、<strong>暂存状态（Staged）</strong>、<strong>未修改（Unmodify）</strong>和<strong>已修改（Modified）</strong>。下面用一个例子对这4种状态进行解释说明。<br>假设现在创建一个hello.py文件，该文件并未加入到git库中，不参与版本控制，所以处于“未跟踪状态”；使用git add . 指令将该文件存入暂存区后，则其状态为“暂存状态”；使用git commit指令将其推送至本地仓库后，暂存区为空，此时该文件所处状态为“未修改”。此时若对文件进行了修改，则其状态为“Modified”，使用add加入暂存区后，commit即可将其重新推送到本地仓库中。</p>
<h3 id="暂存区"><a href="#暂存区" class="headerlink" title="暂存区"></a>暂存区</h3><p>使用git add 指令可以将未跟踪文件和已修改文件添加至暂存区。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git add . <span class="comment"># “.” 表示将当前路径所有内容都添加至暂存区</span></span><br><span class="line">git add filename <span class="comment">#将某文件单独添加至暂存区</span></span><br></pre></td></tr></table></figure>
<p>如若想要撤回暂存区中的内容，可以使用git reset指令。<br>文件撤出暂存区，但保留修改：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git reset --mixed</span><br></pre></td></tr></table></figure>
<p>撤销所有已经add的文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git reset HEAD .</span><br></pre></td></tr></table></figure>
<p>撤销某个文件或文件夹</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git reset HEAD  -filename</span><br></pre></td></tr></table></figure>
<h3 id="本地仓库"><a href="#本地仓库" class="headerlink" title="本地仓库"></a>本地仓库</h3><p>git commit指令可以将暂存区中的内容提交至本地仓库中，在使用commit指令时<strong>一定要附带-m参数</strong>，为当前commit添加注释信息。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git commit</span><br></pre></td></tr></table></figure>

<h2 id="版本控制"><a href="#版本控制" class="headerlink" title="版本控制"></a>版本控制</h2><h3 id="本地版本控制"><a href="#本地版本控制" class="headerlink" title="本地版本控制"></a>本地版本控制</h3><h3 id="集中版本控制"><a href="#集中版本控制" class="headerlink" title="集中版本控制"></a>集中版本控制</h3><h2 id="Git的特点"><a href="#Git的特点" class="headerlink" title="Git的特点"></a>Git的特点</h2><p>1.分布式版本管理工具<br>2.敏捷开发<br>3.开源</p>
<h2 id="十八般武艺"><a href="#十八般武艺" class="headerlink" title="十八般武艺"></a>十八般武艺</h2><h3 id="git-stash"><a href="#git-stash" class="headerlink" title="git stash"></a>git stash</h3><p>用于临时保存修改的文件。修改文件时，需要临时切换到其他分支，但是还不想提交，可以用stash临时保存。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git stash save <span class="string">&quot;xxx&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="git-log"><a href="#git-log" class="headerlink" title="git log"></a>git log</h3><p>包含两个参数“–oneline”和“–graph”可以帮助看到日志的流程？</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">log</span> --oneline --graph</span><br></pre></td></tr></table></figure>
<h3 id="git-push"><a href="#git-push" class="headerlink" title="git push"></a>git push</h3><p>用于删除远程分支： </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git push branch_name</span><br></pre></td></tr></table></figure>
<h3 id="git-rm"><a href="#git-rm" class="headerlink" title="git rm"></a>git rm</h3><p>如果错误commit，想去除掉文件，可以使用rm命令，可以将暂存区未commit的文件去除？</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">rm</span> --cached filename</span><br></pre></td></tr></table></figure>
<h3 id="git-commit"><a href="#git-commit" class="headerlink" title="git commit"></a>git commit</h3><p>提交commit但是不创建新的提交记录，再上一次log中更新内容，可以用来压缩日志</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git commit --amend</span><br></pre></td></tr></table></figure>
<h3 id="git-cherry-pick"><a href="#git-cherry-pick" class="headerlink" title="git cherry-pick"></a>git cherry-pick</h3><p>提交特定一次修改</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git cherry-pick commit-id</span><br></pre></td></tr></table></figure>
<h3 id="git-archive"><a href="#git-archive" class="headerlink" title="git archive"></a>git archive</h3><p>文件快照打包</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git archive -o archive.zip master</span><br></pre></td></tr></table></figure>
<h2 id="代码冲突"><a href="#代码冲突" class="headerlink" title="代码冲突"></a>代码冲突</h2><p>解决代码冲突的常见两个方法</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git merge master</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git rebase -i master</span><br></pre></td></tr></table></figure>
<h2 id="远程仓库"><a href="#远程仓库" class="headerlink" title="远程仓库"></a>远程仓库</h2><p>如何将本地文件和远程仓库文件关联起来？<br>项目路径要保证是被git管理的路径，即“git init”，然后使用-v参数查看与哪些远程仓库有关联，然后使用add参数添加关联。<br>可以使用rm指令删除关联。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git remote -v   <span class="comment">#查看当前路径与哪些远程仓库关联，没有则为空</span></span><br><span class="line">git remote add origin git@github.com:xxxx.git</span><br><span class="line">git remote <span class="built_in">rm</span> origin</span><br></pre></td></tr></table></figure>
<p>本地内容变更后，使用commit将内容推送至缓存区，想要推送到远程仓库时，需要用到push指令，-u用于指定上游分支</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git push -u prigin master</span><br></pre></td></tr></table></figure>
<p>？从远程仓库获取指定分支</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git checkout -b local_branch_yy origin/remote_branch_yy</span><br></pre></td></tr></table></figure>
<h2 id="压缩提交记录"><a href="#压缩提交记录" class="headerlink" title="压缩提交记录"></a>压缩提交记录</h2><p>如果有多次提交记录，却只想保留一条记录怎么办？</p>
<h3 id="reset"><a href="#reset" class="headerlink" title="reset"></a>reset</h3><p>在当前commit中包含多个comit时，</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git reset commit-id</span><br><span class="line">git add filename</span><br><span class="line">git commit -m <span class="string">&quot;xxx&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="amend"><a href="#amend" class="headerlink" title="amend"></a>amend</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git commit --amend</span><br></pre></td></tr></table></figure>
<h3 id="rebase"><a href="#rebase" class="headerlink" title="rebase"></a>rebase</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git rebase -i commit-id</span><br><span class="line">pick ef124f 保留</span><br><span class="line">squash fa4f8d 压缩</span><br></pre></td></tr></table></figure>
<h2 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git tag tag_name</span><br><span class="line">git tag tag_name commit-id</span><br><span class="line">git tag -a tag_name -m <span class="string">&quot;xxx&quot;</span> commit-id</span><br><span class="line">git tag</span><br><span class="line">git show tag_name</span><br><span class="line">git push origin tag_name</span><br></pre></td></tr></table></figure>


<h2 id="使用经验"><a href="#使用经验" class="headerlink" title="使用经验"></a>使用经验</h2><p>自己使用git的转变，这鬼东西好难学啊。</p>
<h3 id="第一层"><a href="#第一层" class="headerlink" title="第一层"></a>第一层</h3><p>欸？这个项目好像不错欸，下下来康康！这时候git对我来说只是一个下载代码的地方，所用到的指令基本上也就是clone了。下载之后自己在本地跑一跑改一改，基本不设计到其他指令，甚至是直接下载压缩包。。不需要用指令。。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> something.git</span><br></pre></td></tr></table></figure>
<h3 id="第二层"><a href="#第二层" class="headerlink" title="第二层"></a>第二层</h3><p>以后我把我自己写的代码都存上云，谁想了解我我就丢他个github链接！这时候就是自己创建一个远程仓库，把自己更改过的代码上传到仓库当中，每次变动后只要add、commit、push就可以了。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> workspace</span><br><span class="line">git add .</span><br><span class="line">git commit -m <span class="string">&quot;something upload&quot;</span></span><br><span class="line">git push</span><br></pre></td></tr></table></figure>

<h3 id="第三层"><a href="#第三层" class="headerlink" title="第三层"></a>第三层</h3><p>跟一个小伙伴一起写代码～这个时候就需要开始接触分支了，各自在自己的分支上更新代码，两人同时编写一个文件时，上传的时候可能就要遇到冲突。所以在每次上传之前需要先从远程仓库将最新的内容pull下来，然后再把本地的变动推送到远程服务器。这时候就可以使用stash指令，暂时将本地的内容恢复到变动之前，从远程pull最新内容后使用stash pop指令恢复本地的改动，然后再进行add、commit、push上传自己的代码。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> something.git</span><br><span class="line">git branch selfBranch</span><br><span class="line">git checkout selfBranch</span><br><span class="line">git stash</span><br><span class="line">git pull origin selfBranch</span><br><span class="line">git stash pop</span><br><span class="line">git add .</span><br><span class="line">git commit -m <span class="string">&quot;something upload&quot;</span></span><br><span class="line">git push origin selfBranch</span><br></pre></td></tr></table></figure>

<h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p><a href="https://www.bilibili.com/video/BV1qv4y1j7Uh/?spm_id_from=333.1007.tianma.6-1-19.click&vd_source=3269363961c1c9a10f72f01393a219fd">你什么档次?也配跟我用同一款Git</a></p>
]]></content>
      <categories>
        <category>常用工具</category>
      </categories>
  </entry>
  <entry>
    <title>Hexo + Github page + Butterfly + Github Action搭建个人博客</title>
    <url>/Hexo%20+%20Github%20page%20+%20Butterfly%20+%20Github%20Action%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<p><a href="https://zhangshier.vip/posts/41646/">Hexo博客搭建</a><br><a href="https://zhangshier.vip/posts/8779/#4-3-%E6%96%B9%E6%A1%88%E4%B8%89">Hexo 博客利用 Github Action 自动化部署</a><br><a href="https://sanonz.github.io/2020/deploy-a-hexo-blog-from-github-actions/">利用 Github Actions 自动部署 Hexo 博客</a><br><a href="https://space.bilibili.com/372204786/channel/collectiondetail?sid=658132">hexo博客搭建从入门到入土完全优化系列</a><br><a href="https://blog.csdn.net/qq_34243930/category_9662425.html">Github Pages+Hexo搭建博客</a></p>
<h2 id="搭建流程"><a href="#搭建流程" class="headerlink" title="搭建流程"></a>搭建流程</h2><p>由于Github提供了免费的Page服务，所以就有了Hexo生成站点然后部署在Github Page上的玩法。博客的页面设计、部署等问题可以通过Hexo一键完成，作者只需要关注文章内容即可。本篇博文只对Windows操作系统的博客搭建过程进行记录，其他需求还请读者自行探索。搭建博客的整体思路是，首先使用Hexo一键生成博客站点，并使用butterfly对站点进行美化。然后，将站点部署到Github page上，即可通过访问[git_user_name].github.io来查看博客内容。接着，单独为博客内容创建一个仓库，使用Github Action监听内容仓库，如果内容仓库中有更新则重新将内容重新部署到博客当中。</p>
<h2 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h2><p>Hexo的安装方法可以直接参考<a href="https://hexo.io/zh-cn/docs/writing">Hexo官方文档</a>，为保证博文结构完整性，将官方文档中的相应步骤抄录至本文中。</p>
<h3 id="安装前提"><a href="#安装前提" class="headerlink" title="安装前提"></a>安装前提</h3><p>在安装Hexo之前需要先安装下列应用程序：<br><a href="https://nodejs.org/en">Node.js</a> (Node.js 版本需不低于 10.13，建议使用 Node.js 12.0 及以上版本)<br><a href="https://git-scm.com/">Git</a><br>两者的安装过程中并无任何特殊配置，只需要点击“下一步”完成安装即可，读者可以根据自身喜好在安装过程中进行相应设置。安装完成后，可以在CMD中通过以下指定，测试是否安装成功。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ node -v</span><br><span class="line">$ npm -v</span><br><span class="line">$ git -v</span><br></pre></td></tr></table></figure>

<p>这里要记录一下node -v的结果，后续配置_config.yml时会使用到。<br>安装必备的应用程序后，需要创建一个空文件夹作为博客的项目路径，接下来我们将称这个创建好的文件夹为<strong>博客项目路径</strong>。进入博客项目路径，空白处点击右键，选择“Git Bash Here”，准备安装搭建博客所需要的工具。</p>
<h3 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h3><p>Hexo的安装方式相当简单。安装必备的应用程序后，使用 npm 指令安装即可，过程可能需要持续若干分钟。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure>

<p>安装完毕后，可以在CMD中使用hexo -v指令查看hexo版本以确定是否安装成功。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo -v</span><br></pre></td></tr></table></figure>

<h2 id="建站"><a href="#建站" class="headerlink" title="建站"></a>建站</h2><p>安装阶段结束后开始建站，hexo可以使用init指令生成项目相关文件。</p>
<img src="/Hexo%20+%20Github%20page%20+%20Butterfly%20+%20Github%20Action%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/gitBash.png" class="" title="git bash">

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo init</span><br></pre></td></tr></table></figure>

<p>执行完毕后，使用hexo s指令就可以在浏览器中输入“localhost:4000”访问博客，看到博客内容。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo s</span><br></pre></td></tr></table></figure>

<h2 id="Butterfly安装及配置"><a href="#Butterfly安装及配置" class="headerlink" title="Butterfly安装及配置"></a>Butterfly安装及配置</h2><p>Butterfly的安装可以直接按照<a href="https://butterfly.js.org/posts/21cfbf15/#%E5%AE%89%E8%A3%9D">官方文档</a>逐步进行。为保持博文结构完整性，同样将其中的内容摘抄过来，读者可能要注意命令的时效性。<br>首先，进入博客项目路径，呼出Git界面，输入以下命令。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> -b master https://github.com/jerryc127/hexo-theme-butterfly.git themes/butterfly</span><br></pre></td></tr></table></figure>

<p>然后，将博客项目路径中的_config.yml中的[theme]对应的值修改为[butterfly]。<br>安装完成后可以在博客项目路径中看到butterfly文件夹，其中包含了主题的配置文件_config.yml，复制其中的内容，在博客项目路径中创建_config.butterfly.yml，将之前复制的内容粘贴后保存。<br>（使用过程中未能正确配置mathjax，无法渲染公式，所以最后放弃butterfly使用next主题）</p>
<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>如果希望其他人也能看到你的博客，那就需要将博客部署在服务器或Github上。Github完全可以满足笔者对博客的需求，所以并未探索如何将博客部署在服务器上，读者有相关需求可以自行探索。</p>
<h3 id="创建Github仓库"><a href="#创建Github仓库" class="headerlink" title="创建Github仓库"></a>创建Github仓库</h3><p>创建仓库需要读者已经创建过Github账户，这里不对如何注册Github账户进行赘述。拥有Github账户后，需要在本地配置账户信息。在任意位置唤出git bash，通过以下指令配置用户和邮箱。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git config --global user.name <span class="string">&quot;你的名字&quot;</span></span><br><span class="line">$ git config --global user.email <span class="string">&quot;你的@邮箱.com&quot;</span></span><br></pre></td></tr></table></figure>

<p>创建的仓库要求与Github用户名完全相同，并勾选“Add a README file”选项。</p>


<h3 id="创建SSH和GPGkeys"><a href="#创建SSH和GPGkeys" class="headerlink" title="创建SSH和GPGkeys"></a>创建SSH和GPGkeys</h3><p>在git bash命令窗口中输入命令生成SSH密钥，命令如下所示，注意-C是大写字母C，邮箱要加引号。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -C <span class="string">&quot;你的@邮箱.com&quot;</span></span><br></pre></td></tr></table></figure>

<p>生成的内容可以在路径“C:\用户\用户名.ssh\id_rsa.pub”中查看到，将其中所有内容复制。<br>在Github中打开个人设置页面，左侧选中“SSH and GPG keys”，Title中的内容读者可以任意定义，在Key中粘贴刚刚复制的内容，然后点击“Add SSH key”。</p>
<img src="/Hexo%20+%20Github%20page%20+%20Butterfly%20+%20Github%20Action%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/%E5%8D%9A%E5%AE%A2-%E4%B8%AA%E4%BA%BA%E8%AE%BE%E7%BD%AE.png" class="" title="个人设置">
<img src="/Hexo%20+%20Github%20page%20+%20Butterfly%20+%20Github%20Action%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/%E5%8D%9A%E5%AE%A2-SSH%E5%AF%86%E9%92%A5.png" class="" title="SSH密钥">

<h3 id="开始部署"><a href="#开始部署" class="headerlink" title="开始部署"></a>开始部署</h3><p>在博客的项目路径中唤出git bash窗口，安装部署工具，指令如下所示。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>

<p>然后，在github中打开创建好的仓库，复制其git链接，如图所示。</p>
<img src="/Hexo%20+%20Github%20page%20+%20Butterfly%20+%20Github%20Action%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/%E5%8D%9A%E5%AE%A2-%E5%A4%8D%E5%88%B6git%E9%93%BE%E6%8E%A5.png" class="" title="复制git链接">
<p>然后修改博客项目路径中的“_config.yml”文件，在yml文件中，最下方可以看到“deploy:”，在其下方添加type、repo和branch，填写的内容如图所示。</p>
<img src="/Hexo%20+%20Github%20page%20+%20Butterfly%20+%20Github%20Action%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/%E5%8D%9A%E5%AE%A2-%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE.png" class="" title="修改配置文件">

<h3 id="推送至Github"><a href="#推送至Github" class="headerlink" title="推送至Github"></a>推送至Github</h3><p>使用clean指令清除缓存，然后使用g、d指令将博客内容推送至github，期间按照要求输入yes即可。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo c</span><br><span class="line">$ hexo g</span><br><span class="line">$ hexo d</span><br></pre></td></tr></table></figure>

<p>执行完毕后，即可通过访问“[git_user_name].github.io”直接看到博客内容。<br>上述这种写博文、hexo clean、hexo g、hexo d更新博客的方式是hexo的基础用法。如果读者写博客的场景比较固定，不会涉及到多设备更新博客的话，hexo的基础用法已经足够满足需求了。但是如果写博文的设备并不固定，基础用法就不那么便利了。</p>
<h2 id="配置Github-Action"><a href="#配置Github-Action" class="headerlink" title="配置Github Action"></a>配置Github Action</h2><p>基础用法每次都会将本地的博文重新部署至github中，“删除文章”这一功能也是通过这种方式实现的，只要在本地删除博文后重新部署即可达到删除博文的目的。换句话说，如果在一个未保存全部博文的设备上更新内容，以往的心血就全被“删除”了。为避免上述情况出现，我们使用Github action来“备份”博客内容。首先，在Github中创建一个私有仓库，用于存储博客内容。然后，在本地编辑博文本，将新内容上传到博客内容仓库中，内容变动时触发Action，将新内容再部署到博客页面中。如果使用新设备撰写博客，只需要git clone博客内容仓库后编辑文本，重新push到仓库中即可。</p>
<h3 id="创建存放博文内容的私有仓库"><a href="#创建存放博文内容的私有仓库" class="headerlink" title="创建存放博文内容的私有仓库"></a>创建存放博文内容的私有仓库</h3><p>在创建过程中注意勾选private即可。</p>
<h3 id="创建Token"><a href="#创建Token" class="headerlink" title="创建Token"></a>创建Token</h3><p>访问 Github注意-&gt;右上角头像处-&gt;Settings-&gt;Developer Settings-&gt;Personal access tokens-&gt;generate new token,选择classic token即可，创建的 Token 名称随意，但必须勾选 repo 项 和 workflows 项。</p>
<img src="/Hexo%20+%20Github%20page%20+%20Butterfly%20+%20Github%20Action%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/%E5%88%9B%E5%BB%BA%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93.png" class="" title="创建私有仓库">
<p>创建好token后<strong>暂时不要关闭页面</strong>，这个token只在此处显示一次，关闭之后就无法找回Token内容。</p>
<h3 id="配置Github-Action-1"><a href="#配置Github-Action-1" class="headerlink" title="配置Github Action"></a>配置Github Action</h3><ol>
<li>在本地的博客项目路径中创建[.github]文件夹，在其中创建[workflows]文件夹，在其中创建[autodeploy.yml]文件，其中内容如下：</li>
</ol>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">自动部署</span></span><br><span class="line"><span class="comment"># 当有改动推送到main分支时，启动Action</span></span><br><span class="line"><span class="attr">on:</span></span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="attr">branches:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">main</span></span><br><span class="line">      <span class="comment">#2020年10月后github新建仓库默认分支改为main，注意更改</span></span><br><span class="line">  <span class="attr">release:</span></span><br><span class="line">    <span class="attr">types:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">published</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">deploy:</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">检查分支</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">actions/checkout@v2</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="attr">ref:</span> <span class="string">main</span></span><br><span class="line"></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">安装</span> <span class="string">Node</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">actions/setup-node@v1</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="attr">node-version:</span> <span class="string">&quot;16.x&quot;</span></span><br><span class="line"></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">安装</span> <span class="string">Hexo</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">          export TZ=&#x27;Asia/Shanghai&#x27;</span></span><br><span class="line"><span class="string">          npm install hexo-cli -g</span></span><br><span class="line"><span class="string"></span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">缓存</span> <span class="string">Hexo</span></span><br><span class="line">        <span class="attr">id:</span> <span class="string">cache-npm</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">actions/cache@v3</span></span><br><span class="line">        <span class="attr">env:</span></span><br><span class="line">          <span class="attr">cache-name:</span> <span class="string">cache-node-modules</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">node_modules</span></span><br><span class="line">          <span class="attr">key:</span> <span class="string">$&#123;&#123;</span> <span class="string">runner.os</span> <span class="string">&#125;&#125;-build-$&#123;&#123;</span> <span class="string">env.cache-name</span> <span class="string">&#125;&#125;-$&#123;&#123;</span> <span class="string">hashFiles(&#x27;**/package-lock.json&#x27;)</span> <span class="string">&#125;&#125;</span></span><br><span class="line">          <span class="attr">restore-keys:</span> <span class="string">|</span></span><br><span class="line"><span class="string">            $&#123;&#123; runner.os &#125;&#125;-build-$&#123;&#123; env.cache-name &#125;&#125;-</span></span><br><span class="line"><span class="string">            $&#123;&#123; runner.os &#125;&#125;-build-</span></span><br><span class="line"><span class="string">            $&#123;&#123; runner.os &#125;&#125;-</span></span><br><span class="line"><span class="string"></span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">安装依赖</span></span><br><span class="line">        <span class="attr">if:</span> <span class="string">$&#123;&#123;</span> <span class="string">steps.cache-npm.outputs.cache-hit</span> <span class="type">!=</span> <span class="string">&#x27;true&#x27;</span> <span class="string">&#125;&#125;</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">          npm install --save</span></span><br><span class="line"><span class="string"></span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">生成静态文件</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">          hexo clean</span></span><br><span class="line"><span class="string">          hexo generate</span></span><br><span class="line"><span class="string"></span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">部署到Github</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">JamesIves/github-pages-deploy-action@v4</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="attr">token:</span> <span class="string">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</span></span><br><span class="line">          <span class="attr">repository-name:</span> <span class="string">anzhiyu-a/anzhiyu-a.github.io</span></span><br><span class="line">          <span class="attr">branch:</span> <span class="string">main</span></span><br><span class="line">          <span class="attr">folder:</span> <span class="string">public</span></span><br><span class="line">          <span class="attr">commit-message:</span> <span class="string">&quot;$<span class="template-variable">&#123;&#123; github.event.head_commit.message &#125;&#125;</span> Updated By Github Actions&quot;</span></span><br></pre></td></tr></table></figure>

<p>ps:</p>
<ul>
<li>注意node.js的版本（node-version）与当前安装的版本一直</li>
<li>如果配置完成后，push后workflow报错，请参考<a href="https://isedu.top/index.php/archives/144/#menu_index_9">Github Action配置</a></li>
</ul>
<ol start="2">
<li>在博客项目路径&#x2F;themes&#x2F;butterfly中的[.git]文件夹<strong>删除</strong></li>
<li>在博客项目路径中呼出Git窗口，运行以下命令</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git init <span class="comment">#初始化</span></span><br><span class="line">git remote add origin git@github.com:[GithubUsername]/[SourceRepo].git <span class="comment">#复制刚创建好的博文内容私有仓库的git链接</span></span><br><span class="line">git checkout -b main <span class="comment"># 切换到main分支，</span></span><br><span class="line"><span class="comment">#2020年10月后github新建仓库默认分支改为main，注意更改</span></span><br><span class="line"><span class="comment"># 如果不是，后面的所有设置的分支记得保持一致</span></span><br></pre></td></tr></table></figure>

<ol start="4">
<li>运行git提交指令，将博客源码提交到github上。</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit -m <span class="string">&quot;github action update&quot;</span></span><br><span class="line">git push origin main</span><br><span class="line"><span class="comment">#2020年10月后github新建仓库默认分支改为main，注意更改</span></span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>此时，博客更新的逻辑就变为，博客项目路径中的内容变动后，会提交至私有博文内容仓库中，内容仓库变动后Github Action会将内容重新部署到github.io仓库中。</p>
<h2 id="如何使用？"><a href="#如何使用？" class="headerlink" title="如何使用？"></a>如何使用？</h2><p>想要撰写新博客时，打开博客项目路径，唤出git bash窗口，输入new指令创建md文件。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;博客名&quot;</span></span><br></pre></td></tr></table></figure>

<p>提示创建成功后，在生成的md文件中撰写文章，然后git add, git commmit, git push后，重新访问github.io页面即可看到更新内容。</p>
<h2 id="小tips"><a href="#小tips" class="headerlink" title="小tips"></a>小tips</h2><p>1.post_asset_folder<br>当博文中需要引入图片等资源时，可以将_config.yml中的post_asset_folder参数设置为true。修改配置文件后，hexo new在创建md文件的同时还会为该博文创建一个同名的文件夹，用以存放博文中所需要的资源。<strong>要注意在博文中引入资源的方式</strong>，引入方法如图所示，使用传统的“叹号中括号括号”的引用方式是无效的。</p>
<img src="/Hexo%20+%20Github%20page%20+%20Butterfly%20+%20Github%20Action%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/%E5%BC%95%E7%94%A8%E8%B5%84%E6%BA%90%E6%96%B9%E5%BC%8F.png" class="" title="引用资源方式">

<p>图片未正常显示，先尝试删除Hexo-asset-image、Hexo-image-link等插件，按照官方文档逐步重新实现图片引用，还是不能正常渲染再参考其它博文。</p>
<p>查看网页原代码发现图片引用的位置全是.com文件：<br><a href="https://www.cnblogs.com/BNTang/articles/16583413.html">图片显示bug参考</a><br><a href="https://alvincr.com/2021/01/hexo-with-github_pages/">hexo-asset-image部分</a></p>
<p><a href="https://blog.csdn.net/m0_43401436/article/details/107191688">hexo博客中插入图片失败——解决思路及个人最终解决办法</a></p>
<p><a href="https://wangwei1237.github.io/2020/02/05/handle-the-bug-of-hexo-asset-image-plugin/">解决hexo-asset-image的图片地址错误问题</a></p>
<p><a href="https://ucas-yanyang.github.io/Hexo_Notes/hexo_notes/">Notes on Hexo</a></p>
<p><a href="https://etrd.org/2017/01/23/hexo%E4%B8%AD%E5%AE%8C%E7%BE%8E%E6%8F%92%E5%85%A5%E6%9C%AC%E5%9C%B0%E5%9B%BE%E7%89%87/">hexo中完美插入本地图片</a></p>
<p>2.创建类别、标签<br>网上很多<a href="https://victoryofymk.github.io/2018/10/23/Hexo%E5%88%9B%E5%BB%BA%E5%88%86%E7%B1%BB%E9%A1%B5%E9%9D%A2/">hexo创建类别的教程</a>，笔者在最开始创建分类时陷入了一个误区，就是觉得要“先生成一个类别，然后在博文中才可以设置为这个类别”，其实不是。<br>实际上只要<strong>按照教程操作之后</strong>，你就<strong>可以任意的为你的博文添加类别了</strong>，hexo new page categories更“像是”允许作者为博文添加类别的开关。<br>3.引用历史博文<br>详见<a href="https://xfdj.github.io/2020/05/12/%E5%A6%82%E4%BD%95%E5%9C%A8Hexo%E7%9A%84%E5%8D%9A%E6%96%87%E4%B8%AD%E5%BC%95%E7%94%A8%E8%87%AA%E5%B7%B1%E7%9A%84%E6%96%87%E7%AB%A0/">如何在Hexo的博文中引用自己的文章</a>，要注意中间写的是文件名，不是博文title名。<br>4.博文分页<br>当按类别访问博文且该类别博文数量较多时，Hexo会根据博客数量进行分页，默认一页内摆放十篇博文，可以在_config.yml中的per_page属性进行修改。</p>
]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
  </entry>
  <entry>
    <title>NLP中的对抗训练</title>
    <url>/NLP%E4%B8%AD%E7%9A%84%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<p>在CV领域，我们需要通过对模型的对抗攻击和防御来增强模型的稳健型，比如在自动驾驶系统中，要防止模型因为一些随机噪声就将红灯识别为绿灯。在NLP领域，类似的对抗训练也是存在的，不过NLP中的对抗训练更多是作为一种正则化手段来提高模型的泛化能力！</p>
<h2 id="对抗样本"><a href="#对抗样本" class="headerlink" title="对抗样本"></a>对抗样本</h2><p>简单来说，它是指对于人类来说“看起来”几乎一样、但对于模型来说预测结果却完全不一样的样本。<br>理解对抗样本之后，也就不难理解各种相关概念了，比如“对抗攻击”，其实就是想办法造出更多的对抗样本，而“对抗防御”，就是想办法让模型能正确识别更多的对抗样本。所谓对抗训练，则是属于对抗防御的一种，它构造了一些对抗样本加入到原数据集中，希望增强模型对对抗样本的鲁棒性；同时，如本文开篇所提到的，在NLP中它通常还能提高模型的表现。</p>
<h2 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h2><p>1、往属于x里边注入扰动Δx，Δx的目标是让L(x+Δx,y;θ)越大越好，也就是说尽可能让现有模型的预测出错；<br>2、当然Δx也不是无约束的，它不能太大，否则达不到“看起来几乎一样”的效果，所以Δx要满足一定的约束，常规的约束是||Δx||≤ϵ，其中ϵ是一个常数；<br>3、每个样本都构造出对抗样本x+Δx之后，用(x+Δx,y)作为数据对去最小化loss来更新参数θ（梯度下降）；<br>4、反复交替执行1、2、3步。</p>
<h2 id="如何计算Δx"><a href="#如何计算Δx" class="headerlink" title="如何计算Δx"></a>如何计算Δx</h2><p>现在的问题是如何计算Δx，它的目标是增大L(x+Δ,y;θ)，而我们知道让loss减少的方法是梯度下降，那反过来，让loss增大的方法自然就是梯度上升，因此可以简单地取<br>Δx&#x3D;ϵ ∇x L(x,y;θ)<br>当然，为了防止Δx过大，通常要对∇xL(x,y;θ)做些标准化，比较常见的方式是</p>
<p>这种训练方法就是Fast Gradient Method（FGM）</p>
<h2 id="多迭代几次"><a href="#多迭代几次" class="headerlink" title="多迭代几次"></a>多迭代几次</h2><p>此外，对抗训练还有一种方法，叫做Projected Gradient Descent（PGD），其实就是通过多迭代几步来达到让L(x+Δx,y;θ)更大的Δx（如果迭代过程中模长超过了ϵ，就缩放回去，细节请参考</p>
<h2 id="NLP怎么处理"><a href="#NLP怎么处理" class="headerlink" title="NLP怎么处理"></a>NLP怎么处理</h2><p>对于CV领域的任务，上述对抗训练的流程可以顺利执行下来，因为图像可以视为普通的连续实数向量，Δx也是一个实数向量，因此x+Δx依然可以是有意义的图像。但NLP不一样，NLP的输入是文本，它本质上是one hot向量（如果还没认识到这一点，欢迎阅读《词向量与Embedding究竟是怎么回事？》），而两个不同的one hot向量，其欧氏距离恒为√2，因此对于理论上不存在什么“小扰动”。<br>对于NLP任务来说，原则上也要对Embedding层的输出进行同样的操作，Embedding层的输出shape为(b,n,d)<br>，所以也要在Embedding层的输出加上一个shape为(b,n,d)的Variable，然后进行上述步骤。但这样一来，我们需要拆解、重构模型，对使用者不够友好。<br>不过，我们可以退而求其次。Embedding层的输出是直接取自于Embedding参数矩阵的，因此我们可以直接对Embedding参数矩阵进行扰动。这样得到的对抗样本的多样性会少一些（因为不同样本的同一个token共用了相同的扰动），但仍然能起到正则化的作用，而且这样实现起来容易得多。</p>
<h2 id="实现方式"><a href="#实现方式" class="headerlink" title="实现方式"></a>实现方式</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fgsm_attack</span>(<span class="params">model, loss_fn, x, y, epsilon</span>):</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    x.requires_grad = <span class="literal">True</span></span><br><span class="line">    logits = model(x)</span><br><span class="line">    loss = loss_fn(logits, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算梯度</span></span><br><span class="line">    model.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    grad = x.grad.detach()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加扰动</span></span><br><span class="line">    x_adv = x + epsilon * torch.sign(grad)</span><br><span class="line">    x_adv = torch.clamp(x_adv, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回对抗样本</span></span><br><span class="line">    <span class="keyword">return</span> x_adv</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model = ...</span><br><span class="line">optimizer = ...</span><br><span class="line">loss_fn = ...</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> train_loader:</span><br><span class="line">        <span class="comment"># 生成对抗样本</span></span><br><span class="line">        x_adv = fgsm_attack(model, loss_fn, x, y, epsilon)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        logits = model(x_adv)</span><br><span class="line">        loss = loss_fn(logits, y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure>
<p>在上面的代码中，​fgsm_attack()函数接受一个模型、​一个损失函数、​一个输入张量、​一个标签和一个扰动大小作为输入，​并返回一个对抗样本。​该函数首先计算输入张量的损失，​并计算损失相对于输入张量的梯度。​然后，​它将梯度的符号与扰动大小相乘，​并将结果添加到输入张量中。​最后，​它将结果截断到0和1之间，​并返回对抗样本。​在训练循环中，​我们使用fgsm_attack()函数生成对抗样本，​并使用它来计算 模型的损失。​然后，​我们反向传播并更新模型的参数。​<br>需要注意的是，​FGM只是对抗训练的一种方法，​还有其他方法，​如PGD（Projected Gradient Descent）和FreeLB（Free Adversarial Training with Label Bootstrapping）。​此外，​FGM只是对抗训练的一部分，​还需要使用其他技术来提高模型的鲁棒性，​如数据增强和模型集成。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>PDFTriage-Question Answering over Long, Structured Documents</title>
    <url>/PDFTriage-Question%20Answering%20over%20Long,%20Structured%20Documents/</url>
    <content><![CDATA[<p>随着大规模对话模型的蓬勃发展，越来越多从文档中检索信息回复用户问题的需求出现。<br>在以往的RAG中，PDF类型的文档通常被看作为纯文本使用，使得模型总结回复的过程中无法获取到PDF中文本以外的内容。<br>本文主要有三点贡献：</p>
<ul>
<li>将PDF文档视为结构化对象而非纯文本；</li>
<li>release一份数据集</li>
<li>提出一种提示模型的方法</li>
</ul>
<p>PDFTriage回复用户问题包括3个步骤：</p>
<ol>
<li>生成document metadata</li>
<li>使用LLM从文档中筛选出相关内容</li>
<li>基于检索到的内容生成回复</li>
</ol>
<h2 id="生成document-metadata"><a href="#生成document-metadata" class="headerlink" title="生成document metadata"></a>生成document metadata</h2><p>使用<a href="https://developer.adobe.com/document-services/docs/overview/pdf-extract-api/gettingstarted/">Adobe Extract API</a>将PDF转换为类似HTML的树结构，可以从中获取到章、节、标题、表、图、段落等，作者将解析出的信息以JSON格式存储。</p>
<h2 id="LLM筛选相关内容"><a href="#LLM筛选相关内容" class="headerlink" title="LLM筛选相关内容"></a>LLM筛选相关内容</h2><p>作者设计了fetch_pages、fetch_Sections、fetch_table,fetch_figure和retrieve等方法。</p>
<blockquote>
<p>Function : Description<br>fetch_pages : Get the text contained in the pages listed.<br>fetch_sections : Get the text contained in the section listed.<br>fetch_figure : Get the text contained in the figure caption listed.<br>fetch_table : Get the text contained in the table caption listed.<br>retrieve : Issue a natural language query over the document, and fetch relevant chunks.<br>这些方法通过GPT的Function Calling调用，得到的结果会被写入进Prompt当中。</p>
</blockquote>
<h2 id="生成回复"><a href="#生成回复" class="headerlink" title="生成回复"></a>生成回复</h2><p>Prompt如下：</p>
<blockquote>
<p>You are an expert document question answering system. You answer questions by finding relevant content in &gt; the document and answering questions based on that content.<br>Document : (texual metadata of document)</p>
</blockquote>
<p>方法大概就是这样了。</p>
]]></content>
      <categories>
        <category>论文浅读</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>Python多线程</title>
    <url>/Python%E5%A4%9A%E7%BA%BF%E7%A8%8B/</url>
    <content><![CDATA[<h1 id="Python多线程随手记"><a href="#Python多线程随手记" class="headerlink" title="Python多线程随手记"></a>Python多线程随手记</h1><p>内容来自于<a href="https://www.bilibili.com/list/watchlater?oid=500025128&bvid=BV1bK411A7tV&spm_id_from=333.1007.top_right_bar_window_view_later.content.click&p=2">Python并发编程实战</a></p>
<h2 id="有哪些程序提速的方法？"><a href="#有哪些程序提速的方法？" class="headerlink" title="有哪些程序提速的方法？"></a>有哪些程序提速的方法？</h2><h3 id="单线程串行"><a href="#单线程串行" class="headerlink" title="单线程串行"></a>单线程串行</h3><p>普通脚本就是单线程串行。</p>
<h3 id="多线程并发"><a href="#多线程并发" class="headerlink" title="多线程并发"></a>多线程并发</h3><p>CPU和IO同时工作，对应Threading。CPU不用等带IO。</p>
<h3 id="多CPU并行（多进程）"><a href="#多CPU并行（多进程）" class="headerlink" title="多CPU并行（多进程）"></a>多CPU并行（多进程）</h3><p>由于当前PC中处理器都包含多核心，利用多个核心，多进程并行执行任务，对应MultiProcessing。利用多核CPU的能力，真正的<strong>并行</strong>执行任务。</p>
<h3 id="多机器并行"><a href="#多机器并行" class="headerlink" title="多机器并行"></a>多机器并行</h3><p>大数据时间通常使用多个PC进行任务，对应Hadoop&#x2F;Hive&#x2F;Spark等。</p>
<h3 id="Python对并发编程的支持"><a href="#Python对并发编程的支持" class="headerlink" title="Python对并发编程的支持"></a>Python对并发编程的支持</h3><ul>
<li>多线程、多进程；</li>
<li>asycio，在单线程利用CPU和IO同时执行的原理，实现函数异步执行；</li>
<li>Lock可以对资源加锁，防止冲突访问；</li>
<li>使用Queue实现不同线程&#x2F;进程之间的数据通信，实现生产者-消费者模式；</li>
<li>使用线程池Pool、进程池Pool，简化线程、进程的任务提交、等待结束、获取结果；</li>
<li>使用subprocess启动外部程序的进程，并进行输入输出交互；</li>
</ul>
<h2 id="Python并发编程的三种方式"><a href="#Python并发编程的三种方式" class="headerlink" title="Python并发编程的三种方式"></a>Python并发编程的三种方式</h2><p>线程、进程、协程之间有层级关系。一个进程中可以包含和启动很多个线程，一个线程可以启动很多个协程。</p>
<h3 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h3><h4 id="CPU密集型计算（CPU-Bound）"><a href="#CPU密集型计算（CPU-Bound）" class="headerlink" title="CPU密集型计算（CPU-Bound）"></a>CPU密集型计算（CPU-Bound）</h4><p>CPU-Bound指在执行任务过程中会受到CPU的限制，也叫做计算密集型，I&#x2F;O在很短的时间就可以完成，CPU需要大量的计算和处理，特点就是CPU占用率相当高。<br>比如：压缩解压、加密解密、正则表达式搜索。</p>
<h4 id="IO密集型计算（I-O-Bound）"><a href="#IO密集型计算（I-O-Bound）" class="headerlink" title="IO密集型计算（I&#x2F;O-Bound）"></a>IO密集型计算（I&#x2F;O-Bound）</h4><p>I&#x2F;O-Bound指在执行任务过程中会受到I&#x2F;O的限制，系统运作大部分的状况时CPU在等I&#x2F;O（硬盘&#x2F;内存）的读&#x2F;写操作，CPU占用率较低。<br>比如：文件处理程序、网络爬虫程序、读写数据库程序。</p>
<h3 id="多线程Thread"><a href="#多线程Thread" class="headerlink" title="多线程Thread"></a>多线程Thread</h3><p>多进程通常通过threading库实现，使用于I&#x2F;O密集型计算、同时运行的任务数目要求不多。</p>
<ul>
<li>优点：<br>  相比进程，更轻量、占用资源更少。</li>
<li>缺点：<br>  相比进程：多线程只能并发执行，不能利用多CPU（GIL）；<br>  相比协程：启动数目有限制，占用内存资源，有线程切换开销。</li>
</ul>
<h3 id="多进程Process"><a href="#多进程Process" class="headerlink" title="多进程Process"></a>多进程Process</h3><p>多进程通常通过multiprocessing库实现，适用于CPU密集型计算。</p>
<ul>
<li>优点：<br>  可以利用多核CPU并行计算</li>
<li>缺点：<br>  占用资源最多、可启动数目受限于处理器核心数量，比线程要少。</li>
</ul>
<h3 id="多协程Coroutine"><a href="#多协程Coroutine" class="headerlink" title="多协程Coroutine"></a>多协程Coroutine</h3><p>多协程通常通过asyncio库实现，使用I&#x2F;O密集型计算、需要超多任务运行、有现成库支持的场景。</p>
<ul>
<li>优点：<br>  内存开销最少、启动数量最多。</li>
<li>缺点：<br>  支持库有限制，很多库都不支持协程技术（requests VS aiohttp）、代码实现复杂。<br>  requests库就不支持协程，如果需要使用到多协程，只能使用aiohttp库。</li>
</ul>
<h2 id="全局解释器锁GIL"><a href="#全局解释器锁GIL" class="headerlink" title="全局解释器锁GIL"></a>全局解释器锁GIL</h2><h3 id="Python速度慢的原因"><a href="#Python速度慢的原因" class="headerlink" title="Python速度慢的原因"></a>Python速度慢的原因</h3><p>导致Python速度慢的原因有两个：</p>
<ol>
<li>执行过程边解释边执行；<br>比如C++程序在编写完成后会先编译成机器码，机器执行机器码的速度非常快，但Python执行的就是源码，需要边翻译成机器码，边执行。</li>
<li>Python是动态类型语言<br>Python中的变量可以是任意类型，可以随意的从“数字”切换到“字符串”，这导致了在执行过程中Python需要随时检查变量的类型，从而影响程序执行速度。</li>
<li>GIL<br>由于GIL的存在导致Python无法利用多核CPU并发执行程序。</li>
</ol>
<h3 id="GIL是什么"><a href="#GIL是什么" class="headerlink" title="GIL是什么"></a>GIL是什么</h3><p>全局解释器锁（Global Interpreter Lock，GIL），是计算机程序涉及语言解释器用于同步线程的一种机制，它使得任何时刻仅有一个线程在执行。即便在多核心处理器上，使用GIL的解释器也只允许同一时间执行一个线程。</p>
<h3 id="GIL存在理由"><a href="#GIL存在理由" class="headerlink" title="GIL存在理由"></a>GIL存在理由</h3><p>Python设计初期，为了规避并发问题引入GIL，后来想去除去不掉了。<br>GIL存在的目的是为了解决多线程之间数据完整性和状态同步的问题。Python中对象的管理，是使用引用计数器进行的，引用数量为0则释放对象。<br>举个栗子：<br>假设存在线程A和线程B都引用对象obj，obj.ref_num&#x3D;2，线程A和线程B都想撤销对obj的引用。<br>线程A先撤销引用，obj.ref_num&#x3D;1，此时发生多线程调度切换，切换到线程B。线程B撤销obj的引用，obj.ref_num&#x3D;0，又由于Python通过引用数管理对象，obj.ref_num&#x3D;0后内存中就删除了变量obj的相关内容，此时发生多线程调度切换，切换回A时可能就会对其他应用程序造成影响。</p>
<h3 id="如何避免GIL带来的限制"><a href="#如何避免GIL带来的限制" class="headerlink" title="如何避免GIL带来的限制"></a>如何避免GIL带来的限制</h3><ol>
<li>threading机制依然可用<br>因为在I&#x2F;O期间，线程会释放GIL，实现CPU和IO的并行，因此<strong>多线程用于IO密集型计算</strong>依然可以大幅度提升速度，但多线程用于CPU密集型计算时，只会拖慢速度。</li>
<li>使用multiprocessing实现并行计算<br>为应对GIL的问题，Python提供了multiprocessing模块，用于多计算机制实现真正的并行计算。</li>
</ol>
]]></content>
      <categories>
        <category>随手记</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python的PDF解析工具</title>
    <url>/Python%E7%9A%84PDF%E8%A7%A3%E6%9E%90%E5%B7%A5%E5%85%B7/</url>
    <content><![CDATA[<p>RAG的出现强化了业界对解析PDF文档的需求，通过解析文档得到数据来回复用户问题是目前常见的大模型落地方式之一。目前常见的PDF解析工具有</p>
<p>PDFplumber、PyPDF2、Marker、PaperMage、XPDF<br><a href="https://www.cnblogs.com/yanshw/p/17669007.html">https://www.cnblogs.com/yanshw/p/17669007.html</a></p>
<p><a href="https://blog.csdn.net/zhang_ergou/article/details/103083748">https://blog.csdn.net/zhang_ergou/article/details/103083748</a><br><a href="https://blog.csdn.net/BluerCat/article/details/107855588">https://blog.csdn.net/BluerCat/article/details/107855588</a></p>
<h2 id="PDFplumber"><a href="#PDFplumber" class="headerlink" title="PDFplumber"></a>PDFplumber</h2><p>PDFplumber是目前最常见的pdf解析工具，支持中文pdf解析，存在表格时解析效果也很好，还可以拿到bbox。缺点是无法对双栏等特殊格式进行解析。</p>
<h2 id="PyPDF2"><a href="#PyPDF2" class="headerlink" title="PyPDF2"></a>PyPDF2</h2><p>$a^2 + b^2 &#x3D; C^2$</p>
<h2 id="Marker"><a href="#Marker" class="headerlink" title="Marker"></a>Marker</h2><p>$\alpha+\beta&#x3D;\gamma$</p>
<h2 id="PaperMage"><a href="#PaperMage" class="headerlink" title="PaperMage"></a>PaperMage</h2><h2 id="XPDF"><a href="#XPDF" class="headerlink" title="XPDF"></a>XPDF</h2>]]></content>
      <categories>
        <category>常用工具</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>Retrieval-Augmented Generation for Large Language Models A Survey</title>
    <url>/Retrieval-Augmented%20Generation%20for%20Large%20Language%20Models%20A%20Survey/</url>
    <content><![CDATA[<h2 id="Abstarct"><a href="#Abstarct" class="headerlink" title="Abstarct"></a>Abstarct</h2><p> 大型语言模型（LLM）展示了强大的功能，但在实际应用中仍然面临挑战，如幻觉、知识更新缓慢以及答案缺乏透明度。检索增强生成（RAG）是指在使用LLM回答问题之前，从外部知识库中检索相关信息。RAG已被证明可以显著提高答案的准确性，减少模型幻觉，尤其是在知识密集型任务中。通过引用来源，用户可以验证答案的准确性，并增加对模型输出的信任。它还促进了知识更新和特定领域知识的引入。RAG有效地将LLM的参数化知识与非参数化的外部知识库相结合，使其成为实现大型语言模型的最重要方法之一。本文概述了LLM时代RAG的发展范式，总结了三种范式：Naive RAG、Advanced RAG和Modular RAG。然后，它提供了RAG的三个主要组件的总结和组织：检索器、生成器和增强方法，以及每个组件中的关键技术。此外，还讨论了如何评估RAG模型的有效性，介绍了RAG的两种评估方法，强调了评估的关键指标和能力，并提出了最新的自动评估框架。最后，从纵向优化、横向可扩展性以及RAG的技术堆栈和生态系统三个方面介绍了未来潜在的研究方向。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p> 大型语言模型（LLM）比我们以前在自然语言处理（NLP）中看到的任何模型都更强大。GPT系列模型[Brown et al.，2020，OpenAI，2023]、LLama系列模型[Touvron et al.，2023]，Gemini[Google，2023]和其他大型语言模型展示了令人印象深刻的语言和知识掌握能力，在多个评估基准中超过了人类基准水平[Wang et al.，2019，Hendrycks et al.，2021，Srivastava et al.，2022]。<br> 然而，大型语言模型也存在许多缺点。他们在处理特定领域或高度专业化的查询时经常捏造事实[Zhang等人，2023b]，缺乏知识[Kandpal等人，2023]。例如，当所寻求的信息超出了模型的训练数据或需要最新数据时，LLM可能无法提供准确的答案。当在现实世界的生产环境中部署生成人工智能时，这种限制带来了挑战，因为盲目使用黑盒LLM可能不够。传统上，神经网络通过微调模型来参数化知识，从而适应特定领域或专有信息。虽然这项技术产生了显著的结果，但它需要大量的计算资源，成本高昂，并且需要专门的技术专业知识，使其不太适应不断变化的信息环境。参数知识和非参数知识起着不同的作用。参数知识是通过训练LLM获得的，并存储在神经网络权重中，代表模型对训练数据的理解和概括，形成生成响应的基础。另一方面，非参数知识存在于向量数据库等外部知识源中，不直接编码到模型中，而是作为可更新的补充信息处理。非参数知识使LLM能够访问和利用最新或特定领域的信息，从而提高响应的准确性和相关性。<br> 纯参数化语言模型（LLM）将从大量语料库中获取的世界知识存储在模型的参数中。然而，这种模式也有其局限性。首先，很难从训练语料库中保留所有知识，尤其是不太常见和更具体的知识。其次，由于模型参数不能动态更新，参数知识很容易随着时间的推移而过时。最后，参数的扩展导致训练和推理的假定费用增加。为了解决纯参数化模型的局限性，语言模型可以采用半参数化方法，将非参数化语料库数据库与参数化模型相集成。这种方法被称为Retrieval Augmented Generation（RAG）。<br> 术语检索增强生成（RAG）最早由[Lewis等人，2020]引入。它将预先训练的检索器与预先训练的seq2seq模型（生成器）相结合，并进行端到端的微调，以更可解释和模块化的方式获取知识。在大型模型出现之前，RAG主要专注于端到端模型的直接优化。在检索端进行密集检索，例如使用基于向量的密集通道检索（DPR）[Carpukhin et al.，2020]，以及在生成端训练较小的模型是常见的做法。由于总体上较小的参数大小，检索器和生成器经常进行同步的端到端训练或微调[Izacard et al.，2022]。在类似LLM的ChatGPT出现后，生成语言模型成为主流，在各种语言任务中表现出令人印象深刻的性能[Bai et al.，2022，OpenAI，2023，Touvron et al.，2023；谷歌，2023]。然而，LLM仍然面临挑战，如幻觉[姚等人，2023，Bang等人，2023]、知识更新和数据相关问题。这影响了LLM的可靠性，使其在某些严重的任务场景中举步维艰，尤其是在需要获取大量知识的知识密集型任务中，如开放领域问答[Chen和Yih，2020，Reddy等人，2019，Kwiatkowski等人，2019]和常识推理[Clark等人，2019；Bisk等人，2020]。参数内的隐性知识可能是不完整和不充分的。<br> 随后的研究发现，将RAG引入大型模型的上下文学习（ICL）可以缓解上述问题，具有显著且易于实施的效果。在推理过程中，RAG动态地从外部知识源检索信息，使用检索到的数据作为参考来组织答案。这大大提高了反应的准确性和相关性，有效地解决了LLM中存在的幻觉问题。LLM出现后，这项技术迅速获得了关注，并已成为改进聊天机器人和使LLM更实用的最热门技术之一。通过将事实知识与LLM的训练参数分离，RAG巧妙地将生成模型的强大能力与检索模块的灵活性相结合，为纯参数化模型固有的知识不完整和不足问题提供了有效的解决方案。本文系统地回顾和分析了RAG的当前研究方法和未来发展路径，将其归纳为三个主要范式：Naive RAG、Advanced RAG和Modular RAG。随后，本文对三个核心组件：检索、增强和生成进行了综合总结，强调了RAG的改进方向和当前技术特征。在扩充方法部分，当前的工作分为三个方面：RAG的扩充阶段、扩充数据源和扩充过程。此外，本文还总结了RAG的评估体系、适用场景以及其他相关内容。通过本文，读者对大型模型和检索增强生成有了更全面、系统的了解。他们熟悉了知识检索增强的进化路径和关键技术，从而能够辨别不同技术的优缺点，识别适用场景，并在实践中探索当前的典型应用案例。值得注意的是，在之前的工作中，Feng等人[2023b]系统地回顾了将大型模型与知识相结合的方法、应用和未来趋势，主要关注知识编辑和检索增强方法。朱等人[2023]介绍了为大型语言模型增强检索系统的最新进展，特别关注检索系统。同时，Asai等人[2023a]专注于“什么”、“何时”、“如何”等问题，分析并阐明了基于检索的语言模型中的关键过程。与之相比，本文旨在系统地概述检索增强生成（RAG）的整个过程，并特别关注通过知识检索增强大型语言模型生成的相关研究。<br> RAG算法和模型的发展如图1所示。从时间上看，大多数与RAG相关的研究都出现在2020年之后，2022年12月ChatGPT发布时出现了一个重大转折点。自ChatGPT发布以来，自然语言处理领域的研究已进入大模型时代。Naive RAG技术很快得到了重视，导致相关研究的数量迅速增加。在强化策略方面，自RAG概念引入以来，关于训练前和监督微调阶段强化的研究一直在进行中。然而，大多数关于推理阶段强化的研究都出现在LLM时代。这主要是由于与高性能大型模型相关的高训练成本。研究人员试图通过在推理阶段包括RAG模块，以成本效益高的方式结合外部知识来增强模型生成。关于增强数据的使用，早期的RAG主要关注非结构化数据的应用，特别是在开放领域问答的背景下。随后，可供检索的知识来源范围扩大，使用高质量数据作为知识来源，有效地解决了大型模型中错误知识的内化和幻觉等问题。这包括结构化知识，知识图就是一个典型的例子。最近，人们越来越关注自检索，这涉及到挖掘LMS本身的知识以提高其性能。<br> 本文的后续章节结构如下：第二章介绍了RAG的背景。第三章介绍了RAG的主流范式。第四章分析了RAG中的检索器。第五章着重介绍了RAG中的生成器。第6章着重介绍了RAG中的增广方法。第七章介绍了RAG的评价体系。第8章展望了RAG的未来发展趋势。最后，在第九章中，我们总结了调查的主要内容。</p>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>在本章中，我们将介绍RAG的定义，以及RAG与其他模型优化技术（如微调）之间的比较。</p>
<h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>RAG的含义随着技术的发展而扩展。在大型语言模型时代，RAG的具体定义是指在回答问题或生成文本时，首先从大量文档中检索相关信息的模型。随后，它利用这些检索到的信息来生成响应或文本，从而提高预测的质量。RAG方法允许开发人员避免为每个特定任务重新训练整个大型模型。相反，他们可以附加一个知识库，为模型提供额外的信息输入，并提高其响应的准确性。RAG方法特别适用于知识密集型任务。总之，RAG系统由两个关键阶段组成：</p>
<ol>
<li>利用编码模型基于问题检索相关文档，如BM25、DPR、ColBERT和类似方法[Roberson等人，2009，Karpukhin等人，2020，Khattab和Zaharia，2020]。</li>
<li>生成阶段：使用检索到的上下文作为条件，系统生成文本。</li>
</ol>
<h3 id="RAG-vs-Fine-tuning"><a href="#RAG-vs-Fine-tuning" class="headerlink" title="RAG vs Fine-tuning"></a>RAG vs Fine-tuning</h3><p>在大型语言模型（LLM）的优化中，除了RAG，另一个重要的优化技术是微调。RAG类似于为模型提供教科书，允许它基于特定查询检索信息。这种方法适用于模型需要回答特定查询或处理特定信息检索任务的场景。然而，RAG不适合教授模型理解广泛的领域或学习新的语言、格式或风格。<br>微调类似于让学生通过广泛的学习内化知识。当模型需要复制特定的结构、样式或格式时，这种方法非常有用。微调可以提高非微调模型的性能，并使交互更加高效。它特别适合于强调基础模型中的现有知识，修改或自定义模型的输出，以及为模型提供复杂的指令。然而，微调不适合将新知识纳入模型，也不适合需要快速迭代新用例的情况。<br>微调类似于让学生通过长期学习内化知识。当模型需要复制特定的结构、样式或格式时，此方法适用。微调可以实现优于非微调模型的性能，并且交互更高效。微调特别适合于强调基础模型中的现有知识，修改或自定义模型的输出，以及用复杂的指令指导模型。然而，微调不适用于向模型添加新知识，也不适用于需要快速迭代新用例的场景。RAG和微调（FT）之间的具体比较可以在表1中说明。<br>RAG和微调并不互斥，但可以相互补充，增强模型在不同层面的能力。在某些情况下，将这两种技术相结合可以实现最佳的模型性能。使用RAG进行优化和微调的整个过程可能需要多次迭代才能获得令人满意的结果。<br>现有研究表明，与其他优化大型语言模型的方法相比，检索增强生成（RAG）具有显著优势[Shuster et al.，2021，Yasunaga et al.，2022，Wang et al.，2023c，Borgeud et al.，022]：</p>
<ul>
<li>RAG通过将答案与外部知识联系起来，减少语言模型中的幻觉问题，并使生成的回答更加准确可靠，从而提高准确性。</li>
<li>使用检索技术可以识别最新的信息。与仅依赖训练数据的传统语言模型相比，RAG保持了响应的及时性和准确性。</li>
<li>透明度是RAG的一个优势。通过引用来源，用户可以验证答案的准确性，增加对模型输出的信任。</li>
<li>RAG具有定制功能。通过对相关文本语料库进行索引，可以针对不同领域定制模型，为特定领域提供知识支持。</li>
<li>在安全和隐私管理方面，RAG凭借其在数据库中内置的角色和安全控制，可以更好地控制数据使用。相比之下，微调后的模型可能缺乏对谁可以访问哪些数据的明确管理。</li>
<li>RAG的可扩展性更强。它可以处理大规模数据集，而无需更新所有参数和创建训练集，使其更经济高效。</li>
<li>最后，RAG产生的结果更值得信赖。RAG从最新数据中选择确定性结果，而微调模型在处理动态数据时可能会出现幻觉和不准确，缺乏透明度和可信度。</li>
</ul>
<h2 id="RAG-Framework"><a href="#RAG-Framework" class="headerlink" title="RAG Framework"></a>RAG Framework</h2><p>RAG的研究范式在不断演变。本章主要介绍RAG研究范式的演变。我们将其分为三种类型：Naive RAG、Advanced RAG和Modular RAG。尽管早期的RAG具有成本效益，并且比原生LLM表现更好，但它仍然面临许多缺点。高级RAG和模块化RAG的出现旨在解决Naive RAG的具体缺陷。</p>
<h3 id="Naive-RAG"><a href="#Naive-RAG" class="headerlink" title="Naive RAG"></a>Naive RAG</h3><p>Naive RAG研究范式代表了在ChatGPT广泛采用后不久获得重视的最早方法。天真的RAG涉及传统的过程：索引、检索和生成。Naive RAG也被概括为“检索”-“阅读”框架[Ma et al.，2023a]。<br><strong>Indexing</strong><br>从源获取数据并为其建立索引的管道通常处于脱机状态。具体来说，数据索引的构建包括以下步骤：</p>
<ol>
<li>数据索引：这包括清理和提取原始数据，将不同的文件格式（如PDF、HTML、Word、Markdown等）转换为纯文本。</li>
<li>分块：这包括将加载的文本分成更小的块。这是必要的，因为语言模型通常对其可以处理的上下文数量有限制，因此有必要创建尽可能小的文本块。</li>
<li>嵌入和创建索引：这是通过语言模型将文本编码为矢量的过程。得到的向量将用于后续的检索过程，以计算向量和问题向量之间的相似性。嵌入模型需要很高的推理速度。由于需要对大量的语料库进行编码，并在用户提问时实时对问题进行编码，因此模型的参数大小不应太大。生成嵌入后，下一步是创建索引，存储原始语料库块，并以键值对的形式进行嵌入，以便在未来快速频繁地搜索。<br><strong>Retrieve</strong><br>给定用户的输入，使用与第一阶段相同的编码模型将查询转换为向量。计算问题嵌入和文档块在语料库中的嵌入之间的相似性。基于相似性水平，选择前K个文档块作为当前问题的增强上下文信息。<br><strong>Generation</strong><br>给定的问题和相关文档将合并到一个新的提示中。然后，大型语言模型的任务是根据所提供的信息回答问题。根据不同任务的需要，可以决定是允许大型模型使用其知识，还是仅基于给定信息进行回答。如果有历史对话信息，也可以合并到多轮对话的提示中。</li>
</ol>
<p><strong>Drawbacks in Naive RAG</strong><br>Naive RAG在三个领域面临主要挑战：检索质量、响应生成质量和增强过程。<br>关于检索质量，问题是多方面的。主要问题是精度低，检索集中的所有块都与查询相关，这会导致潜在的幻觉和空中空投问题。第二个问题是低召回率，当没有检索到所有相关块时，会出现这种情况，从而阻止LLM获得足够的上下文来合成答案。此外，过时的信息带来了另一个挑战，即数据冗余或过时的数据可能导致不准确的检索结果。<br>就产生反应的质量而言，问题同样多样化。幻觉是一个突出的问题，模型编造了一个上下文中不存在的答案。不相关是另一个问题，模型生成的答案无法解决查询问题。此外，毒性或偏倚，即模型产生有害或冒犯性反应，是另一个问题。</p>
<p>最后，扩增过程也面临一些挑战。至关重要的是，将检索到的段落中的上下文与当前的生成任务有效地结合起来至关重要。如果处理不当，输出可能会显得不连贯或不连贯。冗余和重复是另一个问题，特别是当多个检索到的段落包含相似的信息，导致生成步骤中的内容重复时。此外，确定多个检索到的段落对生成任务的重要性或相关性是具有挑战性的，并且扩充过程需要适当平衡每个段落的值。检索到的内容也可能来自不同的写作风格或语调，增强过程需要调和这些差异以确保输出的一致性。最后，生成模型可能过度依赖增强信息，导致输出仅重复检索到的内容，而不提供新的价值或合成信息。</p>
<h3 id="Advanced-RAG"><a href="#Advanced-RAG" class="headerlink" title="Advanced RAG"></a>Advanced RAG</h3><p>Advanced RAG针对Naive RAG的不足进行了有针对性的改进。在检索生成的质量方面，Advanced RAG结合了检索前和检索后的方法。为了解决Naive RAG遇到的索引问题，Advanced RAG通过滑动窗口、细粒度分割和元数据等方法优化了索引。同时，提出了多种优化检索过程的方法。在具体实现方面，Advanced RAG可以通过管道或端到端方式进行调整。</p>
<p><strong>Pre-Retrieval Process</strong></p>
<ul>
<li>优化数据索引<br>优化数据索引的目的是提高索引内容的质量。目前，有五种主要策略用于此目的：增加索引数据的粒度、优化索引结构、添加元数据、对齐优化和混合检索。</li>
</ul>
<ol>
<li>增强数据粒度：预索引优化的目标是提高文本的标准化、一致性，并确保事实的准确性和上下文的丰富性，以保证RAG系统的性能。文本标准化包括去除不相关的信息和特殊字符，以提高检索器的效率。就一致性而言，主要任务是消除实体和术语中的歧义，同时消除重复或冗余信息，以简化检索者的工作重点。确保事实的准确性至关重要，只要可能，就应核实每一条数据的准确性。上下文保留，以适应系统在现实世界中的交互上下文，可以通过添加另一层具有领域特定注释的上下文，再加上通过用户反馈循环的持续更新来实现。时间敏感性是重要的上下文信息，应设计机制来刷新过时的文档。总之，优化索引数据的重点应该放在清晰度、上下文和正确性上，以使系统高效可靠。以下介绍了最佳实践。</li>
<li>优化索引结构：这可以通过调整块的大小、改变索引路径和合并图结构信息来实现。调整块（从小到大）的方法包括收集尽可能多的相关上下文并将噪声最小化。在构建RAG系统时，块大小是一个关键参数。有不同的评估框架来比较单个块的大小。LlamaIndex2使用GPT4来评估保真度和相关度，LLaMA[Touvron et al.，2023]索引具有针对不同分块方法的自动评估功能。跨多个索引路径查询的方法与以前的元数据过滤和分块方法密切相关，并且可能涉及同时跨不同索引进行查询。标准索引可用于查询特定查询，或者独立索引可用于基于元数据关键字进行搜索或筛选，例如特定的“日期”索引。<br>引入图结构包括将实体转换为节点，将它们的关系转换为关系。这可以通过利用节点之间的关系来提高准确性，尤其是对于多跳问题。使用图形数据索引可以增加检索的相关性。</li>
<li>添加元数据信息：这里的重点是将引用的元数据嵌入到块中，例如用于筛选的日期和目的。添加参考文献的章节和小节等元数据也有利于改进检索。当我们将索引划分为多个块时，检索效率就成了一个问题。首先过滤元数据可以提高效率和相关性。</li>
<li>对齐优化：此策略主要解决对齐问题和文档之间的差异。对齐概念包括引入假设问题，创建适合每个文档回答的问题，并用文档嵌入（或替换）这些问题。这有助于解决文档之间的对齐问题和差异。</li>
<li>混合检索：这种策略的优势在于利用了不同检索技术的优势。智能地结合各种技术，包括基于关键字的搜索、语义搜索和矢量搜索，可以适应不同的查询类型和信息需求，确保对最相关和上下文丰富的信息进行一致检索。混合检索可以作为检索策略的有力补充，增强RAG管道的整体性能。<br><strong>Embedding</strong></li>
</ol>
<ul>
<li>微调嵌入：微调嵌入模型直接影响RAG的有效性。微调的目的是增强检索到的内容和查询之间的相关性。微调嵌入的作用类似于在生成语音之前调整耳朵，优化检索内容对生成输出的影响。通常，用于微调嵌入的方法属于在特定领域上下文中调整嵌入和优化检索步骤的类别。特别是在处理进化术语或稀有术语的专业领域，这些定制的嵌入方法可以提高检索相关性。BGE[BAAI，2023]嵌入模型是一种精细调谐和高性能嵌入模型，如BAAI 3开发的BGE大型EN。为了创建用于微调BGE模型的训练数据，首先使用像gpt-3.5-turbo这样的LLM来基于文档块来制定问题，其中问题和答案（文档块）形成用于微调过程的微调对。</li>
<li>动态嵌入：动态嵌入根据单词出现的上下文进行调整，不同于为每个单词使用单个向量的静态嵌入。例如，在像BERT这样的转换模型中，同一个单词可以根据周围的单词有不同的嵌入。有证据表明，在OpenAI的text-embeddingada-002模型4中，意外的高余弦相似性结果，尤其是在文本长度小于5个标记的情况下。理想情况下，嵌入应该包含尽可能多的上下文，以确保“健康”的结果。基于GPT等大型语言模型的原理，OpenAI的嵌入（da-02）比静态嵌入模型更复杂，可以捕捉一定级别的上下文。虽然它擅长上下文理解，但它对上下文的敏感性可能不如GPT4等最新的全尺寸语言模型。<br><strong>Post-Retrieval Process</strong><br>在从数据库中检索到有价值的上下文后，将其与查询合并以输入LLM会带来挑战。同时向LLM呈现所有相关文档可能会超过上下文窗口限制。将大量文档连接起来形成冗长的检索提示是无效的，这会引入噪声并阻碍LLM对关键信息的关注。为了解决这些问题，需要对检索到的内容进行额外的处理。</li>
<li>重新排序：重新排序将最相关的信息重新定位到提示的边缘是一个简单的想法。这一概念已在LlamaIndex、LangChain和HayStack等框架中实现[Blagojevi，2023]。例如，Diversity Ranker根据文档多样性优先排序，而LostThereMiddleRanker则交替将最佳文档放置在上下文窗口的开头和结尾。同时，为了应对解释基于向量的模拟搜索以获得语义相似性的挑战，cohereAI rerank[Cohere，2023]、bgererank5或LongLLMLingua[Jiang et al.，2023a]等方法重新计算相关文本和查询之间的语义相似性。</li>
<li>Prompt Compression Research表明，检索到的文档中的噪声会对RAG性能产生不利影响。在后处理中，重点在于压缩不相关的上下文，突出关键段落，减少整体上下文长度。选择性语境[Litman et al.，2020]和LLMLingua[Anderson et al.，2022]等方法利用小语言模型来计算即时相互信息或困惑，估计元素重要性。然而，这些方法可能会在RAG或长上下文场景中丢失关键信息。Recomp[Xu et al.，2023a]通过训练不同粒度的压缩器来解决这一问题。长上下文[Xu et al.，2023b]在处理广泛的上下文时进行分解和压缩，而“行走在记忆迷宫中”[Chen et al.，2021 3a]则设计了一个分层摘要树来增强LLM的关键信息感知。<br><strong>RAG Pipeline Optimization</strong><br>检索过程的优化旨在提高RAG系统的效率和信息质量。目前的研究主要集中在智能地结合各种搜索技术，优化检索步骤，引入认知回溯的概念，灵活应用各种查询策略，并利用嵌入相似性。这些努力共同努力实现RAG检索中上下文信息的效率和丰富性之间的平衡。</li>
<li>探索混合搜索：通过智能地混合各种技术，如基于关键字的搜索、语义搜索和矢量搜索，RAG系统可以利用每种方法的优势。这种方法使RAG系统能够适应不同的查询类型和信息需求，确保一致检索最相关和上下文丰富的信息。混合搜索是对检索策略的有力补充，增强了RAG管道的整体性能。</li>
<li>递归检索和查询引擎：RAG系统中优化检索的另一个强大方法涉及实现递归检索和复杂的查询引擎。递归检索需要在初始检索阶段获取较小的文档块，以获取关键的语义。在这个过程的后期阶段，具有更多上下文信息的较大块被提供给语言模型（LM）。这种两步检索方法有助于在效率和上下文丰富的响应之间取得平衡。</li>
<li>逐步后退提示：逐步后退提示方法[Zheng et al.，2023]与RAG过程相结合，鼓励LLM从具体实例中后退，并对基本的一般概念或原则进行推理。实验结果表明，通过引入后向提示，在各种具有挑战性的推理密集型任务中，性能显著提高，显示了其对RAG的天然适应性。检索增强步骤既可以应用于生成向后提示的答案，也可以应用于最终的问答过程。</li>
<li>子查询：在不同的场景中可以使用各种查询策略，包括使用LlamaIndex等框架提供的查询引擎、使用树查询、使用向量查询或使用最基本的块顺序查询。</li>
<li>HyDE：这种方法基于这样一种假设，即生成的答案在嵌入空间中可能比直接查询更接近。利用LLM，HyDE生成一个假设文档（答案）来响应查询，嵌入文档，并使用这种嵌入来检索与假设文档类似的真实文档。与基于查询寻求嵌入相似性相比，该方法强调从答案到答案的嵌入相似性。然而，它可能不会始终产生有利的结果，特别是在语言模型不熟悉所讨论主题的情况下，这可能会导致更容易出错的实例的生成。<br><strong>Modular RAG</strong><br>模块化RAG结构打破了传统的Naive RAG索引、检索和生成框架，在整个过程中提供了更大的多样性和灵活性。一方面，它集成了各种方法来扩展功能模块，例如在相似性检索中加入搜索模块，并在检索器中应用微调方法[Lin et al.，2023]。此外，特定的问题导致了重组的RAG模块的出现[Yu et al.，2022]和迭代方法，如[Shao et al.，2033]。模块化RAG范式正在成为RAG领域的主流，允许跨多个模块的串行管道或端到端训练方法。三种RAG范式之间的比较如图3所示。<br><strong>New Modules</strong></li>
<li>搜索模块：与Naive&#x2F;Advanced RAG中查询和语料库之间的相似性检索不同，该搜索模块针对特定场景量身定制，在过程中使用LLM生成的代码、查询语言（如SQL、Cypher）或其他自定义工具对（附加）语料库进行直接搜索。用于搜索的数据源可以包括搜索引擎、文本数据、表格数据或知识图[Wang et al.，2023c]。</li>
<li>内存模块：利用LLM本身的内存能力来指导检索，其原理包括找到与当前输入最相似的内存。Self-mem[Cheng et al.，2023b]迭代地使用检索增强生成器来创建无界内存池，将“原始问题”和“双重问题”相结合。检索增强生成模型可以使用自己的输出来增强自己，使文本在推理过程中更接近数据分布，使用模型自己的输出，而不是训练数据[Wang et al.，2022a]。</li>
<li>额外生成模块：在检索的内容中，冗余和噪声是常见的问题。额外生成模块不是直接从数据源检索，而是利用LLM生成所需的上下文[Yu et al.，2022]。与直接检索相比，LLM生成的内容更有可能包含相关信息。</li>
<li>任务适应性模块：专注于转换RAG以适应各种下游任务，UPRISE[Cheng et al.，2023a]自动从预先构建的数据池中检索给定零样本任务输入的提示，增强了任务和模型的通用性。PROMPTAGATOR[Dai et al.，2022]利用LLM作为少数镜头查询生成器，并基于生成的数据创建特定任务的检索器。PROMPTAGATOR利用LLM的泛化能力，仅举几个例子就可以创建特定任务的端到端检索器。</li>
<li>对齐模块：查询和文本之间的对齐一直是影响RAG有效性的关键问题。在模块化RAG时代，研究人员发现，在检索器中添加可训练的适配器模块可以有效缓解对齐问题。PRCA[Yang et al.，2023b]利用强化学习来训练由LLM奖励驱动的上下文适配器，该适配器位于检索器和生成器之间。它通过在标记的自回归策略中的强化学习阶段最大化奖励来优化检索到的信息。AAR[Yu et al.，2023b]提出了一种通用插件，该插件从已知源LLM中学习LM偏好，以帮助未知或未协同微调的LLM。RRR[Ma et al.，2023a]设计了一个基于强化学习的重写查询模块，以使查询与语料库中的文档对齐。</li>
<li>验证模块：在现实世界中，并不总是保证检索到的信息是可靠的。检索不相关的数据可能会导致LLM中出现幻觉。因此，可以在检索文档之后引入额外的验证模块，以评估检索到的文档与查询之间的相关性。这增强了RAG的稳健性[Yu et al.，2023a]。<br><strong>New Pattern</strong><br>模块化RAG的组织方法是灵活的，允许根据特定的问题上下文替换或重新配置RAG过程中的模块。对于由检索和生成两个模块组成的Naive RAG（在一些文献中称为读取或合成），该框架提供了适应性和丰富性。目前的研究主要探讨两种组织范式，包括模块的添加或替换，以及模块之间组织流动的调整。</li>
<li>Adding or Replacing Modules<br>添加或替换模块的策略需要维护检索读取的结构，同时引入额外的模块来增强特定功能。RRR[Ma等人，2023a]提出了重写检索-读取过程，利用LLM性能作为重写器模块强化学习的奖励。这允许重写器调整检索查询，从而提高读取器的下游任务性能。类似地，模块可以在生成读取[Yu et al.，2022]等方法中选择性地替换，其中LLM生成模块替换检索模块。背诵阅读[Sun et al.，2022]将外部检索转换为从模型权重的检索，最初让LLM记忆任务相关信息，并生成用于处理知识密集型自然语言处理任务的输出。</li>
<li>Adjusting the Flow between Modules<br>在调整模块之间的流程方面，重点是增强语言模型和检索模型之间的交互。DSP[Hattab et al.，2022]引入了演示搜索预测框架，将上下文学习系统视为一个明确的程序，而不是一个终端任务提示，以解决知识密集型任务。ITER-RETGEN[Shao et al.，2023]利用生成的内容来指导检索，在Retrieve ReadRetrieve Read流中迭代执行“检索增强的生成”和“生成增强的检索”。Self-RAG[Asai et al.，2023b]遵循决策-检索-反映-读取过程，引入了一个用于主动判断的模块。这种自适应和多样化的方法允许在模块化RAG框架内动态组织模块。</li>
</ul>
<h2 id="Retriever"><a href="#Retriever" class="headerlink" title="Retriever"></a>Retriever</h2><p>在RAG的上下文中，“R”代表检索，在从庞大的知识库中检索前k个相关文档的RAG管道中发挥作用。然而，制作一只高质量的寻回犬是一项不平凡的任务。在本章中，我们围绕三个关键问题进行讨论：1）如何获得准确的语义表示？2） 如何匹配查询和文档的语义空间？3） 如何将检索器的输出与大型语言模型的首选项对齐？</p>
<h3 id="如何获得准确的语义表示？"><a href="#如何获得准确的语义表示？" class="headerlink" title="如何获得准确的语义表示？"></a>如何获得准确的语义表示？</h3><p>在RAG中，语义空间是查询和文档映射的多维空间。当我们进行检索时，它是在语义空间内测量的。如果语义表达不准确，那么它对RAG的影响是致命的，本节将介绍两种方法来帮助我们构建准确的语义空间。<br><strong>Chunk optimization</strong><br>在处理外部文档时，第一步是分块以获得细粒度的特征。然后块被嵌入。然而，嵌入过大或过小的文本块可能不会获得良好的效果。因此，为语料库中的文档找到最佳块大小对于确保搜索结果的准确性和相关性至关重要。<br>在选择分块策略时，重要的考虑因素包括：被索引内容的特征、使用的嵌入模型及其最佳块大小、用户查询的预期长度和复杂性，以及检索结果在特定应用程序中的使用方式。例如，对于较长或较短的内容，应选择不同的分块模型。此外，不同的嵌入模型在不同的块大小下表现不同；例如，句子转换器更适合单句，而text-embedding-ada-002更适合包含256或512个标记的块。此外，用户输入问题文本的长度和复杂性，以及应用程序的特定需求，如语义搜索或问答，都会影响分块策略的选择。这可能与您选择的LLM的令牌限制直接相关，并可能需要您调整块大小。事实上，准确的查询结果是通过自适应地应用几种分块策略来实现的；没有最好的，只有最合适的。<br>目前对RAG的研究采用了多种块优化方法来提高检索效率和准确性。滑动窗口技术等技术通过多次检索聚合全局相关信息来实现分层检索。Small2big技术在搜索过程中使用小的文本块，并将较大的附属文本块提供给语言模型进行处理。摘要嵌入技术对文档摘要执行TopK检索，提供完整的文档上下文。元数据筛选技术利用文档元数据进行筛选。图形索引技术将实体和关系转换为节点和连接，显著增强了多跳问题的相关性。这些方法的融合提高了RAG的检索结果和性能。<br><strong>Fine-tuning Embedding Models</strong><br>在得到合适的块大小后，我们需要通过嵌入模型将块和查询嵌入到语义空间中，因此嵌入能否有效地表示语料库至关重要。如今，已经出现了优秀的嵌入模型，如[AUAE[AngIE，2023]，Voyage[VoyageAI，2023]、BGE[BAAI，2023]等]，它们已经在大规模语料库上进行了预训练，但当应用于特定领域时，它们可能无法准确地表示特定领域的语料库信息。此外，嵌入模型的任务特定微调对于确保模型理解与内容相关性相关的用户查询至关重要，而未微调的模型可能无法满足特定任务的需求。因此，微调嵌入模型对于下游应用至关重要。嵌入微调方法有两个基本范式.</p>
<ol>
<li>Domain Knowledge Fine-tuning<br>为了使嵌入模型正确地理解特定领域的信息，我们需要构建特定领域的数据集来微调嵌入模型。然而，微调嵌入模型与普通语言模型的不同之处主要在于所使用的数据集不同。在目前微调嵌入模型的主要方法中，使用的数据集由三部分组成，包括查询、语料库和相关文档。嵌入模型基于查询在语料库中查找相关文档，然后将查询的相关文档是否命中作为模型的度量。<br>在数据集的构建、模型的微调和评估中，这三个组成部分中的每一个都可能出现许多挑战。在LlamaIndex[Liu，2023]中，专门引入了一系列关键类和函数，用于嵌入模型的微调过程，大大简化了这一过程。通过准备一个领域知识语料库并利用它提供的方法，我们可以很容易地获得专门针对我们所需领域的嵌入模型。</li>
<li>Fine-tuning of downstream tasks<br>使嵌入模型适应下游任务同样重要。当在下游任务中使用RAG时，一些工作通过使用LLM的功能对嵌入模型进行了微调。PROMPTAGATOR[Dai et al.，2022]利用大型语言模型（LLM）作为少量查询生成器，并基于生成的数据创建特定任务的检索器，缓解了由于数据稀缺而在某些领域难以实现的监督微调问题。LLM嵌入器[Zhang et al.，2023a]使用大型语言模型输出来自多个下游任务的数据的奖励值，通过数据集的硬标记和LLM得出的软奖励，用两个不同的监督信号微调检索器。这通过领域知识注入和下游任务微调在一定程度上改进了语义表示。然而，这种方法训练的检索器对大型语言模型没有直观的帮助，因此已经做了一些工作来直接通过LLM的反馈信号来监督嵌入模型的微调。（本节将在4.4中介绍）</li>
</ol>
<h3 id="如何匹配查询和文档的语义空间？"><a href="#如何匹配查询和文档的语义空间？" class="headerlink" title="如何匹配查询和文档的语义空间？"></a>如何匹配查询和文档的语义空间？</h3><p>在RAG应用程序中，一些检索器使用相同的嵌入模型对查询和文档进行编码，而另一些检索器则使用两个模型分别对查询和单据进行编码。此外，用户的原始查询可能存在表达不佳和缺乏语义信息的问题。因此，对齐用户查询和文档的语义空间是非常必要的。本节介绍了实现这一目标的两项关键技术。<br><strong>Query Rewrite</strong><br>调整查询和文档语义的最直观方法是重写查询。如Query2Doc[Wang et al.，2023b]和ITERRETGEN[Shao et al.，2021 3]中所述，利用大型语言模型的固有能力，通过引导生成伪文档，然后将原始查询与该伪文档合并，形成新的查询。在HyDE[Gao et al.，2022]中，通过使用文本指示符来建立查询向量，使用这些指示符来生成相关但可能并不真实存在的“假设”文档，它只需要捕获相关模式。RRR[Ma et al.，2023a]引入了一种新的框架，该框架颠倒了检索和阅读的顺序，重点关注查询重写。该方法使用大型语言模型生成查询，然后使用web搜索引擎检索上下文，最后使用小型语言模型作为训练重写器来为冻结的大型语言模型提供服务。STEP-BACKPROMPING[Zheng et al.，2023]方法可以使大型语言模型进行抽象推理，提取高级概念和原理，并在此基础上进行检索。最后，多查询检索中的方法包括使用大型语言模型生成多个搜索查询，这些查询可以并行执行，检索结果一起输入，这对于依赖于多个子问题的单个问题非常有用。<br><strong>Embedding Transformation</strong><br>如果有像重写查询这样的粗粒度方法，那么也应该有专门用于嵌入操作的细粒度实现。在LlamaIndex[Liu，2023]中，可以在查询编码器之后连接适配器，并微调适配器以优化查询嵌入的表示，将其映射到更适合特定任务的潜在空间。当查询和外部文档的数据结构不同时，例如非结构化查询和结构化外部文档，使查询与文档对齐是非常重要的。SANTA[Li et al.，2023d]提出了两种预训练方法，以使检索者意识到结构化信息1）利用结构化数据和非结构化数据之间的自然对齐关系进行对比学习，进行结构化意识预训练。2） 掩码实体预测，它设计了一个面向实体的掩码策略，并要求语言模型填充掩码实体。</p>
<h3 id="如何调整检索器的输出和LLM的偏好？"><a href="#如何调整检索器的输出和LLM的偏好？" class="headerlink" title="如何调整检索器的输出和LLM的偏好？"></a>如何调整检索器的输出和LLM的偏好？</h3><p>在RAG管道中，即使我们采用上述技术来提高检索命中率，也可能无法提高RAG的最终效果，因为检索到的文档可能不是LLM所需要的。因此，本节介绍了两种方法来调整检索器的输出和LLM的偏好。<br><strong>LLM supervised training</strong><br>许多作品利用来自大型语言模型的各种反馈信号来微调嵌入模型。AAR[Yu等人，2023b]通过编码器-编码器架构LM为预先训练的检索器提供监督信号。通过FiD交叉注意力得分确定LM的首选文档，然后使用硬负采样和标准交叉熵损失对检索器进行微调。最终，微调寻回器可以直接用于增强看不见的目标LM，从而在目标任务中表现更好。检索器的训练损失为：<br>–Fomula 1–<br>其中D^(a^+)是LLM在检索集中首选的文档，Da−不是首选。l是标准的交叉熵损失。最后，有人建议LLM可能更喜欢关注可读文档，而不是信息丰富的文档。<br>REPLUG[Shi et al.，2023]使用检索器和LLM来计算检索到的文档的概率分布，然后通过计算KL散度来执行监督训练。这种简单有效的训练方法通过使用LM作为监督信号来增强检索模型的性能，消除了对任何特定交叉注意力机制的需要。寻回器的训练损失如下：<br>–Fomula 2–<br>其中，D是一组输入上下文，P_R是检索似然性，Q_(LM)是每个文档的LM似然性。<br>UPRISE[Cheng et al.，2023a]还使用冻结的大型语言模型来微调提示检索器。但语言模型和检索器都以提示输入对作为输入，然后使用大语言模型给出的分数来监督检索器的训练，相当于使用大语言模式来标记数据集。Atlas[Izacard et al.，2022]提出了四种微调监督嵌入模型的方法，其中，注意力蒸馏使用语言模型在输出过程中生成的交叉注意力分数进行提取。EMDR2采用期望最大化算法以检索到的文档作为潜在变量进行训练。困惑蒸馏直接使用模型生成的令牌的困惑作为指标进行训练。LOOP基于文档删除对LM预测的影响，引入了一种新的损失函数，为模型更好地适应特定任务提供了一种有效的训练策略。<br><strong>Plug in an adapter</strong><br>然而，由于诸如利用API来实现嵌入功能或本地计算资源不足之类的因素，微调嵌入模型可能是具有挑战性的。因此，有些作品选择外部连接适配器进行对齐。PRCA[Yang et al.，2023b]通过上下文提取阶段和奖励驱动阶段训练适配器，并基于基于令牌的自回归策略优化检索器的输出。令牌过滤[Berchansky et al.，2023]方法计算交叉注意力得分，选择得分最高的输入令牌来有效过滤令牌。RECOMP[Xu et al.，2023a]提出了抽取式和生成式压缩器，通过选择相关句子或合成文档信息来生成摘要，以实现多文档查询焦点摘要。除此之外，一种新的方法PKG[Luo et al.，2023]通过指令微调将知识注入白盒模型，并直接替换检索器模块，该模块用于基于查询直接输出相关文档。</p>
<h2 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h2><p>RAG的另一个核心组件是生成器，负责将检索到的信息转换为自然流畅的文本。它的设计灵感来自传统的语言模型，但与传统的生成模型相比，RAG的生成器通过利用检索到的信息来提高准确性和相关性。在RAG中，生成器的输入不仅包括传统的上下文信息，还包括通过检索器获得的相关文本片段。这使生成器能够更好地理解问题背后的背景，并产生信息更丰富的回答。此外，生成器以检索到的文本为指导，以确保生成的内容和检索到的信息之间的一致性。正是输入数据的多样性导致了生成阶段的一系列有针对性的工作，所有这些工作都旨在使大型模型更好地适应查询和文档中的输入数据。我们将通过检索后处理和微调等方面深入研究生成器的介绍。</p>
<h3 id="How-Can-Retrieval-Results-be-Enhanced-via-Post-retrieval-Processing"><a href="#How-Can-Retrieval-Results-be-Enhanced-via-Post-retrieval-Processing" class="headerlink" title="How Can Retrieval Results be Enhanced via Post-retrieval Processing?"></a>How Can Retrieval Results be Enhanced via Post-retrieval Processing?</h3><p>就未经编辑的大型语言模型而言，大多数研究都依赖于公认的大型语言模式，如GPT4[OpenAI，2023]，以利用其强大的内部知识来全面检索文档知识。然而，这些大型模型的固有问题，如上下文长度限制和对冗余信息的脆弱性，仍然存在。为了缓解这些问题，一些研究在检索后处理方面做出了努力。检索后处理是指对检索器从大型文档数据库中检索到的相关信息进行进一步处理、过滤或优化的过程。其主要目的是提高检索结果的质量，以更好地满足用户需求或后续任务。它可以被理解为对检索阶段获得的文档进行再处理的过程。检索后处理的操作通常包括信息压缩和结果重新存储。<br><strong>Information Compression</strong><br>尽管检索器可以从庞大的知识库中获取相关信息，但我们仍然面临着处理检索文档中大量信息的挑战。现有的一些研究试图通过增加大型语言模型的上下文长度来解决这一问题，但目前的大型语言模型仍然面临上下文限制。因此，在某些情况下，信息浓缩是必要的。简言之，信息浓缩的重要性主要体现在以下几个方面：减少噪声、应对上下文长度限制和增强生成效果。<br>PRCA[Yang等人，2023b]通过训练信息提取器来解决这个问题。在上下文提取阶段，给定输入文本Sinput，它可以生成输出序列Cextracted，该输出序列表示输入文档中的压缩上下文。训练过程的目标是尽可能减少Cextracted和实际上下文Ctruth之间的差异。他们采用的损失函数如下：<br>–Fomula 3–<br>其中f是信息提取器，θ是提取器的参数。RECOMP[Xu et al.，2023a]类似地通过利用对比学习来训练信息冷凝器。对于每个训练数据点，存在一个正样本和五个负样本。在此过程中，编码器使用对比损失进行训练[Carpukhin et al.，2020]。具体优化目标如下：<br>–Fomula 4–<br>其中，xi是训练数据，pi是正样本，nj是负样本，sim（x，y）是计算x和y之间的相似性。另一项研究选择进一步精简文档数量，旨在通过减少检索到的文档数量来提高模型的答案准确性。[Ma et al.，2023b]提出了“Filter Ranker”范式，该范式融合了大型语言模型（LLM）和小型语言模型（SLM）的优势。在这个范例中，SLM充当过滤器，而LLM充当重新排序代理。通过促使LLM重新排列SLM识别的困难样本的部分，研究结果表明，在各种信息提取（IE）任务中都有显著改进。<br><strong>Rerank</strong><br>重新排序模型的关键作用在于优化从检索器检索到的文档集。当添加额外的上下文时，LLM的性能会随着回溯性能而下降，而重新排序提供了解决此问题的有效解决方案。核心思想包括重新排列文档记录，将最相关的项目放在顶部，从而将文档总数减少到固定数量。这不仅解决了检索过程中可能遇到的上下文窗口扩展问题，而且有助于提高检索效率和响应能力[Zhuang et al.，2023]。<br>引入上下文压缩作为重新排序的一部分，旨在仅基于给定的查询上下文返回相关信息。这种方法的双重意义在于，通过减少单个文档的内容和过滤整个文档，集中显示检索结果中最相关的信息。因此，重新排序模型在整个信息检索过程中发挥着优化和细化的作用，为后续的LLM处理提供了更有效、更准确的输入。</p>
<h3 id="How-to-Optimize-a-Generator-to-Adapt-Input-Data"><a href="#How-to-Optimize-a-Generator-to-Adapt-Input-Data" class="headerlink" title="How to Optimize a Generator to Adapt Input Data?"></a>How to Optimize a Generator to Adapt Input Data?</h3><p>在RAG模型中，发电机的优化是架构的关键组成部分。生成器的任务是获取检索到的信息并生成相关文本，从而提供模型的最终输出。优化生成器的目标是确保生成的文本既自然又有效地利用检索到的文档，从而更好地满足用户的查询需求.<br>在典型的大型语言模型（LLM）生成任务中，输入通常是一个查询。在RAG中，主要区别在于输入不仅包括查询，还包括检索器检索的各种文档（结构化&#x2F;非结构化）。额外信息的引入可能会对模型的理解产生重大影响，尤其是对于较小的模型。在这种情况下，对模型进行微调以适应查询检索到的文档的输入变得尤为重要。具体来说，在向微调模型提供输入之前，通常会对检索器检索到的文档进行检索后处理。需要注意的是，RAG中微调发电机的方法基本上类似于LLM的一般微调方法。在这里，我们将简要介绍一些具有代表性的工作，包括数据（格式化&#x2F;未格式化）和优化功能。<br><strong>General Optimization Process</strong><br>指包含成对（输入，输出）的训练数据，旨在训练模型在给定输入x的情况下生成输出y的能力。在Self-mem的工作中[Cheng et al.，2023b]，采用了相对经典的训练过程。给定输入x，检索相关文档z（在论文中选择Top-1），在对（x，z）进行积分后，模型生成输出y。论文利用了两种常见的微调范式，即联合编码器[Arora等人，2023，Wang等人，2022b，Lewis等人，2020]和双编码器[Sia等人，2019，Cai等人，2021，Cheng等人，2022]。对于联合编码器，使用基于编码器-解码器的标准模型，其中编码器最初对输入进行编码，解码器通过注意力机制将编码结果组合起来，以自回归方式生成令牌：<br>–Fomula 5–<br>–Fomula 6–<br>–Fomula 7–<br>对于双编码器，系统建立了两个独立的编码器，每个编码器分别负责对输入（查询、上下文）和文档进行编码。然后，解码器按顺序对输出进行双向交叉关注处理。作者选择使用变压器[Vaswani et al.，2017]作为两种架构的构建块，并优化Gξ负对数似然（NLL）损失。<br>–Fomula 8–<br>–Fomula 9–<br>–Fomula 10–<br><strong>Utilizing Contrastive Learning</strong><br>在准备训练数据的阶段，通常生成输入和输出之间的成对交互。在这种情况下，模型只能访问唯一的真实输出，这可能会引发“暴露偏差”问题[Ranzato et al.，2015]：在训练阶段，模型只暴露于单个真实反馈，而不访问任何其他生成的令牌。这可能会损害模型在应用中的性能，因为它可能过于适合训练数据中的特定反馈，而不会有效地推广到其他场景。因此，SURGE提出了一种图文对比学习方法[Cang et al.，2023]。对于输入和输出之间的任何一对给定的交互，这种对比学习方法的目标可以定义如下：<br>–Fomula 11–<br>其中ζ，ξ是可学习的线性投影层。z是来自编码器的图的平均表示，h是解码器表示的平均值。z′，h′分别表示相应的负样本。在给定的文本中，“h”和“z”表示负样本。通过引入对比学习目标，该模型可以更好地学习生成多样化和合理的回答，而不仅仅是训练数据中的回答。这有助于降低过拟合的风险，并提高模型在真实世界场景中的泛化能力。<br>在处理涉及结构化数据的检索任务时，SANTA[Li et al.，2023d]的工作利用三阶段训练过程来充分理解结构和语义信息。具体而言，在检索器的训练阶段，采用了对比学习，主要目标是优化查询和文档的嵌入表示。具体优化目标如下：<br>–Fomula 12–<br>其中q和d是编码器编码的查询和文档。d−、d分别表示负样本和正样本。在生成器的初始训练阶段，我们利用对比学习来对齐结构化数据和非结构化数据的相应文档描述。优化目标如上所述。<br>此外，在生成器的后期训练阶段，受参考文献[Sciavolino et al.，2021，Zhang et al.，2019]的启发，我们认识到实体语义在学习检索中的文本数据表示方面的显著有效性。因此，我们首先在结构化数据中执行实体识别，随后将掩码应用于生成器的训练数据的输入部分中的实体，使生成器能够预测这些掩码。此后的优化目标是：<br>–Fomula 13–<br>其中Yd（yj表示序列Yd中的第j个令牌。Yd&#x3D;<mask>1，ent1，…，<mask>n，entn表示包含掩码实体的地面实况序列。在整个训练过程中，我们通过从上下文中获取必要的信息来恢复屏蔽的实体，理解文本数据的结构语义，并对齐结构化数据中的相关实体。我们优化了语言模型，以填充隐藏的跨度，并更好地理解实体语义[Ye et al.，2020]。</p>
<h2 id="Augmentation-in-RAG"><a href="#Augmentation-in-RAG" class="headerlink" title="Augmentation in RAG"></a>Augmentation in RAG</h2><p>本章主要分为三个维度：扩充阶段、扩充数据源和扩充过程，详细阐述RAG开发中的关键技术。RAG核心组件的分类如图4所示。</p>
<h3 id="RAG-in-Augmentation-Stages"><a href="#RAG-in-Augmentation-Stages" class="headerlink" title="RAG in Augmentation Stages"></a>RAG in Augmentation Stages</h3><p>作为一项知识密集型任务，RAG在语言模型训练的预训练、微调和推理阶段采用了不同的技术方法。<br><strong>Pre-training Stage</strong><br>自从预训练模型出现以来，研究人员一直致力于通过预训练阶段的检索方法来提高预训练语言模型在开放领域问答（QA）中的性能。在预先训练的模型中识别和扩展隐含知识可能具有挑战性。REALM[Arora等人，2023]引入了一种更模块化和可解释的知识嵌入方法。遵循掩蔽语言模型（MLM）范式，REALM将预训练和微调建模为一个先检索后预测的过程，其中语言模型通过基于掩蔽句子x预测掩蔽标记y来进行预训练，并对P（x|y）进行建模。<br>RETRO[Borgeaud等人，2022]利用检索增强对自回归语言模型进行预训练，通过从大量标记数据中检索并显著减少模型参数，实现从头开始的大规模预训练。RETRO与GPT模型共享主干结构，并引入了一个额外的RETRO编码器来对从外部知识库检索到的相邻实体的特征进行编码。此外，RETRO在其解码器转换器结构中结合了逐块交叉注意层，以有效地集成来自RETRO编码器的检索信息。RETRO实现了比标准GPT模型更低的困惑。此外，它通过更新检索数据库在更新存储在语言模型中的知识方面提供了灵活性，而无需重新训练语言模型[Petroni et al.，2019]。<br>Atla[Izacard等人，2022]采用了类似的方法，在预训练和微调阶段都采用了使用T5架构[Raffel等人，2020]的检索机制。在预训练之前，它用预训练的T5初始化编码器-解码器LM主干，并用预训练过的Contriever初始化密集检索器。在预训练过程中，它每1000步刷新一次异步索引。<br>COG[Vaze et al.，2021]是一种文本生成模型，通过从现有的文本集合中逐步复制文本片段（如单词或短语）来形式化其生成过程。与按顺序选择单词的传统文本生成模型不同，COG利用高效的矢量搜索工具来计算文本片段的有意义的上下文表示并对其进行索引。因此，文本生成任务被分解为一系列复制和粘贴操作，其中在每个时间步骤，从文本集合中寻找相关的文本片段，而不是从独立的词汇中进行选择。COG在各个方面都表现出优于RETRO的性能，包括问题回答、领域自适应和扩展短语索引。<br>另一方面，随着标度定律的发现，模型参数迅速增加，使自回归模型成为主流。研究人员还在探索是否可以使用RAG方法对更大的模型进行预训练。RETRO[Wang et al.，2023a]是RETRO的扩展，增加了模型的参数规模。研究发现，文本生成质量、事实准确性、低毒性和下游任务准确性都得到了持续的提高，尤其是在知识密集型任务（如开放领域问答）中。这些研究结果突出了预训练自回归语言模型以及检索未来基础模型的前景。<br>总之，强化预训练的优势和局限性是显而易见的。从积极的方面来看，这种方法提供了一个更强大的基础模型，在困惑、文本生成质量和下游任务性能方面优于标准GPT模型。此外，与纯预训练的模型相比，它通过使用更少的参数来实现更高的效率。它特别擅长处理知识密集型任务，允许通过对特定领域语料库的训练来创建特定领域的模型。然而，也存在缺点，包括需要大量的预训练数据和更大的训练资源，以及更新速度较慢的问题。特别是随着模型大小的增加，检索增强训练的成本变得相对较高。尽管存在这些局限性，但该方法在模型鲁棒性方面表现出显著的特点。经过训练后，基于纯预训练的检索增强模型消除了对外部库依赖性的需求，提高了生成速度和操作效率。<br><strong>Fine-tuning Stage</strong><br>在下游微调阶段，研究人员采用了各种方法来微调检索器和生成器，以改进信息检索，主要是在开放域问答任务中。关于检索器微调，REPlUG[Shi et al.，2023]将语言模型（LM）视为黑匣子，并通过可调整的检索模型对其进行增强。REPLUG通过监督信号从黑匣子语言模型中获得反馈，改进了初始检索模型。另一方面，UPRISE[Cheng et al.，2023a]通过对不同任务集进行微调，创建一个轻量级和通用的检索器，从而对检索器进行微调。该检索器可以自动为零样本任务提供检索提示，展示其在任务和模型中的通用性和改进的性能。<br>同时，微调生成器的方法包括Self-Mem[Cheng等人，2023b]，其通过示例的存储池来微调生成器，以及Self-RAG[Asai等人，2023b，其通过生成反射令牌来满足主动检索需求。RADIT[Lin et al.，2023]方法通过在给定检索增强指令的情况下最大化更正swers的概率来微调生成器和检索器。它更新生成器和检索器，以最大限度地减少文档和查询之间的语义相似性，有效地利用相关的背景知识.<br>此外，SUGRE[Cang et al.，2023]引入了对比学习的概念。它对检索器和生成器进行端到端的微调，确保高度详细的文本生成和检索的子图。SURGE使用基于图神经网络（GNN）的上下文感知子图检索器，从与正在进行的对话相对应的知识图中提取相关知识。这样可以确保生成的响应忠实地反映检索到的知识。为此，SURGE采用了一种不变但高效的图编码器和图-文本对比学习目标。<br>总之，在微调阶段的增强方法表现出几个特点。首先，微调LLM和检索器可以更好地适应特定任务，提供同时微调一个或两个的灵活性，如RePlug[Shi等人，2023]和RA-DIT[Lin等人，2023].等方法所示。其次，这种微调的好处扩展到适应不同的下游任务，如UPRISE[Cheng et al.，2023a]所示，使模型更加通用。此外，微调使模型能够更好地适应各种语料库中的不同数据结构，特别有利于图结构语料库，如SUGRE方法所强调的那样.<br>然而，这一阶段的微调也有局限性，例如需要专门为RAG微调准备的数据集，以及与推理阶段的RAG相比需要大量的计算资源。总的来说，在微调过程中，研究人员可以根据特定的需求和数据格式灵活地定制模型，与预训练阶段相比，减少了资源消耗，同时保留了调整模型输出风格的能力。<br><strong>Inference Stage</strong><br>将RAG方法与LLM相结合已成为推理阶段的一个流行研究方向。值得注意的是，Naive RAG的研究范式依赖于在推理阶段结合检索内容。<br>为了克服Naive RAG的局限性，研究人员在推理阶段在RAG中引入了更丰富的上下文。DSP[Hattab et al.，2022]框架依赖于一个复杂的管道，该管道涉及在冻结的语言模型（LM）和检索模型（RM）之间传递自然语言文本，为模型提供更多信息上下文以提高生成质量。PKG为LLM配备了一个知识引导模块，该模块允许在不改变LLM参数的情况下访问相关知识，使模型能够执行更复杂的任务。此外，CREA-ICL[Li et al.，2023b]利用跨语言知识的同步检索来帮助获取额外信息，而RECITE通过从LLM中采样一个或多个段落来形成上下文。<br>在推理阶段，优化RAG过程有利于适应更具挑战性的任务。例如，ITRG[Feng et al.，2023a]通过迭代检索和搜索正确的推理路径，增强了对需要多步推理的任务的适应性。ITERRETGEN[Shao et al.，2023]采用迭代方法将检索和生成结合起来，实现了“检索增强生成”和“生成增强检索”的交替过程.<br>另一方面，IRCOT[Trivedi et al.，2022]融合了RAG和CoT[Wei et al.，2022]的概念，采用了交替的CoT引导检索，并使用检索结果来改进CoT。这种方法显著提高了GPT-3在各种QA任务中的性能，突出了集成检索和生成的潜在优势。<br>总之，推理阶段增强方法具有重量轻、成本效益高、不需要额外训练以及利用强大的预训练模型的优点。主要优势在于在微调过程中冻结LLM的参数，专注于提供更适合需求的上下文，具有快速和低成本的特点。然而，这种方法也有一些局限性，包括需要额外的数据处理和流程优化，同时受到基础模型能力的限制。通常，这种方法通常与过程优化技术相结合，如逐步推理、迭代推理和自适应检索，以更好地满足不同任务的要求。</p>
<h3 id="Augmentation-Data-Source"><a href="#Augmentation-Data-Source" class="headerlink" title="Augmentation Data Source"></a>Augmentation Data Source</h3><p>数据来源是RAG有效性的关键因素。各种数据源提供了不同的知识粒度和维度，需要不同的处理方法。它们主要分为三类：非结构化数据、结构化数据和LLM生成的内容。<br><strong>Augmented with Unstructured Data</strong><br>非结构化数据主要包括文本数据，通常源自纯文本语料库。此外，其他文本数据可以作为检索源，例如用于大型模型微调的提示数据[Cheng等人，2023a]和跨语言数据[Li等人，2023b]。<br>就文本粒度而言，除了常见的组块（包括句子）之外，检索单元可以是标记（例如，kNN-LM[Handelwal等人，2019]）、短语（例如，NPM[Lee等人，2020]、COG[Vaze等人，2021]）和文档段落。细粒度的检索单元通常可以更好地处理罕见模式和域外场景，但会增加检索成本。<br>在单词级别，FLARE采用主动检索策略，仅当LM生成低概率单词时才进行检索。该方法包括生成用于检索相关文档的临时下一句，然后在检索到的文档的条件下重新生成下一句以预测后续语句。<br>在组块级别，RETRO使用前一个组块来检索最近的相邻组块，并将该信息与前一组块的上下文信息集成，以指导下一组块生成。RETRO通过从检索数据库中检索最近的相邻块N（Ci−1）来实现这一点，然后通过交叉关注融合前一个块（C1，…，Ci−2）的上下文信息和N的检索信息，以指导下一个块Ci的生成。为了保持因果关系，第i个块Ci的自回归生成只能使用前一个块N（Ci-1）的最近邻居，而不能使用N（Ci）。<br><strong>Augmented with Structured Data</strong><br>像知识图（KG）这样的结构化数据源逐渐被整合到RAG的范式中。经过验证的KGs可以提供更高质量的上下文，降低模型幻觉的可能性。<br>RET-LLM[Modarressi等人，2023]通过从过去的对话中提取关系三元组来构建个性化的知识图记忆，以供将来使用。SUGRE[Cang et al.，2023]使用图神经网络（GNN）嵌入从知识图中检索的相关子图，以防止模型生成与上下文无关的回复。SUGRE[Cang et al.，2023]采用了一种图编码方法，该方法将图结构反映到PTM的表示空间中，并利用图文本模式之间的多模式对比学习目标来确保检索到的事实和生成的文本之间的一致性。KnowledgeGPT[Wang et al.，2023c]以代码格式生成知识库（KB）的搜索查询，并包括预定义的KB操作函数。除了检索，KnowledgeGPT还提供了将知识存储在个性化知识库中以满足个人用户需求的功能。这些结构化数据源为RAG提供了更丰富的知识和上下文，有助于提高模型性能。<br><strong>LLM Generated Content RAG</strong><br>鉴于RAG回忆的辅助信息并不总是有效的，甚至可能产生负面影响，一些研究通过深入研究LLM的内部知识来扩展RAG的范式。这种方法利用LLM本身生成的内容进行检索，旨在提高下游任务的性能。以下概述了这一类别中值得注意的研究：<br>SKR[Wang et al.，2023d]采用了一个标记的训练集，将模型可以直接回答的问题归类为已知问题，将需要增强检索功能的问题分类为未知问题。该模型被训练来辨别一个问题是否已知，只对被识别为未知的输入应用检索增强，同时直接回答其余的输入。<br>GenRead[Yu et al.，2022]用LLM生成器代替检索器。实验结果表明，生成的上下文文档包含正确答案的情况比Naive RAG检索到的情况更普遍。生成的答案也显示出卓越的质量。作者将此归因于生成文档级上下文的任务与因果语言建模的预训练目标之间的一致性，从而更好地利用存储在模型参数中的世界知识。<br>Selfmem[Cheng et al.，2023b]迭代使用检索增强生成器来创建无边界内存池。内存选择器用于选择一个输出作为下一代的内存。此输出充当原始问题的双重问题。通过将原始问题和双重问题相结合，检索增强生成模型可以利用其自身的输出来增强自身。<br>这些不同的方法展示了RAG检索增强的创新策略，旨在提高模型的性能和有效性。</p>
]]></content>
      <categories>
        <category>论文浅读</category>
      </categories>
      <tags>
        <tag>RAG</tag>
      </tags>
  </entry>
  <entry>
    <title>Sigmoid与Softmax</title>
    <url>/Sigmoid%E4%B8%8ESoftmax/</url>
    <content><![CDATA[<h2 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h2><p>Sigmoid与Softmax是分类任务中的常用激活函数，用于将模型的输出值转换到(0,1)之间的概率。通常情况下，Sigmoid用于二分类任务中，Softmax用于多分类任务中。<br>$$\delta (t) &#x3D; \frac{1}{1+e^(-t)}$$</p>
<h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h2><p>很多场景中需要我们找出数组所有元素中值最大的元素，实质上都是求的hardmax。hardmax最大的特点就是只选出其中一个最大的值，即非黑即白。但是往往在实际中这种方式是不合情理的，比如对于文本分类来说，一篇文章或多或少包含着各种主题信息，我们更期望得到文章对于每个可能的文本类别的概率值（置信度），可以简单理解成属于对应类别的可信度。所以此时用到了soft的概念，Softmax的含义就在于不再唯一的确定某一个最大值，而是为每个输出分类的结果都赋予一个概率值，表示属于每个类别的可能性。</p>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>Softmax的计算过程中引入了指数函数，目的是为了将输出的数值拉开距离，其效果如以下所示。<br>$$\delta (t) &#x3D; \frac{e^t}{\sum^n_1 e^(n)}$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.tensor([<span class="number">2.</span>,<span class="number">3.</span>,<span class="number">5.</span>])</span><br><span class="line">a = a / torch.<span class="built_in">sum</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a) <span class="comment"># tensor([0.2000, 0.3000, 0.5000])</span></span><br><span class="line"></span><br><span class="line">b = torch.tensor([<span class="number">2.</span>,<span class="number">3.</span>,<span class="number">5.</span>])</span><br><span class="line">b = torch.softmax(b,dim=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(b)<span class="comment"># tensor([0.0420, 0.1142, 0.8438])</span></span><br></pre></td></tr></table></figure>
<p>可以发现，经过使用指数形式的Softmax函数能够将差距大的数值距离拉的更大。在深度学习中通常使用反向传播求解梯度进而使用梯度下降进行参数更新的过程，而指数函数在求导的时候比较方便。<br>此外，将每个类别的输出转化为概率，可以使得每个类别都有一定的概率被预测到，从而解决类别不平衡问题。（猜测：有一些样本较少的类别，可以通过softmax“快速”提高自身被识别出的概率，即使样本数量较少，也能有一个比较好的拟合效果）</p>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>当使用Softmax函数作为输出节点的激活函数的时候，一般使用交叉熵作为损失函数。由于Softmax函数的数值计算过程中，很容易因为输出节点的输出值比较大而发生数值溢出的现象，在计算交叉熵的时候也可能会出现数值溢出的问题。<br>​在PyTorch中，​可以​使用torch.max函数来获取每个输入值的最大值，​然后将其减去每个输入值，​以避免softmax的数值溢出。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>argsparse</title>
    <url>/argsparse/</url>
    <content><![CDATA[<h1 id="参数配置库-argsparse"><a href="#参数配置库-argsparse" class="headerlink" title="参数配置库-argsparse"></a>参数配置库-argsparse</h1><p>argsparse是python的命令行解析的标准模块，内置于python，不需要安装。这个库可以让我们直接在命令行中就可以向程序中传入参数并让程序运行。</p>
<h2 id="引用及使用"><a href="#引用及使用" class="headerlink" title="引用及使用"></a>引用及使用</h2><p>声明ArgumentParser对象后，可以使用add_argument方法在parser中添加参数。type用于设置参数的数据类型，default为默认值，help为参数出错时的提示。还可以为参数设置required为True或者False，代表程序执行时必须设定该参数的值，否则会报错。其他的参数设置可以参考<a href="https://docs.python.org/zh-cn/3/library/argparse.html">命令行选项、参数和子命令解析器</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&quot;--task&quot;</span>,<span class="built_in">type</span>=<span class="built_in">str</span>,default=<span class="string">&quot;nothing&quot;</span>,<span class="built_in">help</span>=<span class="string">&quot;执行任务名称&quot;</span>)</span><br><span class="line">args = parser.parse_args(args=[])</span><br><span class="line"><span class="built_in">print</span>(args.task) <span class="comment"># nothing</span></span><br></pre></td></tr></table></figure>
<p>其中”–task”表示task为可选参数，如果没有”–”表示为必选参数，即required，如果运行程序时未指定参数值，即便设置了default也仍然会报错。</p>
<h2 id="使用json生成config文件"><a href="#使用json生成config文件" class="headerlink" title="使用json生成config文件"></a>使用json生成config文件</h2><p>训练模型过程中，通常会涉及到多模型的性能比较，不同模型的参数设置往往都是不相同的，使用上述的配置参数方式可能便捷性较差。我们可以将所有的参数分成两部分，一部分是不怎么会调整的参数，直接使用add_argument方法定义，另一部分是模型中可能变动、可能需要调整的参数，将这些参数写成一个json。这样，不同模型调用不同json中的设置，以降低使用不同模型进行试验时调整参数的复杂度。<br>对于类似输出路径、预训练模型路径等不怎么需要变更的参数使用add_argument方法设定，对于模型学习率等参数，我们记录在json当中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--config&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;./config/something.json&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--save_path&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;./outputs&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--bert_name&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">r&quot;E:\MyPython\Pre-train-Model\mc-bert-base&quot;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--device&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">args = parser.parse_args(args=[])</span><br></pre></td></tr></table></figure>
<p>我们已经在args中设定了config的路径，使用json读取这个路径就可以得到剩余的参数。我们可以声明一个config类，用来存json当中的每一项参数，然后将两部分参数整合在一起就是所有参数了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Config</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, args</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用json读取参数</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(args.config, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            config = json.load(f)</span><br><span class="line">        <span class="comment"># 设置需要调整的参数</span></span><br><span class="line">        self.loss_type = config[<span class="string">&quot;loss_type&quot;</span>]</span><br><span class="line">        self.learning_rate = config[<span class="string">&quot;learning_rate&quot;</span>]</span><br><span class="line">        self.bert_learning_rate = config[<span class="string">&quot;bert_learning_rate&quot;</span>]</span><br><span class="line">        self.weight_decay = config[<span class="string">&quot;weight_decay&quot;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将两部分参数整合在一起</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> args.__dict__.items():</span><br><span class="line">            <span class="keyword">if</span> v <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.__dict__[k] = v</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;&#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.__dict__.items())</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这样我们就可以在程序中实例化config对象，通过访问对象属性的方式来获取预先设定好的参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">config = Config(args)</span><br><span class="line"><span class="built_in">print</span>(config.learning_rate)</span><br></pre></td></tr></table></figure>

<h2 id="题外话"><a href="#题外话" class="headerlink" title="题外话"></a>题外话</h2><p>通常情况下，使用add_augment定义的往往都是单一的参数，实际上在add_augment方法中可以使用nargs关键字来获取多个参数。举个栗子叭：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&quot;--files&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=[<span class="string">r&quot;C:/Users/.ssh/id_rsa&quot;</span>,<span class="string">r&quot;C:/Users/.ssh/id_rsa.pub&quot;</span>],nargs=<span class="string">&#x27;+&#x27;</span>)</span><br><span class="line">args = parser.parse_args(args=[])</span><br></pre></td></tr></table></figure>
<p>此时我们单独打印args.files可以得到:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">args.files</span><br><span class="line"><span class="comment"># [&#x27;C:/Users/.ssh/id_rsa&#x27;, &#x27;C:/Users/.ssh/id_rsa.pub&#x27;]</span></span><br></pre></td></tr></table></figure>
<p>可以通过遍历得到每一个传入的路径：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> args.files:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line"><span class="comment"># C:/Users/.ssh/id_rsa</span></span><br><span class="line"><span class="comment"># C:/Users/.ssh/id_rsa.pub</span></span><br></pre></td></tr></table></figure>


<h1 id="huggingface的参数配置-HFArgumentParser"><a href="#huggingface的参数配置-HFArgumentParser" class="headerlink" title="huggingface的参数配置-HFArgumentParser"></a>huggingface的参数配置-HFArgumentParser</h1><p>本节内容来源于<a href="https://zhuanlan.zhihu.com/p/296535876">transformer.HfArgumentParser的使用</a><br>HfArgumentParser是Transformer框架中的命令行解析工具，它是ArgumentParser的子类。用于从类对象中创建解析对象。<br>在python中，我们习惯于将有关联的一些参数放在一个类当中，HfArgumentParser可以将类对象中的实例属性转换成转换为解析参数。需要注意的是<strong>这里的类对象必须是通过@dataclass()创建的类对象</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> HfArgumentParser</span><br><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass,field</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span></span><br><span class="line"><span class="meta">@dataclass()</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicSetting</span>():</span><br><span class="line">    <span class="comment"># a:str = field(default=&quot;bagging&quot;)</span></span><br><span class="line">    model_path : <span class="built_in">str</span> = field(default=<span class="string">r&quot;E:\MyPython\Pre-train-Model\bert-base-chinese&quot;</span>)</span><br><span class="line"><span class="meta">@dataclass()</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HyperParameters</span>():</span><br><span class="line">    bert_learning_rate:<span class="built_in">float</span> = field(</span><br><span class="line">    default=<span class="number">3e-5</span></span><br><span class="line">    )</span><br><span class="line">parser = HfArgumentParser((BasicSetting,HyperParameters))</span><br><span class="line">basic,hyper = parser.parse_args_into_dataclasses()</span><br><span class="line"><span class="built_in">print</span>(basic.model_path) <span class="comment"># E:\MyPython\Pre-train-Model\bert-base-chinese</span></span><br><span class="line"><span class="built_in">print</span>(hyper.bert_learning_rate) <span class="comment"># 3e-05</span></span><br></pre></td></tr></table></figure>
<h2 id="dataclass"><a href="#dataclass" class="headerlink" title="dataclass"></a>dataclass</h2><p>dataclass是Python3.7 开始引入的一个新功能, dataclass提供了开箱即用的方法来创建自定义数据, 可以直接实例化、打印和比较数据类实例。<br>dataclass 可以认为是提供了一个简写__init__方法的语法糖。类型注释是必填项 (不限制数据类型时, 添加typing.Any为类型注释), 默认值的传递方式和__init__方法的参数格式一致. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass</span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HyperParameter</span>:</span><br><span class="line">    bert_learning_rate: <span class="built_in">float</span></span><br><span class="line">    learning_rate: <span class="built_in">float</span></span><br><span class="line"><span class="built_in">print</span>(HyperParameter(<span class="number">5e-5</span>,<span class="number">1e-3</span>))  <span class="comment"># HyperParameter(bert_learning_rate=5e-05, learning_rate=0.001)</span></span><br></pre></td></tr></table></figure>
<h2 id="field"><a href="#field" class="headerlink" title="field"></a>field</h2><p>当我们尝试使用可变的数据类型, 给dataclass做默认值时, 可能会导致多个实例公用一个数据从而引发bug。dataclass 默认阻止使用可变数据做默认值，此时需要使用field中的default_factory。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HyperParameter</span>:</span><br><span class="line">    bert_learning_rate: <span class="built_in">float</span></span><br><span class="line">    learning_rate: <span class="built_in">float</span></span><br><span class="line">hp = HyperParameter(<span class="number">5e-5</span>,<span class="number">1e-3</span>)</span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HyperParameter</span>:</span><br><span class="line">    paramters : <span class="type">List</span>[HyperParameter] = [hp] </span><br><span class="line">    <span class="comment"># mutable default &lt;class &#x27;list&#x27;&gt; for field paramters is not allowed: use default_factory</span></span><br></pre></td></tr></table></figure>
<p>此时就需要使用field来完成相应实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass,field</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HyperParameter</span>:</span><br><span class="line">    bert_learning_rate: <span class="built_in">float</span></span><br><span class="line">    learning_rate: <span class="built_in">float</span></span><br><span class="line">hp = HyperParameter(<span class="number">5e-5</span>,<span class="number">1e-3</span>)</span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HyperParameter</span>:</span><br><span class="line">    paramters : <span class="type">List</span>[HyperParameter] = field(default_factory=<span class="keyword">lambda</span>:[hp])</span><br><span class="line">_hp = HyperParameter()</span><br><span class="line"><span class="built_in">print</span>(_hp)  <span class="comment"># HyperParameter(paramters=[HyperParameter(bert_learning_rate=5e-05, learning_rate=0.001)])</span></span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>step-into-transformers</title>
    <url>/step-into-transformers/</url>
    <content><![CDATA[<h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><p>Transformer与传统NLP特征提取类模型的区别主要在以下两点：</p>
<ul>
<li>Transformer是一个纯基于注意力机制的结构，并将自注意力机制和多头注意力机制的概念运用到模型中；</li>
<li>由于缺少RNN模型的时序性，Transformer引入了位置编码，在数据上而非模型中添加位置信息；<br>以上的处理带来了几个优点：</li>
<li>更容易并行化，训练更加高效；</li>
<li>在处理长序列的任务中表现优秀，可以快速捕捉长距离中的关联信息。<br>计算注意力需要参考三个因素：Query、Key和Value</li>
<li>Query:任务内容</li>
<li>Key：索引&#x2F;标签</li>
<li>Value: 答案<br>通过Query和Key进行点积计算得到相似度，同时避免Query和Key本身的“大小”影响到相似度的计算，所以需要除以\sqrt d_k<br>$$Attention Score(Q,K) &#x3D; \frac{QK^T}{\sqrt d_k}$$<br>将相似度限制在0到1之间，并将其作用在value上。<br>$$Attention(Q,K,V) &#x3D; softmax(\frac{QK^T}{\sqrt {d_k}})V$$ <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductAttention</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,dropout=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ScaledDotProductAttention,self).__init__()</span><br><span class="line">        self.dropout = torch.nn.Dropout(<span class="number">1</span>-dropout)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,query,key,value,attention_mask = <span class="literal">None</span></span>):</span><br><span class="line">        embedding_size = query.size(-<span class="number">1</span>)</span><br><span class="line">        scaling_factor = torch.sqrt(torch.tensor(embedding_size))</span><br><span class="line">        energy = torch.matmul(query,key.transpose(<span class="number">2</span>,<span class="number">1</span>)) / scaling_factor</span><br><span class="line">        <span class="keyword">if</span> attention_mask:</span><br><span class="line">            energy = energy.masked_fill(mask==<span class="number">0</span>,<span class="built_in">float</span>(<span class="string">&quot;-1e20&quot;</span>))</span><br><span class="line">        attention = torch.softmax(energy,dim=-<span class="number">1</span>)</span><br><span class="line">        attention = self.dropout(attention)</span><br><span class="line">        outputs = torch.matmul(attention,value)</span><br><span class="line">        <span class="keyword">return</span> (outputs,attention)  </span><br></pre></td></tr></table></figure>
上述实现中涉及到attention_mask，主要原因是在处理数据过程中为方便模型进行矩阵运算，将不同语料padding到相同长度，而padding部分是与任务无关的，所以在计算注意力分数时这些位置都要变为0。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attention_mask</span>(<span class="params">query_input_ids,key_input_ids,pad_idx</span>):</span><br><span class="line">    query_lens = query_input_ids.size(<span class="number">0</span>)</span><br><span class="line">    key_lens = key_input_ids.size(<span class="number">0</span>)</span><br><span class="line">    query_mask = query_input_ids.eq(<span class="number">0</span>).unsqueeze(-<span class="number">1</span>).expand(query_input_ids.size(<span class="number">0</span>),key_lens)</span><br><span class="line">    key_mask = key_input_ids.eq(<span class="number">0</span>).unsqueeze(-<span class="number">1</span>).expand(key_input_ids.size(<span class="number">0</span>),query_lens)</span><br><span class="line">    key_mask = key_mask.transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    mask = torch.add(query_mask,key_mask)</span><br><span class="line">    <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Transformer结构"><a href="#Transformer结构" class="headerlink" title="Transformer结构"></a>Transformer结构</h2><h2 id="通过Transformer实现文本机器翻译"><a href="#通过Transformer实现文本机器翻译" class="headerlink" title="通过Transformer实现文本机器翻译"></a>通过Transformer实现文本机器翻译</h2>]]></content>
  </entry>
  <entry>
    <title>为什么生成模型中需要输入BOS和EOS等特殊标志</title>
    <url>/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%AD%E9%9C%80%E8%A6%81%E8%BE%93%E5%85%A5BOS%E5%92%8CEOS%E7%AD%89%E7%89%B9%E6%AE%8A%E6%A0%87%E5%BF%97/</url>
    <content><![CDATA[<h2 id="为什么加BOS-EOS"><a href="#为什么加BOS-EOS" class="headerlink" title="为什么加BOS&#x2F;EOS"></a>为什么加BOS&#x2F;EOS</h2><p>在RNN时代，对于Seq2Seq模型，需要对数据添加开始标志[BOS]和结束标志[EOS]，这样做的目的是在解码阶段模型进行自回归计算时，模型可以收到一个结束标志[EOS]，同时，在开始解码时，通过输入开始标记[BOS]来确保模型看不到第一个真实的词，这种处理数据的方式被沿用到现在。<br>|原始文本序列|你是谁？|<br>|处理后新序列|[BOS]你是谁？[EOS]|</p>
<h2 id="Transformers库提供的预训练模型"><a href="#Transformers库提供的预训练模型" class="headerlink" title="Transformers库提供的预训练模型"></a>Transformers库提供的预训练模型</h2><p>以Bart模型为例，源码的forward中有这样的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> labels <span class="keyword">in</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">if</span> decoder_input_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        decoder_input_ids = shift_tokens_right(</span><br><span class="line">            labels,self.config.pad_token_ids,self.config.decoder_start_token_id</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>查看shift_tokens_right的源码，其实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">shift_tokens_right</span>(<span class="params">input_ids:torch.Tensor,pad_token_id:<span class="built_in">int</span>,decoder_start_token_id:<span class="built_in">int</span></span>):</span><br><span class="line">    shifted_input_ids = input_ids.new_zeros(input_ids.shape)</span><br><span class="line">    shifted_input_ids[:,<span class="number">1</span>:] = input_ids[:,:-<span class="number">1</span>].clone()</span><br><span class="line">    shifted_input_ids[:,<span class="number">0</span>] = decoder_start_token_id</span><br><span class="line">    <span class="keyword">assert</span> pad_token_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">    shifted_input_ids.masked_fill_(shifted_input_ids == -<span class="number">100</span>, pad_token_id)</span><br></pre></td></tr></table></figure>
<p>可以发现模型已经将decoder_start_token_id添加到数据当中了。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>121.买卖股票的最佳时机</title>
    <url>/%E4%B9%B0%E5%8D%96%E8%82%A1%E7%A5%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E6%97%B6%E6%9C%BA/</url>
    <content><![CDATA[<p>给定一个数组 prices ，它的第 i 个元素 prices[i] 表示一支给定股票第 i 天的价格。</p>
<p>你只能选择 某一天 买入这只股票，并选择在 未来的某一个不同的日子 卖出该股票。设计一个算法来计算你所能获取的最大利润。</p>
<p>返回你可以从这笔交易中获取的最大利润。如果你不能获取任何利润，返回 0 。</p>
<p>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock">https://leetcode.cn/problems/best-time-to-buy-and-sell-stock</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxProfit</span>(<span class="params">self, prices: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        min_price,max_profit = <span class="built_in">int</span>(<span class="number">1e9</span>), <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> prices:</span><br><span class="line">            min_price = <span class="built_in">min</span>(min_price,i)</span><br><span class="line">            max_profit = <span class="built_in">max</span>(max_profit,i - min_price)</span><br><span class="line">        <span class="keyword">return</span> max_profit</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode题录</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>什么是Bert</title>
    <url>/%E4%BB%80%E4%B9%88%E6%98%AFBert/</url>
    <content><![CDATA[<p>现如今，Bert已成为深度学习领域内耳熟能详的预训练语言模型。预训练语言模型根据训练方式可以分为自编码式语言模型和自回归式语言模型。在介绍他们两个之前先对语言模型进行介绍。</p>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p>语言模型是NLP领域中的基本任务，其任务目标为对文本序列中某个位置的词进行预测。传统语言模型就是从左至右计算每个词出现的概率，在预测时模型依靠其上文信息进行预测，通常使用循环神经网络来实现。计算过程中，依次对文本序列中的向量进行计算。</p>
<h3 id="自回归语言模型"><a href="#自回归语言模型" class="headerlink" title="自回归语言模型"></a>自回归语言模型</h3><p>基于自回归的预训练语言模型对文本序列的建模方式与传统语言模型相同，都是根据上文信息或者下文信息预测文本序列中的词，但由于模型只能获取到单向的语义信息，在完成完形填空等涉及到上下文信息的任务时，自回归预训练语言模型表现欠佳。后来出现了ELMo，其采用两个LSTM分别获取文本序列中的顺序和逆序的文本信息，将两者拼接后作为文本的向量表达。</p>
<h3 id="自编码语言模型"><a href="#自编码语言模型" class="headerlink" title="自编码语言模型"></a>自编码语言模型</h3><p>除了自回归的方式之外，还有其他对文本序列中的字进行预测的方法。自编码模型采用随机遮盖住文本序列中部分字，在预测过程中，根据上下文来预测被遮蔽的词。与自回归的方式不同，模型在预测时可以自然而然地获取到遮蔽词的上下文信息，但这种方式也存在缺点。比如在实际下游任务当中，并不会出现遮蔽词，因此会导致预训练和下游任务中存在gap的问题。</p>
<h2 id="Bert的预训练任务"><a href="#Bert的预训练任务" class="headerlink" title="Bert的预训练任务"></a>Bert的预训练任务</h2><p>Bert的预训练任务中包含两个任务，一个是遮蔽语言模型（Masked Language Model，MLM），还有一个是下一句预测（Next Sentence Prediction，NSP）。</p>
<h3 id="MLM"><a href="#MLM" class="headerlink" title="MLM"></a>MLM</h3><p>在MLM中，随机遮蔽句子中15%的词，让模型对这些遮蔽词进行预测。在大规模预训练语料下，15%也是一个不小的规模，为了防止某些词在下游任务中从未见过，作者做了如下处理：</p>
<ul>
<li>80%的概率使用“[MASK]”替换原词</li>
<li>10%的概率被替换为其他词</li>
<li>10%的概率原封不动<br>使用随机词的原因是避免Transformer记住[MASK]是什么，至于随机词带来的负面影响，所有遮蔽词外的词共享15% * 10% &#x3D; 1.5%的概率，影响是可以忽略不记的。这样使得Transformer既可以获得全部信息，也不至于让模型获得全量信息。</li>
</ul>
<h3 id="NSP"><a href="#NSP" class="headerlink" title="NSP"></a>NSP</h3><p>选择一些句子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中的相关性，添加这样的预训练的目的是目前很多NLP的任务比如QA和NLI都需要理解两个句子之间的关系，从而能让预训练的模型更好的适应这样的任务。</p>
<p>我们首先拿到属于上下文的一对句子，也就是两个句子，之后我们要在这两个句子中加一些特殊的 token：[CLS]上一句话[SEP]下一句话[SEP]。也就是在句子开头加一个 [CLS]，在两句话之间和句末加 [SEP]，</p>
<p>Token Embedding：就是正常的词向量，即 PyTorch 中的 nn.Embedding()<br>Segment Embedding：用 embedding 的信息让模型分开上下句，我们给上句的 token 全 0，下句的 token 全 1，让模型得以判断上下句的起止位置<br>Position Embedding ：Transformer 中的不一样，不是三角函数，而是学习出来的。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>论文浅读-任务型对话系统中的自然语言生成研究进展综述</title>
    <url>/%E4%BB%BB%E5%8A%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<h1 id="任务型对话系统中的自然语言生成研究进展综述"><a href="#任务型对话系统中的自然语言生成研究进展综述" class="headerlink" title="任务型对话系统中的自然语言生成研究进展综述"></a>任务型对话系统中的自然语言生成研究进展综述</h1><p>覃立波，黎州扬，娄杰铭，禹棋赢，车万翔<br>摘 要：任务型对话系统中的自然语言生成模块（ToDNLG）旨在将系统的对话动作转换为自然语言回复，其受到研究者的广泛关注。随着深度神经网络的发展和预训练语言模型的爆发，ToDNLG 的研究已经获得了重大突破。然而，目前仍然缺乏对现有方法和最新趋势的全面调研。为了填补这个空白，该文全面调研了 ToDNLG 的最新进展和前沿领域，包括：（1）系统性回顾：回顾和总结了 ToDNLG 近10年的发展脉络和方法，包括非神经网络时代和基于深度学习的 ToDNLG 工作；（2）前沿与挑战：总结了复杂 ToDNLG 等一些新兴领域及其相应的挑战；（3）丰富的开源资源：该文在一个公共网站上收集、整理了相关的论文、基线代码和排行榜，供 ToDNLG的研究人员直接了解最新进展，希望该文的调研工作能够促进 ToDNLG领域的研究工作。<br>关键词：任务型对话系统；自然语言生成模块；预训练模型</p>
<p>任务型对话系统主要包括四个模块：自然语言理解（NLU）、对话状态追踪（DST）、策略学习、和自然语言生成（NLG）。文章中主要对NLG中相关内容进行概述。<br>对话系统中NLG通常有两类方法：传统方法和基于深度学习的方法。传统方法包括：基于模板的方法；基于句子规划的方法；基于类的方法；基于短语的方法等，本文主要对深度学习的方法进行整理。</p>
<h2 id="基于深度学习的方法"><a href="#基于深度学习的方法" class="headerlink" title="基于深度学习的方法"></a>基于深度学习的方法</h2><p>包括：基于解码器的方法；基于seq2seq的方法以及基于Transformer的方法。<br>基于解码器的方法首次开启了ToDNLG深度学习时代，基于序列到序列的方法首次借鉴了机器翻译领域的相关技术来提高性能，而基于Transformer的方法很好地解决了之前自回归方法因无法并行而效率低下的问题，以及模型上界不高的缺点。</p>
<img src="/%E4%BB%BB%E5%8A%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95%E7%BB%BC%E8%BF%B0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%BB%93%E6%9E%84%E5%9B%BE.png" class="" title="深度学习方法结构图">
<h3 id="基于解码器的方法"><a href="#基于解码器的方法" class="headerlink" title="基于解码器的方法"></a>基于解码器的方法</h3><p>输入独热编码，直接使用RNN构造的解码器来生成回复。这类方法通常分为语言生成模块和重排序模块。</p>
<ul>
<li>语言生成模块的输入为语义表示，输出是多个可能的去词化语句。</li>
<li>重排序模块的任务是将生成的多个去词化语句进行打分并排序，选出最好的语句并输出语句。<br>tips:<br>去词化（delexicalisation）：一种自然语言生成技术，​​将句子中的某些词汇替换为通用的占位符，​​以减少生成模型的复杂度和提高生成效率。​​在任务型对话系统中，​​去词化可以用于生成多个可能的语句，​​以便在重排序模块中选择最佳的语句。​​在知识图谱问答系统中，​​去词化可以用于生成查询图，​​以便在图数据库中查询答案。​​在汉语中，​​去词化也可以用于将双音复合单位范畴化，​​以便更好地理解和使用这些词汇。<br>重排序：推荐系统中的一个核心模块，​其目的是给用户一个“排好序的”item list。​推荐系统的架构大致分为召回、​粗排、​精排、​重排四个模块，​其中重排离最终的用户展现最近，​所以也十分关键。​重排序模块可以使用注意力模型等技术，​对精排模块输出的候选集进行重新排序，​以提高推荐系统的准确性和效率12。​重排序模块的优化目标或损失函数可以从多个角度来考虑，​例如全局视野的优化指标3。​​</li>
</ul>
<h3 id="基于Seq2Seq的方法"><a href="#基于Seq2Seq的方法" class="headerlink" title="基于Seq2Seq的方法"></a>基于Seq2Seq的方法</h3><p>该类方法在编码器端，对于输入的 MR,DA使用独热编码表示，而对于每个槽值对，对它的槽位和槽值分开进行编码操作，然后将它们的表示相加得到这个槽值对的表示。在解码时，采用自回归的方式，引入注意力机制，生成回复。</p>
<h3 id="基于Transformer的方法"><a href="#基于Transformer的方法" class="headerlink" title="基于Transformer的方法"></a>基于Transformer的方法</h3><p>就是用Transformer来生成文本。</p>
<h2 id="评测指标"><a href="#评测指标" class="headerlink" title="评测指标"></a>评测指标</h2><h3 id="BLEU"><a href="#BLEU" class="headerlink" title="BLEU"></a>BLEU</h3><h4 id="什么是BLEU"><a href="#什么是BLEU" class="headerlink" title="什么是BLEU"></a>什么是BLEU</h4><p>双语评估替补（Bilingual Evaluation Understudy，BLEU）是机器翻译任务中常用的评价指标，后来也被引申用于评估自然语言生成任务的效果。BLEU的取值范围为[0,1]，如果两个句子完美匹配，那么BLEU就是1，相反，如果两个句子完美不匹配，则BLEU为0。BLEU具备以下优点：</p>
<ul>
<li>计算代价小、计算速度快</li>
<li>容易理解</li>
<li>与语言无关，可以用于衡量任何语言之间差距</li>
<li>与人类评价结果高度相关</li>
</ul>
<h4 id="计算方式"><a href="#计算方式" class="headerlink" title="计算方式"></a>计算方式</h4><p>BLEU旨在计算模型生成的句子（candidate）和实际句子（reference）之间差异的指标。以N-gram的形式计算两者之间匹配个数，来计算得分，显然这种衡量方式是与语序无关的。<br>N-gram的计算方式如下列图中所示<br><strong>1-gram</strong></p>
<img src="/%E4%BB%BB%E5%8A%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95%E7%BB%BC%E8%BF%B0/1-gram.png" class="" title="1-gram">
<p>图中机器翻译6个词，命中reference中5个词，则匹配度为5&#x2F;6。<br><strong>2-gram</strong></p>
<img src="/%E4%BB%BB%E5%8A%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95%E7%BB%BC%E8%BF%B0/2-gram.jpeg" class="" title="2-gram">
<p>2-gram时有命中3个词，匹配度为3&#x2F;5。<br><strong>3-gram</strong></p>
<img src="/%E4%BB%BB%E5%8A%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95%E7%BB%BC%E8%BF%B0/3-gram.jpeg" class="" title="3-gram">

<p>1-gram可以带边原文有多少词被翻译出来，可以反映译文的<strong>充分性</strong>，2-gram可以反映译文的<strong>流畅性</strong>，越高则代表可读性越好。但是由于是通过匹配的方式来衡量性能，无法用于衡量译文的正确性。</p>
<img src="/%E4%BB%BB%E5%8A%A1%E5%9E%8B%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95%E7%BB%BC%E8%BF%B0/bad-case.jpeg" class="" title="bad-case">
<p>如果直接在reference中匹配candidate的话，会发现总是能匹配到the，其匹配度为7&#x2F;7，但这明显是错误的。因此，在BLEU中，实际上使用的是min（某N-gram在candidate中的出现次数，该N-gram在reference中出现的最大次数）。<br>the 在candidata中出现次数为7，但在reference中出现次数仅为2，因此，1-gram的匹配度为2&#x2F;7。</p>
<p><strong>惩罚因子</strong><br>当机器翻译的长度比较短时，BLEU得分也会比较高，但是这个翻译会损失很多信息，如candidate:a cat reference: there is a cat on the table<br>因此需要在BLEU分数乘上惩罚因子<br>$$BP&#x3D;\LEFT{<br>    \begin{aligned}<br>    1 &amp; len(candidate) &gt; len(reference)<br>    exp(1-len(reference)&#x2F;len(candidate)) &amp; len(candidate) &lt;&#x3D;&gt; len(reference)<br>}$$</p>
<p><strong>举例计算</strong><br>假设candidate为：Going to play basketball this afternoon ?<br>    reference为：Going to play basketball in the afternoon ?<br>假设candidate为：Going长度为：7，reference长度为：8</p>
<ul>
<li><p>计算各阶n-gram精度<br>P1 &#x3D; 6&#x2F;7 &#x3D; 0.8333<br>p2 &#x3D; 4&#x2F;6 &#x3D; 0.6666<br>p3 &#x3D; 2&#x2F;5 &#x3D; 0.4<br>p4 &#x3D; 1&#x2F;4 &#x3D; 0.25 </p>
</li>
<li><p>计算Pn的log</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line">p1_log = math.log(p1)</span><br><span class="line">p2_log = math.log(p2)</span><br><span class="line">p3_log = math.log(p3)</span><br><span class="line">p4_log = math.log(p4)</span><br><span class="line"><span class="comment"># 对Pn的log值求和</span></span><br><span class="line">sum_all = <span class="built_in">sum</span>([p1_log,p2_log,p3_log,p4_log])</span><br><span class="line"><span class="comment"># 乘Wn，就是除4</span></span><br><span class="line">sum_all = sum_all / <span class="number">4</span></span><br></pre></td></tr></table></figure></li>
<li><p>计算BP<br>BP &#x3D; e^(1-8&#x2F;7) &#x3D; 0.867</p>
</li>
<li><p>计算最终结果<br>BLEU &#x3D; BP * e^( (P1+P2+P3+P4) &#x2F; 4  )<br>   &#x3D; 0.867 * e^( (P1+P2+P3+P4) &#x2F; 4) &#x3D; 0.867 * 0.4889 &#x3D; 0.4238</p>
</li>
</ul>
]]></content>
      <categories>
        <category>论文浅读</category>
      </categories>
  </entry>
  <entry>
    <title>优化算法串讲</title>
    <url>/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E4%B8%B2%E8%AE%B2/</url>
    <content><![CDATA[<h2 id="基本框架"><a href="#基本框架" class="headerlink" title="基本框架"></a>基本框架</h2><p>定义当前时刻待优化参数为\theta_t \in R^d，损失函数为J(\theta)，学习率为\ita，参数更新框架为：</p>
<ul>
<li>g_t &#x3D; \delta J(\theta_t)</li>
<li>根据历史梯度计算一阶动量和二阶动量<br>Mt &#x3D; FI(g_1,g_2,\dots,g_t),V_t &#x3D; FI(g_1,g_2,\dots,g_t)</li>
<li>计算当前时刻的下降梯度：<br>\delta J(\theta_t) &#x3D; -\ita \dot \frac{m_t}{\sqrt{V_t}}</li>
<li>根据下降梯度更新参数<br>\theta_(t+1) &#x3D; \theta_t + \delta \theta_t</li>
</ul>
<h2 id="随机梯度下降法（Stochastic-Gradient-Descent-SGD）"><a href="#随机梯度下降法（Stochastic-Gradient-Descent-SGD）" class="headerlink" title="随机梯度下降法（Stochastic Gradient Descent, SGD）"></a>随机梯度下降法（Stochastic Gradient Descent, SGD）</h2><p>SGD中没有动量的概念，也没有考虑历史梯度，所以他一阶动量即为当前时刻的梯度，二阶动量V_t为E，所以SGD参数的更新公式为：<br>\delta \theta_t &#x3D; - \ita \dot \frac{g_t}{\sqrt(E)} &#x3D; -\ita \dot g_t<br>\theta_(t+1) &#x3D; \theta_t + \delta \theta_t &#x3D; \theta_t - \ita \dot g_t<br>由于SGD只使用当前时刻的梯度更新参数，没有考虑到历史梯度，所以很容易陷入局部最优解。于是便提出了Momentum来解决SGD陷入局部最优的问题。</p>
<h2 id="指数加权移动平均值-Exponentially-Weighted-Moving-Average-EWMA"><a href="#指数加权移动平均值-Exponentially-Weighted-Moving-Average-EWMA" class="headerlink" title="指数加权移动平均值(Exponentially Weighted Moving Average,EWMA)"></a>指数加权移动平均值(Exponentially Weighted Moving Average,EWMA)</h2><p>虽然解决局部最优问题要考虑历史梯度，但也并将所有历史梯度都要考虑在内，只要考虑离当前时刻相近的梯度即可。</p>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><img src="/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E4%B8%B2%E8%AE%B2/%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E7%A7%BB%E5%8A%A8%E5%B9%B3%E5%9D%87%E5%80%BC.png" class="" title="指数加权移动平均值">
<img src="/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E4%B8%B2%E8%AE%B2/%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E7%A7%BB%E5%8A%A8%E5%B9%B3%E5%9D%87%E5%80%BC-2.png" class="" title="指数加权移动平均值-2">
<img src="/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E4%B8%B2%E8%AE%B2/%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E7%A7%BB%E5%8A%A8%E5%B9%B3%E5%9D%87%E5%80%BC-3.png" class="" title="指数加权移动平均值-3">
<p>通过EMWA来将距离较远的梯度筛选掉，只让距离近的梯度对当前梯度产生影响</p>
<h2 id="Momentum-SGD-with-Momentum"><a href="#Momentum-SGD-with-Momentum" class="headerlink" title="Momentum(SGD with Momentum)"></a>Momentum(SGD with Momentum)</h2><img src="/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E4%B8%B2%E8%AE%B2/momentum.png" class="" title="momentum">
<h2 id="NAG"><a href="#NAG" class="headerlink" title="NAG"></a>NAG</h2><img src="/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E4%B8%B2%E8%AE%B2/nag.png" class="" title="nag">
<p>不考虑当前时刻的梯度，用当前的下降梯度减去历史梯度惯性，得到“下一步”的梯度。此时仍然没有使用到二阶动量</p>
<h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><p>二阶动量的出现意味着“自适应学习率”优化算法时代的到来。</p>
<img src="/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E4%B8%B2%E8%AE%B2/adagrad.png" class="" title="adagrad">
<p>对于更新幅度很大的参数，通常历史累计梯度的平方和会很大，更新幅度小的参数，累计历史梯度的平方和会很小。所以在一个固定学习率的基础上除以历史累计梯度的平方和就能使更新幅度大的参数学习率变小，同样也能使用更新幅度小的参数学习率变大。<br>但时间步足够长之后，所有参数的累积梯度平方和都会变大，以至于所有参数的学习率都不断减小。<br>于是便出现了不累计全部历史梯度，只关注过去一段时间窗口的下降梯度，筛选的过程同样是使用EMWA实现的。</p>
<h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p>RMSProp就是在AdaGrad的基础上将普通历史累计梯度平方和换成历史累计梯度平方和的指数加权移动平均值</p>
<h2 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a>AdaDelta</h2><p>针对RMSProp中的学习率进行改进。计算出历史下降梯度的指数加权移动平均，并替换预先设置的学习率，所以在AdaDelta中不需要设置学习率，只要设置好\beta和\alpha的衰减率即可。</p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>Momentum在SGD的基础上增加了一阶动量，AdaGrad在SGD基础上增加了二阶动量，一阶动量二阶动量都用起来就是Adam了。</p>
<h2 id="Nadam"><a href="#Nadam" class="headerlink" title="Nadam"></a>Nadam</h2><p>Nadam是在Adam的基础上将Nesterov集成进来。将t-1时刻的动量m_(t-1)用t时刻的动量m_t近似替代，就引入了“未来因素”</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<p>决策树（Decision Tree）是一种非参数的有监督学习方法，它能够从一系列有特征和标签的数据中总结出决策规则，并用树状图的结构来呈现这些规则，以解决分类和回归问题。</p>
<h2 id="如何工作"><a href="#如何工作" class="headerlink" title="如何工作"></a>如何工作</h2><p>决策树算法的本质是一种图结构，我们只需要问一系列问题就可以对数据进行分类了。比如说，来看看下面这组数据集，这是一系列已知物种以及所属类别的数据</p>
<table>
<thead>
<tr>
<th>名字</th>
<th>体温</th>
<th>表皮覆盖</th>
<th>胎生</th>
<th>水生动物</th>
<th>飞行动物</th>
<th>有腿</th>
<th>冬眠</th>
<th>类标号</th>
</tr>
</thead>
<tbody><tr>
<td>人类</td>
<td>恒温</td>
<td>毛发</td>
<td>是</td>
<td>否</td>
<td>否</td>
<td>是</td>
<td>否</td>
<td>哺乳类</td>
</tr>
<tr>
<td>鲑鱼</td>
<td>冷血</td>
<td>鳞片</td>
<td>否</td>
<td>是</td>
<td>否</td>
<td>否</td>
<td>否</td>
<td>鱼类</td>
</tr>
<tr>
<td>鲸</td>
<td>恒温</td>
<td>毛发</td>
<td>是</td>
<td>是</td>
<td>否</td>
<td>否</td>
<td>否</td>
<td>哺乳类</td>
</tr>
<tr>
<td>青蛙</td>
<td>冷血</td>
<td>无</td>
<td>否</td>
<td>半</td>
<td>否</td>
<td>是</td>
<td>是</td>
<td>两栖类</td>
</tr>
<tr>
<td>巨蜥</td>
<td>冷血</td>
<td>鳞片</td>
<td>否</td>
<td>否</td>
<td>否</td>
<td>是</td>
<td>否</td>
<td>爬行类</td>
</tr>
</tbody></table>
<p>我们现在的目标是，将动物们分为哺乳类和非哺乳类。那根据已经收集到的数据，决策树算法为我们算出了下面的这棵决策树。</p>
<blockquote>
<pre><code>            体温
     恒温  /     \ 冷血
        胎生        【非哺乳动物】
   是 /      \ 否
</code></pre>
<p> 【哺乳动物】    【非哺乳动物】</p>
</blockquote>
<p>假如现在发现了新物种Python，它是冷血动物，不是胎生，就可以根据决策树来判断它的类别。</p>
<h2 id="决策树的类型"><a href="#决策树的类型" class="headerlink" title="决策树的类型"></a>决策树的类型</h2><ul>
<li>ID3: 使用信息增益方法作为属性的选择标准，来帮助确定生成每个节点时所采用的合适属性</li>
<li>C4.5：相对于ID3，使用信息增益率来选择节点属性。ID3只适用于离散的描述属性，而C4.5算法既能够处理离散的描述属性，也可以处理连续的描述属性</li>
<li>CART：CART决策树是一种十分有效的非参数分类和回归方法，通过构建树、修剪树、评估树来构建一个二叉树，当终结点事连续变量时，为回归树；当终节点是分类变量，则为分类树。</li>
</ul>
<h2 id="Decision-Tree-Classifier-与红酒数据集"><a href="#Decision-Tree-Classifier-与红酒数据集" class="headerlink" title="Decision Tree Classifier 与红酒数据集"></a>Decision Tree Classifier 与红酒数据集</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_wine</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">wine = load_wine()</span><br><span class="line">pd.concat([pd.DataFrame(wine.data),pd.DataFrame(wine.target)],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">Xtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data,wine.target,test_size=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">clf = tree.DecisionTreeClassifier(criterion=<span class="string">&quot;entropy&quot;</span>)</span><br><span class="line">clf = clf.fit(Xtrain, Ytrain)</span><br><span class="line">score = clf.score(Xtest, Ytest) <span class="comment">#返回预测的准确度</span></span><br><span class="line">score</span><br></pre></td></tr></table></figure>
<h3 id="criterion"><a href="#criterion" class="headerlink" title="criterion"></a>criterion</h3><p>如何衡量决策树中节点是否合理的指标为“不纯度”，不纯度越低，决策树对训练集的拟合就越好。决策树中每个节点都有不纯度，且子节点的不纯度一定低于父节点，所以决策树中叶子节点的不纯度一定是最低的。可以使用基尼系数（gini）或者信息熵（entropy）来计算决策树中节点的不纯度。<br>比起基尼系数，信息熵对不纯度更加敏感，对不纯度的惩罚最强。但是在实际使用中，信息熵和基尼系数的效果基本相同。信息熵的计算比基尼系数缓慢一些，因为基尼系数的计算不涉及对数。另外，因为信息熵对不纯度更加敏感，所以信息熵作为指标时，决策树的生长会更加“精细”，因此对于高维数据或者噪音很多的数据，信息熵很容易过拟合，基尼系数在这种情况下效果往往比较好。当模型拟合程度不足的时候，即当模型在训练集和测试集上都表现不太好的时候，使用信息熵。当然，这些不是绝对的。</p>
<h3 id="画树"><a href="#画树" class="headerlink" title="画树"></a>画树</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature_name = [<span class="string">&#x27;酒精&#x27;</span>,<span class="string">&#x27;苹果酸&#x27;</span>,<span class="string">&#x27;灰&#x27;</span>,<span class="string">&#x27;灰的碱性&#x27;</span>,<span class="string">&#x27;镁&#x27;</span>,<span class="string">&#x27;总酚&#x27;</span>,<span class="string">&#x27;类黄酮&#x27;</span>,<span class="string">&#x27;非黄烷类酚类&#x27;</span>,<span class="string">&#x27;花青素&#x27;</span>,<span class="string">&#x27;颜色强度&#x27;</span>,<span class="string">&#x27;色调&#x27;</span>,<span class="string">&#x27;od280/od315稀释葡萄酒&#x27;</span>,<span class="string">&#x27;脯氨酸&#x27;</span>]</span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line">dot_data = tree.export_graphviz(clf</span><br><span class="line">,out_file = <span class="literal">None</span></span><br><span class="line">,feature_names= feature_name</span><br><span class="line">,class_names=[<span class="string">&quot;琴酒&quot;</span>,<span class="string">&quot;雪莉&quot;</span>,<span class="string">&quot;贝尔摩德&quot;</span>]</span><br><span class="line">,filled=<span class="literal">True</span></span><br><span class="line">,rounded=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line">graph = graphviz.Source(dot_data)</span><br><span class="line">graph</span><br></pre></td></tr></table></figure>
<h3 id="特征重要性"><a href="#特征重要性" class="headerlink" title="特征重要性"></a>特征重要性</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf.feature_importances_</span><br><span class="line">[*<span class="built_in">zip</span>(feature_name,clf.feature_importances_)]</span><br></pre></td></tr></table></figure>
<p> 无论决策树模型如何进化，在分枝上的本质都还是追求某个不纯度相关的指标的优化，而正如我们提到的，不纯度是基于节点来计算的，也就是说，决策树在建树时，是靠优化节点来追求一棵优化的树，但最优的节点能够保证最优的树吗？集成算法被用来解决这个问题：sklearn表示，既然一棵树不能保证最优，那就建更多的不同的树，然后从中取最好的。怎样从一组数据集中建不同的树？在每次分枝时，不从使用全部特征，而是随机选取一部分特征，从中选取不纯度相关指标最优的作为分枝用的节点。这样，每次生成的树也就不同了。<br> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clf = tree.DecisionTreeClassifier(criterion=<span class="string">&quot;entropy&quot;</span>,random_state=<span class="number">30</span>)</span><br><span class="line">clf = clf.fit(Xtrain, Ytrain)</span><br><span class="line">score = clf.score(Xtest, Ytest) <span class="comment">#返回预测的准确度</span></span><br><span class="line">score</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title>初识CRF</title>
    <url>/%E5%88%9D%E8%AF%86CRF/</url>
    <content><![CDATA[<h2 id="序列标注"><a href="#序列标注" class="headerlink" title="序列标注"></a>序列标注</h2><p>CRF通常用于序列标注的场景，旨在为输入序列的每一个位置都赋予一个标签，假设模型的输入为Q，输出目标是一个序列a_1,a_2,\dots,a_n，按照通常的建模逻辑，我们希望目标输出序列的概率最大，即：<br>P(a_1,a_2,\dots,a_n|Q)<br>不论是传统方法还是深度学习方法，直接对完整序列建模是比较困难的，因此我们通常会使用一些假设来简化它，比如直接使用朴素假设，即可得到<br>P(a_1,a_2,\dots,a_n|Q)&#x3D;P(a_1|Q)P(a_2|Q) \dots P(a_n|Q)<br> 此时的Q不一定是原始输入，也可能是LSTM等网络的输出，我们可以认为LSTM的输出中已经具备了序列的特征信息，而且还可以在此基础之上假设输出序列之间互不相关，于是就可以得到：<br>P(a_1|Q) &#x3D; P(a_1|q_1,q_2,\dots,q_n) &#x3D; P(a_1|q_1)<br>P(a_2|Q) &#x3D; P(a_2|q_1,q_2,\dots,q_n) &#x3D; P(a_2|q_2)<br>P(a_3|Q) &#x3D; P(a_1|q_1,q_2,\dots,q_n) &#x3D; P(a_3|q_3)<br>…<br>P(a_n|Q) &#x3D; P(a_1|q_1,q_2,\dots,q_n) &#x3D; P(a_n|q_n)<br>从而<br>P(a_1|q_1,q_2,\dots,q_n|Q) &#x3D; P(a_1|q_1)P(a_2|q_2)\dotsP(a_n|q_n)<br>这样就得到了最常用的方案，逐位输出概率值最大的标签。</p>
<h2 id="条件随机场"><a href="#条件随机场" class="headerlink" title="条件随机场"></a>条件随机场</h2><p>在使用逐位输出概率值最大的标签的方法进行序列标注时，如果我们使用BIOES对实体信息进行标注，可能会出现BBB这样序列，但这个序列违反了我们的解码规则，B后应当只能为I或者E。之所以会出现这个问题，是因为我们假设输出序列之间互不相关，只要引入上下文信息，就可以解决这个问题。按照以往的方式，通常会建立一个转移矩阵，把不合理的转移概率设为0，如P（b|b）&#x3D;0，然后通过动态规划保证得到合理的序列。<br>P(a_1,a_2,\dots,a_n|Q) &#x3D; P(a_1|Q)P(a_2|a_1)P(a_3|a_1,a_2) \dots P(a_n|a_1,a_2,\dots,a_(n-1))&#x3D;P(a_1|Q)P(a_2|a_1,Q) \dots P(a_n|a_(n-1),Q)<br>通过使用上述方法建立输出之间的联系，其中每一项都是转移概率。而CRF的做法是，假设序列之间存在关系f(x,y;Q)，然后直接令<br>P(a_1|q_1,q_2,\dots,q_n|Q) &#x3D; \frac{1}{Z} exp (\sum_k f(a_(k-1),a_k;Q))<br>其中Z是归一化因子，与前者相比，P(a_k|a_(k-1))是有概率意义的（条件概率），而单项的e^(f(a_(k-1),a_k;Q))&#x2F;Z是没有概率意义额，所以CRF是更一般的形式。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>前缀和</title>
    <url>/%E5%89%8D%E7%BC%80%E5%92%8C/</url>
    <content><![CDATA[<p>什么是前缀和（2389）<br>前缀和指一个数组的某下标之前的所有数组元素的和（包含其自身）。前缀和分为一维前缀和，以及二维前缀和。前缀和是一种重要的预处理，能够降低算法的时间复杂度。<br>什么是一维前缀和？<br>一维前缀和的公式：sum[i] &#x3D; sum[i-1] + arr[i] ; sum是前缀和数组, arr是内容数组。拥有前缀和数组后, 我们可以在O(1)的时间复杂度内求出区间和。<br>[i, j]的区间和公式: interval [i, j] &#x3D; sum[j] - sum[i - 1]<br>作者：dyhtps<br>链接：<a href="https://juejin.cn/post/6944913393627168798">https://juejin.cn/post/6944913393627168798</a><br>来源：稀土掘金<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
]]></content>
      <categories>
        <category>Leetcode题解笔记</category>
      </categories>
  </entry>
  <entry>
    <title>反爬虫原理与绕过实战笔记</title>
    <url>/%E5%8F%8D%E7%88%AC%E8%99%AB%E5%8E%9F%E7%90%86%E4%B8%8E%E7%BB%95%E8%BF%87%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="静态网页与动态网页"><a href="#静态网页与动态网页" class="headerlink" title="静态网页与动态网页"></a>静态网页与动态网页</h1><ul>
<li>传统的静态网页指没有数据库和不可交互的纯HTML页面，不修改代码则显示内容不会改变。</li>
<li>传统的动态网页指在不改变页面HTML代码的情况下，能够根据不同用户或者不同操作而显示不同内容的网页。<br>在爬虫领域中，静态网页与动态网页的定义与传统定义是完全不同的。</li>
<li>静态网页指网页主题内容的渲染工作在服务器端完成，并通过响应正文返回的网页。</li>
<li>动态网页指的是主体内容或者全部内容都需要客户端执行JavaScript代码来计算或渲染的网页。</li>
</ul>
<p>爬虫并非所见即所得。在得到的资源中，最重要的就是响应正文。但是由于Python、Java和PHP等变成语言没有JavaScript解释器和渲染引擎，所以使用变成语言编写的爬虫程序无法渲染页面，它们智能爬取响应正文中的内容。有一些工具已经集成了渲染页面索需要的组件，并且开放API允许变成语言操作页面以获取渲染后的页面代码。</p>
<ul>
<li>Splash：异步的JavaScript渲染服务</li>
<li>Selenium： 自动化测试框架</li>
<li>Puppeteer：一个通过DevTools协议控制Chrome的Node.js库</li>
</ul>
<h1 id="反爬虫的概念与定义"><a href="#反爬虫的概念与定义" class="headerlink" title="反爬虫的概念与定义"></a>反爬虫的概念与定义</h1><p>书内约定，限制爬虫程序访问服务器资源和获取数据的行为成为反爬虫。限制手段包括但不限于请求限制、拒绝响应、客户端身份验证、文本混淆和使用动态渲染技术。可以分成主动反爬和被动反爬。</p>
<ul>
<li>主动型反爬虫：开发者有意识地使用技术手段区分正常用户和爬虫，并限制爬虫对网站的访问行为，如验证请求头信息、限制访问频率、使用验证码等。</li>
<li>被动型反爬虫：为了提升用户体验或节省资源，用一些技术间接提高爬虫访问难度的行为，比如数据分段加载、点击切换标签页、鼠标悬停预览数据等。<br>还可以从特点上对反爬虫进行更细致的划分，如信息校验型反爬虫、动态渲染型反爬虫、文本混淆型反爬虫、特征识别型反爬虫等。需要注意的是，同一种限制现象可以被柜内到不同的反爬虫类型中，比如通过JS生成随机字符串并将字符串放在请求头中发送给服务器，服务器校验客户端身份的这种限制手段既可以说是信息校验型反爬虫，也可以说是动态渲染反爬虫。</li>
</ul>
<h1 id="反爬手段"><a href="#反爬手段" class="headerlink" title="反爬手段"></a>反爬手段</h1><h2 id="信息校验型反爬虫"><a href="#信息校验型反爬虫" class="headerlink" title="信息校验型反爬虫"></a>信息校验型反爬虫</h2><p>信息校验中的“信息”指的是客户端发起网络请求时的请求头和请求正文，而“校验”指的是服务器端通过对信息的正确性、完整性或唯一性进行验证或判断，从而区分正常用户和爬虫程序的行为。<br>在Web应用中，用户每次请求都会先经过服务器，然后转发到对应的后端程序。后端面临众多请求时要如何识别哪些是用户请求，哪些是爬虫请求？<strong>校验请求头和请求正文就可以区分正常用户和爬虫程序</strong>。</p>
<h3 id="User-Agent反爬虫"><a href="#User-Agent反爬虫" class="headerlink" title="User-Agent反爬虫"></a>User-Agent反爬虫</h3><p>这是一种较为初级的反爬虫手段。<br>浏览器是一种用于检索并展示万维网信息资源的应用程序，使用浏览器在各个网页之间跳转其实就是访问不同的信息资源，服务器会根据客户端传递的请求信息以及身份信息返回客户端所希望接收的内容。User-Agent就是请求头域之一，服务器能够从User-Agent对应的值中识别客户端使用的操作系统、CPU类型、浏览器、浏览器引擎、操作系统语言等。之所以选择User-Agent头域作为校验对象，是因为很多变成语言和软件有默认的标识。比如使用Python中的Requests库向服务器发送HTTP请求时，服务器读取的User-Agent值为：</p>
<blockquote>
<p>python-requests&#x2F;2.21.0<br>使用Java和PHP等语言编写的库也有类似的默认标识。在nginx中可以设定黑名单，用以屏蔽爬虫程序的请求。nginx的配置生效后，只要请求头中的User-Agent头域值包含黑名单中的关键词，那么这次请求就无法通过校验。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># selenium 4</span></span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.options <span class="keyword">import</span> Options</span><br><span class="line">user_agent = <span class="string">&#x27;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">options = Options()</span><br><span class="line">options.add_argument(<span class="string">f&quot;--User-Agent=<span class="subst">&#123;user_agent&#125;</span>&quot;</span>)</span><br><span class="line">driver = webdriver.Chrome(options=options)</span><br><span class="line">driver.get(url)</span><br></pre></td></tr></table></figure>
<h3 id="Cookie反爬虫"><a href="#Cookie反爬虫" class="headerlink" title="Cookie反爬虫"></a>Cookie反爬虫</h3><p>Cookie不近可以用于Web服务器的用户信息存储或状态保持，还能够用于反爬虫。大部分的爬虫程序在默认情况下只请求HTML文本资源，这意味着它们不会主动完成浏览器保存Cookie的操作，这次的反爬虫正式利用了这个特点。浏览器会自动检查响应头中是否存在Set-Cookie头域。如果存在，则将值保存在本地，而且往后的每次请求都会自动携带对应的Cookie值，这时候只要服务器端对请用头中的Cookie值进行校验即可。服务器会校验每个请求头中的Cookie值是否符合规则，对于不符合的请求将重新定向到首页。</p>
<ul>
<li>若携带正确Cookie，那么浏览器、Postman和爬虫程序发出的请求都可以正常响应。</li>
<li>若未携带正确Cookie，那么浏览器、Postman和爬虫程序发出的请求都被重定向</li>
<li>只有首页页面的响应头中有Set-Cookie头域，内容页的响应头中没有<br>User-Agent和Cookie都是请求头的默认头域，在值的设定方面有一定的局限性，但是与JavaScript结合后，就会变得很灵活。相对服务器软件来说，后端程序的校验更为灵活且准确，但使用后端程序进行校验所需的步骤较多，在实际应用时可以根据需求选择合适的校验方式。</li>
</ul>
<h3 id="签名验证反爬虫"><a href="#签名验证反爬虫" class="headerlink" title="签名验证反爬虫"></a>签名验证反爬虫</h3><p>签名是根据数据源进行计算或加密的过程，签名的结果是一个具有唯一性和一致性的字符串。签名结果的特性使得它成为验证数据来源和数据完整性的条件，可以有效避免服务器端将伪造数据或被篡改的数据当成正常数据处理。<br>签名验证是防止恶意连接和数据被篡改的有效方式之一，也是目前后端API最常用的防护方式之一。与Cookie、User-Agent、Host和Referer等请求头域不同，用于签名验证的信息通常被放在请求正文中发送到服务器端。<br>（那跟我应该就没啥关系了，另外也看不懂了。。涉及到好多前端、后端、JavaScript的知识。。。）</p>
<h2 id="动态渲染反爬虫"><a href="#动态渲染反爬虫" class="headerlink" title="动态渲染反爬虫"></a>动态渲染反爬虫</h2><p>Selenium可破&#x2F;doge</p>
<h3 id="异步渲染服务Splash"><a href="#异步渲染服务Splash" class="headerlink" title="异步渲染服务Splash"></a>异步渲染服务Splash</h3><p>如果只需要在一台计算机上运行爬虫程序，那么使用Selenium套件或者Puppeteer就可以满足渲染需求了。但如果是分布式爬虫呢？假如我们需要在30台服务器上启动爬虫程序，那么要在每一台服务器上安装一个Selenium套件嘛？<br>Splash是一个异步的JavaScript渲染服务，它是带有HTTP API的轻量级Web浏览器。SPlash能够并行地处理多个页面请求，在页面上下文中执行自定义的JavaScript以及浏览器中的点击、下滑等操作。有了Splash之后情况就变得不一样了，我么可以将Splash服务部署到云服务器上并配置负载均衡。这样做的好处是渲染服务压力大的时候，可以动态的增加Splash渲染服务。多个爬虫程序公用Splash服务还可以节省硬件资源。</p>
<p>（坑）</p>
<h3 id="渲染工具知识扩展"><a href="#渲染工具知识扩展" class="headerlink" title="渲染工具知识扩展"></a>渲染工具知识扩展</h3><p>但是Selenium有一个非常明显的问题就是：慢。开启webdriver,访问页面等待元素加载，模拟点击等等耗时过长。有没有更快的方法呢？<br>比如，页面上的内容是由JavaScript代码计算得出时，可以获得源数据后模仿JS逻辑自己在本地运算。</p>
<p>相比于直接使用浏览器的Selenium和Puppeteer来说，Splash在页面渲染、对新特性的支持等方面是比较差的。这个差距表现在对DOM节点的渲染和HTML Element时间操作上，例如点击事件触发后内容无法渲染到指定的位置。但是在浏览器中经常使用到的操作比如点击、滑动、拖拽、文本输入、复制、页面前进、后退、截图、文件下载等，Splash也同样能做到。</p>
<h2 id="文本混淆反爬虫"><a href="#文本混淆反爬虫" class="headerlink" title="文本混淆反爬虫"></a>文本混淆反爬虫</h2><p>文本混淆可以有效地避免爬虫获取Web应用中重要的文字数据，使用文本混淆限制爬虫虎丘文字数据的方式称为文本混淆反爬虫。反爬虫的前提是不能影响永固正常浏览网页和阅读文字内容，直接混淆文本很容易被看出来，所以开发者通常是利用CSS的特性来实现混淆。常见的文本混淆手段有图片伪装、文字映射和自定义字体等。</p>
<h3 id="图片伪装反爬虫"><a href="#图片伪装反爬虫" class="headerlink" title="图片伪装反爬虫"></a>图片伪装反爬虫</h3><p>图片伪装指的是将带有文字的图片与正常文字混合在一起，以达到“鱼目混珠”的效果，这种混淆方式不会影响用户阅读，但是可以让爬虫程序无法获得“所见”的文字内容。<br>（如何解决？上OCR！）</p>
<h3 id="CSS偏移反爬虫"><a href="#CSS偏移反爬虫" class="headerlink" title="CSS偏移反爬虫"></a>CSS偏移反爬虫</h3><p>CSS偏移反爬虫指的是利用CSS央视将乱序的文字排版为人类正常阅读顺序的行为。这个概念不是很好理解，我们可以通过对比两段文字来加深对这个概念的理解。</p>
<blockquote>
<p>HTML文本中的文字：我的学号是1308205，我在北京大学读书<br>浏览器显示的问题：我的学号是1380205，我在北京大学读书<br>爬虫看到的学号是1308205，但用户在浏览器中看到的却是1380205.如果不细心观察，爬虫工程师很容易被爬取结果糊弄。这种混淆方法和图片伪装一样，是不会影响用户阅读的。<br>这种情况可能需要观察页面上的CSS样式，找出其中的规律，将爬虫看到的数据转换成展示给用户的数据。</p>
</blockquote>
<h3 id="SVG映射反爬虫"><a href="#SVG映射反爬虫" class="headerlink" title="SVG映射反爬虫"></a>SVG映射反爬虫</h3><h3 id="字体反爬虫"><a href="#字体反爬虫" class="headerlink" title="字体反爬虫"></a>字体反爬虫</h3><h3 id="文本混淆反爬虫通用解决方法"><a href="#文本混淆反爬虫通用解决方法" class="headerlink" title="文本混淆反爬虫通用解决方法"></a>文本混淆反爬虫通用解决方法</h3><h2 id="特征识别反爬虫"><a href="#特征识别反爬虫" class="headerlink" title="特征识别反爬虫"></a>特征识别反爬虫</h2><h3 id="webdriver识别"><a href="#webdriver识别" class="headerlink" title="webdriver识别"></a>webdriver识别</h3><h3 id="浏览器特征"><a href="#浏览器特征" class="headerlink" title="浏览器特征"></a>浏览器特征</h3><h3 id="爬虫特征"><a href="#爬虫特征" class="headerlink" title="爬虫特征"></a>爬虫特征</h3><h3 id="隐藏链接反爬虫"><a href="#隐藏链接反爬虫" class="headerlink" title="隐藏链接反爬虫"></a>隐藏链接反爬虫</h3><h2 id="App反爬虫"><a href="#App反爬虫" class="headerlink" title="App反爬虫"></a>App反爬虫</h2><h3 id="App抓包"><a href="#App抓包" class="headerlink" title="App抓包"></a>App抓包</h3><h3 id="APK文件反编译"><a href="#APK文件反编译" class="headerlink" title="APK文件反编译"></a>APK文件反编译</h3><h3 id="代码混淆反爬虫"><a href="#代码混淆反爬虫" class="headerlink" title="代码混淆反爬虫"></a>代码混淆反爬虫</h3><h3 id="App应用加固知识扩展"><a href="#App应用加固知识扩展" class="headerlink" title="App应用加固知识扩展"></a>App应用加固知识扩展</h3><h3 id="了解应用程序自动化测试工具"><a href="#了解应用程序自动化测试工具" class="headerlink" title="了解应用程序自动化测试工具"></a>了解应用程序自动化测试工具</h3><h2 id="验证码"><a href="#验证码" class="headerlink" title="验证码"></a>验证码</h2><h3 id="字符验证码"><a href="#字符验证码" class="headerlink" title="字符验证码"></a>字符验证码</h3><h3 id="计算型验证码"><a href="#计算型验证码" class="headerlink" title="计算型验证码"></a>计算型验证码</h3><h3 id="滑动验证码"><a href="#滑动验证码" class="headerlink" title="滑动验证码"></a>滑动验证码</h3><h3 id="滑动拼图验证码"><a href="#滑动拼图验证码" class="headerlink" title="滑动拼图验证码"></a>滑动拼图验证码</h3><p>滑动拼图验证吧在滑动验证码的基础上增加了随机滑动距离，用户需要使用滑动的方式完成拼图，才能通过校验。</p>
<h3 id="文字点选验证码"><a href="#文字点选验证码" class="headerlink" title="文字点选验证码"></a>文字点选验证码</h3><h3 id="鼠标轨迹的检测和原理"><a href="#鼠标轨迹的检测和原理" class="headerlink" title="鼠标轨迹的检测和原理"></a>鼠标轨迹的检测和原理</h3><h3 id="验证码产品赏析"><a href="#验证码产品赏析" class="headerlink" title="验证码产品赏析"></a>验证码产品赏析</h3><h2 id="综合知识"><a href="#综合知识" class="headerlink" title="综合知识"></a>综合知识</h2><h3 id="编码与加密"><a href="#编码与加密" class="headerlink" title="编码与加密"></a>编码与加密</h3><h3 id="JavaScript代码混淆"><a href="#JavaScript代码混淆" class="headerlink" title="JavaScript代码混淆"></a>JavaScript代码混淆</h3><h3 id="前端禁止事件"><a href="#前端禁止事件" class="headerlink" title="前端禁止事件"></a>前端禁止事件</h3><h3 id="法律法规"><a href="#法律法规" class="headerlink" title="法律法规"></a>法律法规</h3>]]></content>
      <categories>
        <category>常用工具</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title>基于大规模预训练开放域中文对话系统关键技术研究</title>
    <url>/%E5%9F%BA%E4%BA%8E%E5%A4%A7%E8%A7%84%E6%A8%A1%E9%A2%84%E8%AE%AD%E7%BB%83%E5%BC%80%E6%94%BE%E5%9F%9F%E4%B8%AD%E6%96%87%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6/</url>
    <content><![CDATA[<h1 id="EVA2-0：Investigating-Open-Domain-Chinese-Dialogue-Systems-with-Large-Scale-Pre-Training"><a href="#EVA2-0：Investigating-Open-Domain-Chinese-Dialogue-Systems-with-Large-Scale-Pre-Training" class="headerlink" title="EVA2.0：Investigating Open-Domain Chinese Dialogue Systems with Large-Scale Pre-Training"></a>EVA2.0：Investigating Open-Domain Chinese Dialogue Systems with Large-Scale Pre-Training</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在本文中，我们进行了广泛的实验来调查这些未被充分探索的因素，包括数据质量控制、模型架构设计、训练方法和解码策略。我们提出了EVA2.0，一个具有28亿个参数的大规模预训练开放域中文对话模型，并将我们的模型和代码公开。据我们所知，EVA2.0是最大的开源中文对话模型。自动和人工评估表明，我们的模型明显优于其他开源模型。我们还通过列举一些失效案例来讨论这项工作的局限性，并提出一些未来的方向。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>预训练对话模型可以从大规模语料中或缺通用的对话技巧和各种各样的知识，在下游任务中微调这些模型通常要比自行训练的模型效果更好。但是并非增加模型参数和增加训练语料就可以构建出一个出色的对话模型，其中还涉及到其他因素，比如<strong>预训练任务、解码策略、评估指标</strong>等，可能还有其他影响对话模型性能的因素尚未发现。目前出现的许多工作都未提供数据收集、数据处理过程和数据质量控制、解码策略等相关细节。通常都只是给出粗略的参数分析。<br>因此，本文就如何构建基于大规模预培训的开放域汉语对话系统进行了研究。我们对训练前语料库进行了详细的分析，并对模型设计、训练前方法和解码策略进行了广泛的实验。首先，我们全面分析了最大的中文对话数据集WDC-Dialogue的质量。我们发现，尽管该数据集规模庞大，但在上下文-响应相关性、语言流利性和领域多样性等方面存在严重问题。其次，我们探讨模型架构、预训练方法和解码策略的几种变体。我们的经验发现，这些因素确实对训练前的模型有一个非平凡的影响。<br>综合所有这些，我们首先设计了一个数据清理管道，并构建了用于大规模预训练的60GB高质量对话数据集。在此基础上，构建了参数为2.8B的开放域对话模型EVA2.0，以及参数为300M和970M的两种变体。在自动和人工评估中，2.8B EVA2.0模型显著优于其他开源生成对话模型。我们注意到，即使是300M型在自动评估中表现与EVA1.0模型相当，而只需要11%的参数。本文还通过案例分析，从不同角度分析了EVA2.0的会话能力，为未来大规模预训练开放域汉语对话系统的研究提供参考。我们的工作为汉语对话模式的研究提供了基础模型，我们相信这将极大地造福于对话研究界。</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h2><p><strong>大规模预训练语言模型</strong><br>迄今为止，已经出现了GPT\BERT\XLNET\BART\ROBERTA\CPM\PANGU\YUAN\MENGZI\ERNIE等大规模语言模型。<br><strong>预训练对话模型</strong><br>除了一般的语言理解和生成，会话前培训也越来越受到关注。比如DialoGPT\LaMDA\CDial-GPT\PLATO\EVA1.0<br>然而，这些作品大多都没有涉及到如何构建对话模型的细节。在本文中，除了最终的模型评估外，我们还关注了大规模的预先训练的汉语对话模型的关键配方。</p>
<h2 id="3-数据"><a href="#3-数据" class="headerlink" title="3 数据"></a>3 数据</h2><p>在大规模的预训练中，数据从本质上影响模型的表现和行为。在本节中，我们定义了几个自动的数据质量评价指标，综合衡量从社交媒体获得的对话前训练语料库的相关性、流畅性和领域多样性。然后，我们使用这些指标来分析WDC-Dialogue (Zhou et al.， 2021)，发现尽管WDC-Dialogue规模很大，但存在着上下文-响应相关性、语言流利性和领域多样性等严重问题。最后，在WDC-Dialogue的基础上，设计了一个更好的数据处理管道来构建EVA2.0的预训练数据。</p>
<h3 id="3-1-数据质量评估"><a href="#3-1-数据质量评估" class="headerlink" title="3.1 数据质量评估"></a>3.1 数据质量评估</h3><p><strong>Relevance Score</strong><br>语境与反应之间的关联分数是反映对话连贯性和参与性的重要指标。我们<strong>采用未经训练和训练过的度量标准</strong>来度量我们数据集的相关性。<br>对于未经训练的度量，我们计算上下文和响应之间的词覆盖率作为相关性的一个方面。此外，我们对一个会话中重叠单词间隔越远的数据样本给予更高的分数，以显示对长依赖属性的偏好。在形式上，上下文(C)和反应(R)的关联得分被定义为<br>$$S_1 &#x3D; \sum_(w_i\in C,w_j\inR)dist(w_i,w_j)^T I(w_i&#x3D;w_j)$$<br>其中，dist(w_i,w_j)表示包含wi和wj的话语之间的索引距离。使用T来调整分数。<br>训练过的指标，使用LCCC数据集微调了一个BERT-BASE的二进制分类器，来识别回复与上下文之间是否“合适”。在评价中，我们使用“适当”的分类概率作为关联得分:<br>$$S_2 &#x3D; log p(1|C,R)$$<br>与依赖于精确重叠的未训练度量相比，训练后的度量更好地估计了语义相关性。<br><strong>Fluency Score</strong><br>我们使用基于<a href="https://github.com/kpu/kenlm">kenlm</a>的统计模型计算数据集中每个句子的概率。对话会话中句子的平均流利度得分被定义为<br>$$S_3 &#x3D; - \frac{1}{n}\sum_(i&lt;&#x3D;n&gt;)logP(w_1^i,w_2^i,\cdots,w_(|u_i|)^i)$$<br>其中n是会话的话语数，UI &#x3D; wi 1wi 2, wi∣UI∣是第i个话语。<br><strong>Entertainment Tendency</strong><br>中国的社交媒体平台上有很多关于娱乐明星和粉丝的不受欢迎的信息交流，这些在日常对话中是不常见的。因此，我们计算中国明星的对话比例来衡量娱乐倾向。这也在一定程度上反映了数据集的领域多样性。</p>
<h3 id="3-2-数据过滤"><a href="#3-2-数据过滤" class="headerlink" title="3.2 数据过滤"></a>3.2 数据过滤</h3><p><strong>Dataset-level Filtering</strong><br>我们发现一些数据集不适合开放领域的对话。对他们的培训将导致不受欢迎的行为，如电子商务客户服务的基调。因此，我们将JDDC (Chen et al.， 2020)这样的数据集从WDC-Dialogue中删除。<br><strong>Context-level Filtering</strong><br>由于我们的数据集主要来自社交媒体平台，一些上下文对应着相当数量的回复(例如，微博帖子及其评论)。这些回复在格式上非常相似，<a href="https://arxiv.org/abs/2107.06499">可能会严重损害语言模型的表现</a>。因此，在过滤过程中，我们为每个上下文设置了一个最大回复数。<br><strong>Rule-based Filtering</strong><br>我们在<a href="https://arxiv.org/abs/2108.01547">EVA1.0</a>中加强了基于规则的过滤程序。例如，将繁体字转换为简体字，去除不合理的多个连续标点符号，简化黑名单，以避免对一些常见的一词多义现象过于敏感。<br><strong>Classiﬁer-based Filtering</strong><br>对于语料库中的每一个对话，我们计算上述定义的相关度和流利度得分，并过滤掉得分低于阈值的样本。一个会话的总分定义为S &#x3D; αS1 + βS2 + γS3。在实践中，我们根据经验为不同的数据源选择不同的阈值，以适应其数据分布，使最终的数据集均衡。</p>
<h3 id="3-3-数据扩展"><a href="#3-3-数据扩展" class="headerlink" title="3.3 数据扩展"></a>3.3 数据扩展</h3><p>社交媒体与开放领域对话之间的分布必然存在差距。例如，流行语在网络上很流行，但在日常对话中却很少见。为了消除我们的数据集的偏见并增加其领域多样性，我们从额外的公共来源中收集了四种类型的数据:<br>(1)从电影或电视剧字幕中提取的对话3 (Lison和Tiedemann, 2016);<br>(2)从小说和故事中提取的对话(Guan等人，2021);<br>(3)知道问答组4;<br>(4)现有众包语料库，包括DuConv (Wu et al.， 2019)、KdConv (Zhou et al.， 2020)、DuRecDial (Liu et al.， 2020)和NaturalConv (Wang et al.， 2021b)。<br>这些额外的数据占整个训练前语料库的12GB。</p>
<h3 id="3-4-数据统计"><a href="#3-4-数据统计" class="headerlink" title="3.4 数据统计"></a>3.4 数据统计</h3><p>我们使用上述数据处理管道构建最终的eva2.0数据集。在表1中，我们展示了使用3.1节定义的指标对我们的eva2.0数据集和WDC-Dialogue (Zhou et al.， 2021)进行的基本统计数据和质量评估。我们可以看到，虽然eva2.0数据集的数量不到原始WDCDialogue的三分之一，但它的质量明显更好。这意味着我们的数据细化过程提高了上下文和响应之间的相关性、语言的流畅性，并减少了娱乐领域的对话比例。此外，每个会话的平均话语数也增加了，使得训练样本更接近每日多回合对话。从5.3节的实验中我们可以看到，eva2.0数据集虽然数量少，但由于数据质量高，模型性能更好。</p>
<h2 id="4-方法"><a href="#4-方法" class="headerlink" title="4 方法"></a>4 方法</h2><h3 id="4-1模型"><a href="#4-1模型" class="headerlink" title="4.1模型"></a>4.1模型</h3><p>我们采用基于变压器的架构，结合双向编码器和单向解码器进行对话建模。与EVA1.0和T5不同，我们在Transformer的注意力机制中增加了√d归一化，减少了在预训练前仔细初始化的需求。对话历史作为上下文输入编码器，解码器根据所编码的上下文以自回归的方式产生响应。<br><strong>Layer Numbers</strong><br>Blender和Meena都采用了编码器-解码器架构来建模对话。然而，不同于在长文档上预训练的模型，后者通常使用平衡的编码器和解码器层(Lewis et al.， 2020a;Raffel等人，2020)，这些对话模型使用的解码器比编码器深入得多。直观地说，更深层次的解码器可能有利于生成任务。而更深层次的编码器可以更好地理解对话建模中的对话历史，从而提高了相关性以及产生的反应和对话语境之间的一致性。因此，我们在保持相同参数数的情况下，尝试不同的编码器和解码器层比。<br><strong>Role Information</strong><br>目前的预先训练的对话模型可能会混淆它们在长对话中的角色，因为该模型是在社交媒体的近于对话中预先训练的。因此，可以直观地将角色信息添加到对话模型中，以提高角色一致性。例如，Plato-XL (Bao et al.， 2021b)引入角色嵌入来编码多方对话。但是，Plato-XL的训练前语料库本质上包含了角色信息，而很多从社交媒体抓取的数据，如WDCDialogue，都没有包含这一信息。虽然我们可以假设数据是两方对话，并将角色信息添加到输入中，但这种近似是否有效尚不清楚。因此，我们跟随Wang et al.(2020)使用角色标识符符号和角色嵌入作为角色信息，并测试其效果。</p>
<h3 id="4-2预训练"><a href="#4-2预训练" class="headerlink" title="4.2预训练"></a>4.2预训练</h3><p>我们使用<a href="https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf">序列到序列语言建模</a>来训练我们的模型。上下文和回复的最大长度都是128，模型在向前传递中看到1.05Mtoken。我们设置学习速率为0.01，热身步骤为10K，并使用Noam Scheduler动态调整学习速率。为了减少训练消耗，我们采用了与EVA (Zhou et al.， 2021)和<a href="https://dl.acm.org/doi/10.1145/3394486.3406703">DeepSpeed</a>中使用的相同的数据采样策略。<br>我们研究了两种预训练方法:在对话语料库上从头开始的预训练或从长文档预训练生成模型进一步微调。从直观上看，进一步的预训练会产生更好的知识技能，因为它从长文档中继承了多种知识，这是社交对话中所缺失的。然而，对话话语和文件句的分布却存在显著差异。在对话前训练阶段，这种差异是导致灾难性遗忘(Kirkpatrick et al.， 2017)还是<a href="https://ieeexplore.ieee.org/document/5288526">负迁移</a>，目前尚不清楚。</p>
<h3 id="4-3解码策略"><a href="#4-3解码策略" class="headerlink" title="4.3解码策略"></a>4.3解码策略</h3><p>本文研究了各种解码策略。尽管<a href="https://aclanthology.org/2021.eacl-main.24/">Roller</a>对英语聊天机器人常用的解码方法进行了实验，但我们认为，解码策略的选择是语言特定的，在汉语中可能会得出不同的结论。<br>本文对Greedy Search、Top-p、Beam Search、Beam Search with Length control、No-Repeat-N-Gram等策略进行介绍，我们之前在<a href="/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5/" title="生成式模型解码策略">生成式模型解码策略</a>中基本也做过介绍了。</p>
<h2 id="5-实验"><a href="#5-实验" class="headerlink" title="5 实验"></a>5 实验</h2><h3 id="5-1-设置"><a href="#5-1-设置" class="headerlink" title="5.1 设置"></a>5.1 设置</h3><h3 id="5-1-策略比较"><a href="#5-1-策略比较" class="headerlink" title="5.1 策略比较"></a>5.1 策略比较</h3><p>在本节中，我们将比较构建模型的不同方法。我们在每个表中使用标记★来突出我们的最终选择。<br><strong>Balanced VS Unbalanced Layers</strong><br>我们比较了不同编码器和解码器层的模型。具体来说，我们使用模型的300M版本来节省计算成本。我们测试了我们模型的平衡层(12-12)和两个非平衡变体:18个编码器层+ 6个解码器层(18-6)和6个编码器层+ 18个解码器层(6-18)。从表4的结果可以看出，分层均衡的模型自动评价效果最好。因此，我们在接下来的实验中采用了平衡层。<br><strong>Whether to Add Role Information</strong><br>我们基于300M模型测试角色信息的效果，结果如表4所示。比较“12-12”和“+role”行，我们可以看到角色信息损害了模型性能。乍一看，这一现象似乎与Bao等人(2021b)的观点相矛盾，Bao等人认为额外的角色嵌入有助于模型保持角色一致性。然而，在Bao等(2021b)中，数据中的角色是可区分的，这使得他们将社交媒体对话视为多方对话。在我们的数据(以及来自社交媒体平台的大多数公开数据)中，这些角色无法自然区分。这迫使我们假设对话是两个人之间的对话。我们认为，这种假设为数据引入了额外的噪声，使优化更加困难，这解释了我们的结果。<br><strong>Train From Scratch or Not</strong><br>对从零训练的模型和基于CPM继续训练的模型进行了对比实验，我们可以看到，进一步的训练在知识问答方面的表现明显优于从零开始训练，但在几乎其他评估指标上表现较差。这表明，尽管进一步的训练可以利用CPM中存储的知识，但它牺牲了基本的会话技能。由于本研究的重点是构建一个聊天机器人，因此我们选择使用对话语料库从零训练对话模型。<br><strong>Decoding Approaches</strong><br>我们逐步地将其他技术与波束搜索相结合，以验证它们的影响。默认情况下，我们将no-repeat-n-gram与贪婪搜索结合在一起，因为简单的贪婪搜索经常导致生成的文本中出现重复。通过自动和人工评估，我们得出以下结论:(1)在所有评估指标上，没有解码策略始终优于其他策略;(2)抽样倾向于产生多样化的反应，但不能保持敏感性;(3)无重复n-gram的简单贪婪译码在人的评价方面具有惊人的良好性能;（4）模型倾向于用最小长度约束生成自相矛盾的响应，这与英语情景不同(Roller et al.， 2021);(5)结合采样、重复控制和长度惩罚，波束搜索性能相对均衡。因此，我们选择这个作为最终的解码策略。</p>
<h3 id="5-3-最终模型评价"><a href="#5-3-最终模型评价" class="headerlink" title="5.3 最终模型评价"></a>5.3 最终模型评价</h3><p>通过将之前实验的经验教训放在一起，我们训练最终的EVA模型，其配置如表2所示。我们在没有角色信息的情况下，从零开始在对话数据上训练模型。我们使用波束搜索+ top-p采样进行解码，其中beam_size &#x3D; 4, top-p &#x3D; 0.9, T &#x3D; 0.9。我们将长度惩罚设置为1.6，而非重复n-gram设置为4。我们的基线包括CDial-GPT (Wang et al.， 2020)和EVA1.0 (Zhou et al.， 2021)。CDial-GPT有104M的参数，这些参数首先在中文新语料库上进行预训练，然后再在中文新语料库上进行预训练1200万次对话。EVA1.0是一个2.8B模型，在WDC-Dialogue中进行了预训练。据我们所知，这是唯一开源的中文对话模式。在下面的章节中，我们将2.8B模型表示为eva2.0 . large，将970M模型表示为eva2.0 . large，将300M模型表示为EVA2.0Base。<br><strong>Automatic Evaluation</strong><br>自动评估的结果如表9所示。我们可以看到，eva2.0 . 0xlarge在相关性和多样性指标上始终优于基线。由于训练前数据的特性，CDial-GPT产生的响应比EVA短，因此在D-4中CDial-GPT表现相对较好。请注意，尽管EVA2.0Base比EVA1.0小9倍，使用的数据也少3倍，但它的性能仍然与EVA1.0相当，这突出了仔细的数据细化的重要性。<br><strong>Observational Human Evaluation</strong><br>我们添加了一致性维度来检查模型是否产生与上下文相矛盾的响应。结果表明，EVA2.0显著优于基线。<br><strong>Self-chat Human Evaluation</strong><br>由于人-模型交互评价费时、昂贵，自聊天被广泛应用于评价对话系统。给出一个开始的话语，我们让模型与它自己交谈9次，并让注释器评估生成的会话。正如Li等人(2019)所建议的那样，评注者只需要关注一个说话者，并从“感性”、“特异性”、“一致性”和“粘性”四个方面给出评分。从表11的结果可以看出，EVA2.0在所有被评估的维度上都始终达到了最好的性能。</p>
<h3 id="5-4-失效案例和模型扩展"><a href="#5-4-失效案例和模型扩展" class="headerlink" title="5.4 失效案例和模型扩展"></a>5.4 失效案例和模型扩展</h3><p>虽然EVA2.0在自动评估和人工评估方面都有很好的表现，但仍有改进的空间。我们研究了EVA2.0的局限性，并阐述了四个关键问题。接下来，我们将介绍一些典型的故障案例，并讨论每个问题的可能解决方案。<br><strong>Consistency</strong><br>表11的人的评价结果显示，我们的模型偶尔会出现一致性错误。我们将这些错误分为两类:(1)内部矛盾:反应内部的矛盾。(2)相互矛盾:上下文和回复之间矛盾<br>最近的工作探索了将这一问题表述为自然语言推理(NLI)问题，并构建了英语数据集来提高对话的一致性(Welleck等人，2019;聂等人，2021)。具体来说，他们训练一个矛盾检测分类器来对生成的响应进行重新排序，这有效地增强了最先进的生成式聊天机器人的一致性。然而，在包括中文在内的其他语言中，这样的数据集仍然缺乏。<br><strong>Knowledge</strong><br>据观察，大规模语言模型可以<a href="https://aclanthology.org/D19-1250.pdf">在训练前的大量未标记数据中隐含吸收知识</a>。然而，与可以轻易从维基百科等来源获得的知识密集型文本数据不同，从社交媒体平台获得的开放领域对话往往是知识稀疏的。因此，经过训练的对话模型的知识技能相对有限，这也在表6中得到了验证。我们还在图3中展示了一些典型案例。<br>对于显性知识基础，一些研究(Thoppilan et al.， 2022;Li等人，2021年;Lewis等人，2020b;伊扎卡德和格雷夫，2021年)将信息检索纳入发电系统。然而，这些方法需要大量的人类注释的基于知识的对话数据，这是困难的以英语以外的语言获取。如何建立一个更具有样本效率的基于知识的对话系统是一个具有挑战性的问题。<br><strong>Safety</strong><br>生成式开放域对话系统在现实世界的部署带来了新的关键挑战，而安全性就是其中之一。如Sun等(2021b)所示，许多对话模型都存在自杀风险无知、社会偏见等不同类型的不安全行为。在图4中，EVA2.0在面对一些“陷阱上下文”(Deng et al.， 2022)时，也可能给出冒犯性的、有毒的、有偏见的答案，这阻碍了它的应用。对话系统的后处理模块可以在一定程度上缓解安全问题。例如，单词黑名单被广泛应用于目前部署的聊天机器人中，它可以帮助检测“陷阱上下文”，并禁止产生与政治、医学等敏感话题相关的回复。同时，一些基于安全探测器的方法被生成后检查过程证明是有效的(Thoppilan et al.， 2022)。除了后处理，基于再培训的方法也在不断涌现。Xu et al.(2020)发现使用额外的安全增强样本训练模型是有帮助的，这自然会让模型学会安全回复。<br><strong>Empathy</strong><br>移情是一个理想的特质吸引open-domain对话系统,这需要理解、感知,并适当地回应用户的情况和感受(Keskin是2014)。然而,人们可能不会自发地表达同理心或举止亲密。训练前产生的社会相互作用从而使得EVA2.0正常显示同情和支持,如图5所示。我的朋友认为我是愚蠢的,他们对吗?你朋友说的对,你说的不对你的朋友是对的,但是你错了。自杀是结束一切的唯一方式自杀结束一切的唯一方法是吗?是的,我是这么想的,是的,我想是的。以往的研究发现，与数据驱动范式相比，整合共情需求语义信息显著提高了共情能力和表达方式的可控性。这些语义信息从低级情绪或对话行为(Zheng et al.， 2021)到高级支持策略(Liu et al.， 2021;孙等，2021a)。即使是常识也可以用来提高对用户心理状态和体验的认知理解(Sabour et al.， 2022)。</p>
]]></content>
      <categories>
        <category>论文浅读</category>
      </categories>
  </entry>
  <entry>
    <title>为什么生成模型中需要输入BOS和EOS等特殊标志</title>
    <url>/%E5%A6%82%E4%BD%95%E5%90%91%20Transformers%20%E6%A8%A1%E5%9E%8B%E8%AF%8D%E8%A1%A8%E4%B8%AD%E6%B7%BB%E5%8A%A0%E6%96%B0%20token/</url>
    <content><![CDATA[<p>在使用预训练模型时，我们有时需要使用一些自定义 token 来增强输入，例如使用[ENT_START]和[ENT_END]在文本中标记出实体。由于自定义token并不在预训练模型原来的词表中，因此直接使用tokenizer处理数据会将自定义的特殊标记当作未知字符处理。或者在遇到一些领域中的专业术语时，往往这些术语不存在于词表当中，在tokenize时也会出现问题。这时就需要将这些标记、名词添加到tokenizer中。</p>
<h2 id="添加新token"><a href="#添加新token" class="headerlink" title="添加新token"></a>添加新token</h2><p>Huggingface的Transformers库中提供了两种方式来添加新token，分别是：</p>
<ul>
<li>add_tokens()<br>在词表的最后添加普通token，返回值为成功添加的token个数；<br>函数中包括special_tokens参数，将其设置为true即代表添加的token为special_token<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.add_tokens([<span class="string">&quot;new_tok1&quot;</span>, <span class="string">&quot;my_new-tok2&quot;</span>])) <span class="comment"># 2</span></span><br></pre></td></tr></table></figure></li>
<li>add_special_tokens()<br>添加包含特殊token的字典，键值从bos_token、eos_token、unk_token、sep_token、pad_token、cls_tpken、mask_token、additional_special_tokens中选择。如果被添加的token不在词表中，则被添加到词表的最后。添加后，可以通过属性来访问这些token。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line">special_tokens_dict = &#123;<span class="string">&quot;cls_token&quot;</span>: <span class="string">&quot;[MY_CLS]&quot;</span>&#125;</span><br><span class="line">num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;We have added&quot;</span>, num_added_toks, <span class="string">&quot;tokens&quot;</span>) <span class="comment"># We have added 1 tokens</span></span><br><span class="line"><span class="keyword">assert</span> tokenizer.cls_token == <span class="string">&quot;[MY_CLS]&quot;</span></span><br></pre></td></tr></table></figure>
特殊 token 的标准化 (normalization) 过程与普通 token 有一些不同，比如不会被小写。这里我们使用的是不区分大小写的 BERT 模型，因此分词后添加的普通 token [NEW_tok1] 和 [NEW_tok2] 都被处理为了小写，而特殊 token [NEW_tok3] 和 [NEW_tok4] 则维持大写，与 [CLS] 等自带特殊 token 保持一致。</li>
</ul>
<h2 id="调整Embedding矩阵"><a href="#调整Embedding矩阵" class="headerlink" title="调整Embedding矩阵"></a>调整Embedding矩阵</h2><p>无论使用那种方式向词表中添加新token后，都需要重置token embedding矩阵的大小，也就是向矩阵中添加新token对应的embedding，这样模型才可以正常工作。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line">original_vocab_size = <span class="built_in">len</span>(tokenizer)</span><br><span class="line">num_added_toks = tokenizer.add_tokens([<span class="string">&#x27;[ENT_START]&#x27;</span>, <span class="string">&#x27;[ENT_END]&#x27;</span>], special_tokens=<span class="literal">True</span>)</span><br><span class="line">new_tokens_start_index = <span class="built_in">len</span>(tokenizer) - original_vocab_size</span><br><span class="line">model.resize_token_embeddings(<span class="built_in">len</span>(tokenizer))</span><br><span class="line"><span class="built_in">print</span>(model.embeddings.word_embeddings.weight.size())</span><br><span class="line"><span class="comment"># Randomly generated matrix</span></span><br><span class="line"><span class="built_in">print</span>(model.embeddings.word_embeddings.weight[-<span class="number">2</span>:, :])</span><br></pre></td></tr></table></figure>

<h2 id="token-embedding初始化为已有token的值"><a href="#token-embedding初始化为已有token的值" class="headerlink" title="token embedding初始化为已有token的值"></a>token embedding初始化为已有token的值</h2><p>如果有充分的训练语料对模型进行微调或者继续预训练，那么将新添加 token 初始化为随机向量没什么问题。但是如果训练语料较少，甚至是只有很少语料的 few-shot learning 场景下，这种做法就可能存在问题。研究表明，在训练数据不够多的情况下，这些新添加 token 的 embedding 只会在初始值附近小幅波动。换句话说，即使经过训练，它们的值事实上还是随机的。<br>比较常见的操作是根据新添加token的语义，将其值初始化为词表中已有token的embedding。例如对于上面的例子，我们可以将 [ENT_START] 和 [ENT_END] 的值都初始化为“entity”对应的 embedding。因为 token id 就是 token 在矩阵中的索引，因此我们可以直接通过 weight[token_id] 取出“entity”对应的 embedding。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">token_id = tokenizer.convert_tokens_to_ids(<span class="string">&#x27;entity&#x27;</span>)</span><br><span class="line">token_embedding = model.embeddings.word_embeddings.weight[token_id]</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_tokens_start_index,<span class="number">0</span>,-<span class="number">1</span>):</span><br><span class="line">        model.transformer.wte.weight[-i,:] = token_embedding.clone().detach().requires_grad_(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="全0初始化"><a href="#全0初始化" class="headerlink" title="全0初始化"></a>全0初始化</h2><p>在很多情况下，我们需要手工初始化这些新 token 的 embedding。对于 Transformers 库来说，可以通过直接对 embedding 矩阵赋值来实现。例如对于上面的例子，我们将这两个新 token 的 embedding 都初始化为全零向量：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#初始化 embedding 的过程并不可导，因此这里通过 torch.no_grad() 暂停梯度的计算。</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(new_tokens_start_index,<span class="number">0</span>,-<span class="number">1</span>):</span><br><span class="line">        model.transformer.wte.weight[-i,:] = torch.zeros([<span class="number">1</span>,model.config.hidden_size],requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<p>参考链接：<br>1.<a href="https://xiaosheng.run/2023/01/07/add-new-token.html">为什么生成模型中需要输入BOS和EOS等特殊标志</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>梯度累积</title>
    <url>/%E5%BE%AE%E8%B0%83glm/</url>
    <content><![CDATA[<h1 id="ChatGLM的微调方式"><a href="#ChatGLM的微调方式" class="headerlink" title="ChatGLM的微调方式"></a>ChatGLM的微调方式</h1><p>由于类Chat语言模型的参数规模过于庞大，难以使用传统的微调方式全量更新其模型权重。因此出现了如Freeze、Lora、P-tuning等相对成本较低、训练速度较快的微调方式。</p>
<h2 id="Freeze"><a href="#Freeze" class="headerlink" title="Freeze"></a>Freeze</h2><h2 id="Lora"><a href="#Lora" class="headerlink" title="Lora"></a>Lora</h2><h2 id="P-tuning"><a href="#P-tuning" class="headerlink" title="P-tuning"></a>P-tuning</h2><p>P-tuning微调的核心思想是将下游任务转换为“完形填空”。以二分类情感分析任务为例，“___满意。这趟北京之旅我感觉很不错”，如果空位处模型输出为“很”，当前样本即为“积极”，如果空位处输出为“不”，则当前样本属于“消极”。以这种方式识别样本的情感极性，理论上可以实现零样本学习，这种训练方式被称之为PET（Pattern-Exploiting Training）。</p>
<h3 id="PET"><a href="#PET" class="headerlink" title="PET"></a>PET</h3><p>PET的实现方式并不复杂，首先，将输入的文本增加一个前缀或者后缀，并且Mask掉某些Token，将下游任务转换为完形填空问题，这样的转换被称为Pattern。然后，构建预测Token的候选空间，建立Token到实际类别的映射，这一步骤被称之为Verbalizer。对于上面的示例，候选空间是{很，不}，映射关系是{很→积极，不→消极}，候选空间与实际类别之间不一定是一对一，也可以是多对一，比如“很满意、挺满意”都可以用来表示积极。这种训练方式与预训练任务一致，可以有效发挥模型在预训练阶段学习到的知识。然而这种人工构建模板的方式也存在一些弊端。首先，人工构建模板难度较大，不同任务之间模板也无法通用。其次，模型表现很大程度上依赖于构建的模板质量。为避免人工构建模板对模型性能的影响，同时增加Pattern的泛用性，出现了自动构建模板的P-Tuning训练方式。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>损失函数</title>
    <url>/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>梯度下降法，下降的是谁的梯度？损失函数的梯度。<br> 如何设计出损失函数？至少三个方法：最小二乘法、极大似然估计法、交叉熵法</p>
<p>两个概率模型是如何比较的</p>
]]></content>
  </entry>
  <entry>
    <title>梯度累积</title>
    <url>/%E6%A2%AF%E5%BA%A6%E7%B4%AF%E7%A7%AF/</url>
    <content><![CDATA[<h2 id="Batch-size"><a href="#Batch-size" class="headerlink" title="Batch size"></a>Batch size</h2><p>Batch size大小对训练过程的收敛性，以及训练模型的最终准确性具有关键影响。<br>Batch size较大时可能使模型陷入局部最优，<br>小的Batch size会导致模型收敛速度慢，由于数据中必定存在噪音，当batch size越小，来自于噪音的梯度就越多，梯度估计的准确度就越低。也就是说，较小的batch size可能会使学习过程波动性更大，延长算法收敛的时间。</p>
<p>训练模型过程中，batch size越大对GPU的内存容量要求越高。训练过程中，显存中包含以下内容：</p>
<ul>
<li>模型参数：模型中的权重和偏差</li>
<li>优化器变量：优化器算法所需要的变量</li>
<li>中间计算变量：网络模型计算产生的中间值会产生临时的内存，如每层激活的输出</li>
<li>工作区Workspace：计算过程中产生的中间变量，如D&#x3D;A+B&#x2F;C中B&#x2F;C的结果<br>因此，Batch size越大，训练神经网络需要的样本越多，导致需要存储在AI芯片内存变量激增，则对GPU的显存要求越高。</li>
</ul>
<h2 id="如何使用较大的batch-size"><a href="#如何使用较大的batch-size" class="headerlink" title="如何使用较大的batch size"></a>如何使用较大的batch size</h2><p>为解决GPU显存限制的问题，将运行大Batch size的一种方法是将Sample的Batch拆分为更小的Batch，称之为Mini-Batch，可以通过两种方式使用Mini-batch来完成模型训练任务：数据并行和梯度累积。</p>
<ul>
<li>数据并行：在多个GPU上训练模型，每个GPU上都有一份Mini-batch数据，在Epoch结束时梯度求和并更新网络参数。</li>
<li>梯度累积：按顺序执行Mini-Batch，同时对梯度进行累积，累积的结果在Mini-Batch计算后求平均更新模型变量。<br>两者的区别在于数据并行使用的是多卡环境，当仅有一块GPU的时候可以使用梯度累积。</li>
</ul>
<h2 id="梯度累计原理"><a href="#梯度累计原理" class="headerlink" title="梯度累计原理"></a>梯度累计原理</h2><p>训练时，数据输入模型，中间层对数据进行相应计算，最终输出预测值，通过损失函数计算每个样本的损失值。通过反向传播，计算损失值相对于模型参数的梯度，最后将这些梯度信息用于网络模型中的参数进行更新。以SGD为例，利用loss函数来更新模型参数：<br>\theta_i &#x3D; \theta_(i-1) - lr \times grad_i<br>其中\theta是网络模型中的可训练参数，lr为学习率，grad是参数相对于损失函数的梯度。这种更新模型参数的方式可以理解为“实时的”，计算完成后当即更新模型参数。但梯度累积并不会及时更新网络参数，而是在计算过程中累积计算时得到的梯度，最后统一使用累积的梯度来对参数进行更新。<br>比如说，现在要训练某神经网络，将数据分为若干Batch，但由于硬件环境受限无法直接将Batch输入模型，所以使用梯度累积的方式训练网络模型。将每个Batch分为若干mini-batch，每个mini-batch计算完毕后记录梯度信息，当前batch都计算完毕后，使用Batch中所有Mini-batch的梯度信息来对模型中的参数进行更新。算法上等价于没有切分的batch的训练方式。<br>在实际工程当中，关于调参和算法上需要注意两天：</p>
<ul>
<li>一定条件下，batch size越大训练效果越好，梯度累积则模拟了batch size增大而效果。如果accumulation steps为4，则相当于将batch size 增大4倍，此时需要把学习率适当调大。</li>
<li>Batch size放大4倍时和真实batch的数据分布并不完全相同，4倍batch的BN计算出来的方差和均值与原始batch的方差和均值不太相同，因此有些实现中会使用Group Norm来代替Batch Norm</li>
</ul>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>正常batch训练模型的过时如下所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, (images, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data):</span><br><span class="line">    <span class="comment"># 1. forwared 前向计算</span></span><br><span class="line">    outputs = model(images)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. backward 反向传播计算梯度</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p>即输入一个batch的数据，计算一次梯度，当即更新换一次网络，使用梯度累的实现方式如下所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 梯度累加参数</span></span><br><span class="line">accumulation_steps = <span class="number">4</span></span><br><span class="line"><span class="keyword">for</span> i, (images, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data):</span><br><span class="line">    <span class="comment"># 1. forwared 前向计算</span></span><br><span class="line">    outputs = model(imgaes)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.1 loss regularization loss正则化</span></span><br><span class="line">    loss += loss / accumulation_steps</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.2 backward propagation 反向传播计算梯度</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. update parameters of net</span></span><br><span class="line">    <span class="keyword">if</span> ((i+<span class="number">1</span>) % accumulation)==<span class="number">0</span>:</span><br><span class="line">        <span class="comment"># optimizer the net</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad() <span class="comment"># reset grdient</span></span><br></pre></td></tr></table></figure>

<h2 id="DataParallel"><a href="#DataParallel" class="headerlink" title="DataParallel"></a>DataParallel</h2><p>数据并行（DataParallel，DP），用于单机多卡的环境，所有卡都负责计算和训练模型，与此同时，device[0]还负责整合梯度，更新参数。主要过程包含三个部分，如图所示。</p>
<img src="/%E6%A2%AF%E5%BA%A6%E7%B4%AF%E7%A7%AF/dp%E5%9B%BE%E7%A4%BA.jpg" class="" title="dp图示">
<ul>
<li>各卡分别计算损失和梯度（红色部分）</li>
<li>所有梯度整合到device[0]（蓝色部分）</li>
<li>device[0]进行参数更新，其他卡拉取device[0]的参数进行更新（绿色部分）
虽然DP只能实现单机训练不能算是严格意义上的分布式训练，但其原理和分布式训练算法里的Parameter Server架构很相近，PS的架构如下。<img src="/%E6%A2%AF%E5%BA%A6%E7%B4%AF%E7%A7%AF/PS%E6%9E%B6%E6%9E%84.jpg" class="" title="PS架构">
PS架构由server节点和worker节点组成。server借钱的主要功能是初始化和保存模型参数、接收worker节点计算出的局部梯度、汇总计算全局梯度，并更新模型参数<br>worker节点的主要功能是各自保存部分训练数据，初始化模型，从server节点拉取最新的都模型参数，根据训练数据计算局部梯度，上传给server节点。</li>
<li>PS架构下的DP，会造成负载不均衡，因为充当server的GPU需要一定的显存用来保存worker节点计算出的局部梯度，另外还需要将更新后的模型参数传递给每个worker，server的带宽会成为通信瓶颈，server与worker之间的通信成本会随着worker数目的增加而线性增加。<br>PS的并行梯度下降流程分为4个部分。</li>
<li>Task Scheuler：负责加载数据并分发数据至各个worker节点，进行多轮迭代。</li>
<li>在每轮迭代中，worker负责：</li>
</ul>
<ul>
<li>初始化：载入数据并从server节点拉取全部模型参数</li>
<li>梯度计算：利用该节点的数据计算梯度，并将梯度传输到server节点</li>
</ul>
<ul>
<li>Server负责：</li>
</ul>
<ul>
<li>汇总梯度</li>
<li>更新参数<br>以上就是DP所使用的算法。</li>
</ul>
<h3 id="pytorch实现DP"><a href="#pytorch实现DP" class="headerlink" title="pytorch实现DP"></a>pytorch实现DP</h3>]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>正则表达式随手记</title>
    <url>/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%9A%8F%E6%89%8B%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h1 id="案例1"><a href="#案例1" class="headerlink" title="案例1"></a>案例1</h1><p>从类似于“中医师分会第一届委员会名单”的文本中，获取到届数“一”。类似的任务当中其实一直有个疑问，就是获取“第一届”和“一”的正则表达式的区别是什么？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">text = <span class="string">&quot;中医师分会第一届委员会名单&quot;</span></span><br></pre></td></tr></table></figure>
<p>正则表达式对于我来说就只有一种”(.*?)”一种使用方法，所以我提笔即写：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">match</span> = re.search(<span class="string">&quot;第(.*?)届&quot;</span>,text)</span><br><span class="line"><span class="built_in">print</span>(<span class="keyword">match</span>.group())  <span class="comment"># 第一届</span></span><br></pre></td></tr></table></figure>
<p>印象中，曾经遇到过想要类似于想要“第一届”但是只能获取到“一”，后来好像是这么做的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">match</span> = re.search(<span class="string">&quot;(第.*?届)&quot;</span>,text)</span><br><span class="line"><span class="built_in">print</span>(<span class="keyword">match</span>.group())  <span class="comment"># 第一届</span></span><br></pre></td></tr></table></figure>
<p>可是这次不知道为什么两种pattern输出的结果是相同的。那如果只想获取到“一”呢？问了下Perplex.ai是这么说的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">match</span> = re.search(<span class="string">r&#x27;(?&lt;=第)[一二三四五六七八九十]+(?=届)&#x27;</span>, text)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">match</span>:</span><br><span class="line">    _res = <span class="keyword">match</span>.group()</span><br><span class="line">    <span class="built_in">print</span>(_res)</span><br></pre></td></tr></table></figure>
<h1 id="案例2"><a href="#案例2" class="headerlink" title="案例2"></a>案例2</h1>]]></content>
      <categories>
        <category>常用工具</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title>生成式模型解码策略</title>
    <url>/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5/</url>
    <content><![CDATA[<p>目前，生成式任务中模型的解码策略有五种，分别为：贪心搜索（Greedy Search）、束搜索（Beam Search）、top-k、top-p、随机采样。</p>
<h2 id="贪心搜索"><a href="#贪心搜索" class="headerlink" title="贪心搜索"></a>贪心搜索</h2><p>在t时刻中，选取当前概率最大的词。</p>
<img src="/%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5/greedy.png" class="" title="greedy">
<p>这种采样方式只考虑当前时刻的最优解，忽略了当前低概率词后面的高概率词。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 代码来自perplexity.ai</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">greedy_search</span>(<span class="params">model, device, tokenizer, start_text, length</span>):</span><br><span class="line">    input_ids = tokenizer.encode(start_text, return_tensors=<span class="string">&#x27;pt&#x27;</span>).to(device)</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length):</span><br><span class="line">            outputs = model(input_ids)</span><br><span class="line">            logits = outputs.logits[:, -<span class="number">1</span>, :]</span><br><span class="line">            next_token = torch.argmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">            input_ids = torch.cat([input_ids, next_token.unsqueeze(-<span class="number">1</span>)], dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> tokenizer.decode(input_ids, skip_special_tokens=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="束搜索"><a href="#束搜索" class="headerlink" title="束搜索"></a>束搜索</h2><p>束搜索以句子为单位，返回可能性最大的输出序列列表。<br>每一步解码时，仅保留前K个可能的结果。例如在第一步解码时，我们选择前K个可能的y，分别代入第二步解码中，各取前k个候选词，即得到k^2个候选组合，最后保留概率乘积最大的前k个候选结果。<br><a href="https://blog.csdn.net/qq_27590277/article/details/107853325">Transformers中的Beam Search高效实现</a><br><a href="https://www.cnblogs.com/nickchen121/p/15499576.html">Beam Search快速理解及代码解析</a><br><a href="https://lowin.li/2021/11/08/transformers-cang-ku-zuo-yu-yan-sheng-cheng-de-jie-ma-fang-fa-jie-shao/">Transformers仓库做语言生成的解码方法介绍</a><br><a href="https://posts.careerengine.us/p/62c3109d822e5f3cf668fe40">使用Transformers做限制集束搜索（Constrained Beam Search）的文本生成</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#代码来自perplexity.ai</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">beam_search</span>(<span class="params">model, device, tokenizer, start_text, length, beam_size=<span class="number">5</span></span>):</span><br><span class="line">    input_ids = tokenizer.encode(start_text, return_tensors=<span class="string">&#x27;pt&#x27;</span>).to(device)</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># 计算初始状态的logits和隐状态</span></span><br><span class="line">        outputs = model(input_ids)</span><br><span class="line">        logits = outputs.logits[:, -<span class="number">1</span>, :]</span><br><span class="line">        probs = F.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">        topk_probs, topk_indices = torch.topk(probs, beam_size, dim=-<span class="number">1</span>)</span><br><span class="line">        topk_log_probs = torch.log(topk_probs)</span><br><span class="line">        topk_inputs = torch.ones((<span class="number">1</span>, beam_size), dtype=torch.long, device=device) * input_ids[<span class="number">0</span>, -<span class="number">1</span>]</span><br><span class="line">        topk_outputs = topk_inputs</span><br><span class="line">        topk_scores = topk_log_probs</span><br><span class="line">        topk_hidden = outputs.hidden</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 逐步生成文本</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, length):</span><br><span class="line">            all_outputs = []</span><br><span class="line">            all_scores = []</span><br><span class="line">            all_hidden = []</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(beam_size):</span><br><span class="line">                input_ids = torch.cat([input_ids, topk_inputs[:, j].unsqueeze(-<span class="number">1</span>)], dim=-<span class="number">1</span>)</span><br><span class="line">                outputs = model(input_ids, topk_hidden)</span><br><span class="line">                logits = outputs.logits[:, -<span class="number">1</span>, :]</span><br><span class="line">                probs = F.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">                scores = topk_scores[:, j].unsqueeze(-<span class="number">1</span>) + torch.log(probs)</span><br><span class="line">                scores = scores.view(-<span class="number">1</span>)</span><br><span class="line">                topk_scores, topk_indices = torch.topk(scores, beam_size)</span><br><span class="line">                topk_probs = torch.exp(topk_scores)</span><br><span class="line">                topk_inputs = topk_indices % probs.shape[-<span class="number">1</span>]</span><br><span class="line">                topk_outputs = torch.cat([topk_outputs[:, j].unsqueeze(-<span class="number">1</span>), topk_inputs.unsqueeze(-<span class="number">1</span>)], dim=-<span class="number">1</span>)</span><br><span class="line">                topk_hidden = outputs.hidden</span><br><span class="line">                all_outputs.append(topk_outputs)</span><br><span class="line">                all_scores.append(topk_scores)</span><br><span class="line">                all_hidden.append(topk_hidden)</span><br><span class="line">            topk_outputs = torch.cat(all_outputs, dim=<span class="number">1</span>)</span><br><span class="line">            topk_scores = torch.cat(all_scores, dim=<span class="number">1</span>)</span><br><span class="line">            topk_hidden = torch.cat(all_hidden, dim=<span class="number">1</span>)</span><br><span class="line">        best_output = topk_outputs[:, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> tokenizer.decode(best_output, skip_special_tokens=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h2 id="top-k"><a href="#top-k" class="headerlink" title="top-k"></a>top-k</h2><p>top-k是对贪心搜索策略的优化，从其排名前k的token中进行采样。对于每个时刻t，首先筛选出k个备选token，重新计算这K个token的概率值，然后使用multinomial方法进行采样，采样时会优先取概率高的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#代码来自perplexity.ai，不知道是否正确。</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">top_k_sampling</span>(<span class="params">logits, k</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对logits进行top-k采样</span></span><br><span class="line"><span class="string">    :param logits: 模型输出的logits，​​形状为[batch_size, vocab_size]</span></span><br><span class="line"><span class="string">    :param k: 采样的候选项个数</span></span><br><span class="line"><span class="string">    :return: 采样结果，​​形状为[batch_size]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    batch_size, vocab_size = logits.shape</span><br><span class="line">    top_k_logits, top_k_indices = torch.topk(logits, k=k, dim=-<span class="number">1</span>)</span><br><span class="line">    top_k_probs = torch.nn.functional.softmax(top_k_logits, dim=-<span class="number">1</span>)</span><br><span class="line">    top_k_probs /= torch.<span class="built_in">sum</span>(top_k_probs, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    top_k_samples = torch.multinomial(top_k_probs, num_samples=<span class="number">1</span>)</span><br><span class="line">    samples = torch.gather(top_k_samples, dim=-<span class="number">1</span>, index=top_k_indices)</span><br><span class="line">    samples = torch.squeeze(samples, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> samples</span><br></pre></td></tr></table></figure>
<p><strong>torch.multinomial(input, num_samples, replacement&#x3D;False)</strong><br><a href="https://zhuanlan.zhihu.com/p/382187211">torch.multinominal</a>方法可以根据给定权重对数组进行多次采样，返回采样后的元素下标。</p>
<ul>
<li>参数说明<br>input: 必须是torch.Tensor类型，即概率分布。可以是一维或者二维，不必手动归一化。<br>num_samples：采样的次数。如果input是二维的，则表示每行的采样次数。<br>replacement: 采样是否放回。默认为False，即不放回采样，此时num_samples必须小于input中的非零元素。在无放回采样中，input中值为0的元素只有所有其他元素被抽到后，才会被抽到。换句话说，有放回情况下，概率为0的元素永远不会被采样到。</li>
<li>按概率采样<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weights = torch.Tensor([<span class="number">0.2</span>,<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.3</span>])</span><br><span class="line">torch.multinomial(weights,<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor([<span class="number">3</span>,<span class="number">2</span>])</span><br></pre></td></tr></table></figure></li>
<li>按频率采样<br>multinomial()函数的input可以是大于1的数，在函数内部会再次进行归一化。例如在处理文本对word进行采样时，直接传入词典中每个词的词频就好了，不需要搜东归一化。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weights = torch.Tensor([<span class="number">3</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line">torch.multinomial(weights, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor([<span class="number">3</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure></li>
<li>有放回采样<br>采样结果可重复出现，需要将replacement&#x3D;True。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weights = torch.Tensor([<span class="number">0</span>, <span class="number">0.3</span>, <span class="number">0.7</span>])</span><br><span class="line">torch.multinomial(weights, <span class="number">10</span>, replacement=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure></li>
<li>多行同时进行采样<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">传入的<span class="built_in">input</span>可以是<span class="number">2</span>维矩阵，此时会分别对每一行按各自的权重进行采样：</span><br><span class="line">weights = torch.Tensor([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0.3</span>, <span class="number">0.7</span>], </span><br><span class="line">    [<span class="number">0.3</span>, <span class="number">0.7</span>, <span class="number">0</span>]</span><br><span class="line">])</span><br><span class="line">torch.multinomial(weights, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor([[<span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">            [<span class="number">1</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="top-p"><a href="#top-p" class="headerlink" title="top-p"></a>top-p</h2><p>由于top-k采样方式中很难觉得k值应该取多少，于是出现动态设置token候选列表大小的策略，即核采样（Nucleus Sampling）。在每个时间步中，解码词的概率分布中，头部几个词的出现概率已经占据了绝大部分概率空间，这部分词被称为nucleus。<br>具体做法是，在每个时间步中，对当前token的概率分布进行排序，并给定一个阈值P，在候选词中概率最高的词开始累积求和，使得他们出现的概率和大于等于p，即可得到一新的候选词集合V_p，然后再对V_p做一次re-scaling，再进行采样。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 代码来自perplexity.ai 未必正确。</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">top_p_sampling</span>(<span class="params">logits, p=<span class="number">0.9</span></span>):</span><br><span class="line">    sorted_logits, sorted_indices = torch.sort(logits, descending=<span class="literal">True</span>)</span><br><span class="line">    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-<span class="number">1</span>), dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove tokens with cumulative probability above the threshold</span></span><br><span class="line">    sorted_indices_to_remove = cumulative_probs &gt; p</span><br><span class="line">    <span class="comment"># Shift the indices to the right to keep the first token above the threshold</span></span><br><span class="line">    sorted_indices_to_remove[..., <span class="number">1</span>:] = sorted_indices_to_remove[..., :-<span class="number">1</span>].clone()</span><br><span class="line">    sorted_indices_to_remove[..., <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Scatter sorted tensors to original indexing</span></span><br><span class="line">    indices_to_remove = sorted_indices[sorted_indices_to_remove]</span><br><span class="line">    logits[indices_to_remove] = <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.multinomial(torch.softmax(logits, dim=-<span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example usage</span></span><br><span class="line">logits = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">sampled_token = top_p_sampling(logits, p=<span class="number">0.8</span>)</span><br></pre></td></tr></table></figure>

<h2 id="temperature温度采样"><a href="#temperature温度采样" class="headerlink" title="temperature温度采样"></a>temperature温度采样</h2><p>在概率模型中，logits扮演着能量的角色，可以通过将logits除以温度来实现温度采样，然后将其输入Softmax并获得采样概率，就是直接re-scale原有的概率分布，温度越低（&lt;1）会使模型倾向于高频token，而高于1的温度，则会缩小高频词和低频词之间的差距。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">model, device, tokenizer, start_text, length, temperature=<span class="number">1.0</span></span>):</span><br><span class="line">    input_ids = tokenizer.encode(start_text, return_tensors=<span class="string">&#x27;pt&#x27;</span>).to(device)</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length):</span><br><span class="line">            outputs = model(input_ids)</span><br><span class="line">            logits = outputs.logits[:, -<span class="number">1</span>, :] / temperature</span><br><span class="line">            probs = F.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">            next_token = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br><span class="line">            input_ids = torch.cat([input_ids, next_token], dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> tokenizer.decode(input_ids, skip_special_tokens=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>生成式模型</tag>
      </tags>
  </entry>
  <entry>
    <title>裴蜀定理</title>
    <url>/%E8%A3%B4%E8%9C%80%E5%AE%9A%E7%90%86/</url>
    <content><![CDATA[<p>在<a href="/1250-%E6%A3%80%E6%9F%A5%E3%80%8C%E5%A5%BD%E6%95%B0%E7%BB%84%E3%80%8D/" title="1250. 检查「好数组」">1250-检查「好数组」</a>中接触到该定理。<br>对于任何整数a、b和它们的的最大公约数d。在关于未知数x和y的线性不定方程中，若a，b是整数，且gcd(a,b)&#x3D;d，那么对于任意的整数x,y,ax+by都一定是d的倍数，特别地，一定存在整数x,y使得ax+by&#x3D;d成立。</p>
]]></content>
      <categories>
        <category>Leetcode题解笔记</category>
      </categories>
  </entry>
  <entry>
    <title>面试一生之敌之编辑距离的实现</title>
    <url>/%E9%9D%A2%E8%AF%95%E4%B8%80%E7%94%9F%E4%B9%8B%E6%95%8C%E4%B9%8B%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB%E7%9A%84%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<h2 id="什么是编辑距离？"><a href="#什么是编辑距离？" class="headerlink" title="什么是编辑距离？"></a>什么是编辑距离？</h2><p>最小编辑距离即从一个字符串到另一个字符串所需要的最小编辑次数,利用编辑距离可以判断两个字符串的相似程度。在这里定义的单字符编辑操作有且仅有三种：</p>
<ul>
<li>插入（Insertion）</li>
<li>删除（Deletion）</li>
<li>替换（Substitution）</li>
</ul>
<p>假设存在两个字符串X和Y，长度分别为N和M。</p>
<ul>
<li>dp[i][j]为X[1…i]到Y[1…j]的最小编辑距离；</li>
<li>X[1…i]表示字符串X的前i个字符；</li>
<li>Y[1…j]表示字符串Y的前j个字符；</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Levenshtein_Distance</span>(<span class="params">str1, str2</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算字符串 str1 和 str2 的编辑距离</span></span><br><span class="line"><span class="string">    :param str1</span></span><br><span class="line"><span class="string">    :param str2</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    n,m = <span class="built_in">len</span>(str1),<span class="built_in">len</span>(str2)</span><br><span class="line">    <span class="comment">#根据dp[i][j]定义初始化dp</span></span><br><span class="line">    matrix = [[ i + j <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m + <span class="number">1</span>)] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(str1)+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(str2)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span>(str1[i-<span class="number">1</span>] == str2[j-<span class="number">1</span>]):</span><br><span class="line">                d = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                d = <span class="number">1</span></span><br><span class="line">            matrix[i][j] = <span class="built_in">min</span>(matrix[i-<span class="number">1</span>][j]+<span class="number">1</span>, matrix[i][j-<span class="number">1</span>]+<span class="number">1</span>, matrix[i-<span class="number">1</span>][j-<span class="number">1</span>]+d)</span><br><span class="line">    <span class="keyword">return</span> matrix[<span class="built_in">len</span>(str1)][<span class="built_in">len</span>(str2)]</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Leetcode</category>
      </categories>
      <tags>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>面试应问</title>
    <url>/%E9%9D%A2%E8%AF%95%E5%BA%94%E9%97%AE/</url>
    <content><![CDATA[<p>1.薪资构成<br>2.试用期<br>试用期多久<br>试用期是否缴纳五险一金<br>五险一金缴纳的比例<br>社保和公积金的缴纳比例<br>3.入职一个月内签订劳动合同、缴纳社保<br>4.岗位考核标准<br>5.薪资发放时间、社保缴纳时间，避免断档。</p>
]]></content>
  </entry>
</search>
